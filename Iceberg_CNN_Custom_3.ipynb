{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\kaggle\\iceberg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import importlib\n",
    "SEED = 1234\n",
    "np.random.seed(SEED) \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D, AveragePooling2D, Concatenate, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import uniform_filter\n",
    "\n",
    "%cd E:\\kaggle\\iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_img(band_1, band_2, is_iceberg, angle = None):\n",
    "    if angle is None:\n",
    "        title_str = 'Iceberg' if is_iceberg == 1 else 'Ship'\n",
    "    else:\n",
    "        title_str = 'Iceberg-' + str(angle) if is_iceberg == 1 else 'Ship-' + str(angle)\n",
    "    fig = plt.figure(0, figsize=(10,10))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    ax.set_title(title_str + ' - Band 1')\n",
    "    ax.imshow(band_1,cmap='jet')\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    ax.set_title(title_str + ' - Band 2')\n",
    "    ax.imshow(band_2,cmap='jet')\n",
    "    plt.show()\n",
    "\n",
    "# implement functions to convert SAR data from decibel units to linear units and back again\n",
    "def decibel_to_linear(band):\n",
    "     # convert to linear units\n",
    "    return np.power(10,np.array(band)/10)\n",
    "\n",
    "def linear_to_decibel(band):\n",
    "    return 10*np.log10(band)\n",
    "\n",
    "# implement the Lee Filter for a band in an image already reshaped into the proper dimensions\n",
    "def lee_filter(band, window, var_noise = 0.25):\n",
    "    # band: SAR data to be despeckled (already reshaped into image dimensions)\n",
    "    # window: descpeckling filter window (tuple)\n",
    "    # default noise variance = 0.25\n",
    "    # assumes noise mean = 0\n",
    "    \n",
    "    mean_window = uniform_filter(band, window)\n",
    "    mean_sqr_window = uniform_filter(band**2, window)\n",
    "    var_window = mean_sqr_window - mean_window**2\n",
    "\n",
    "    weights = var_window / (var_window + var_noise)\n",
    "    band_filtered = mean_window + weights*(band - mean_window)\n",
    "    return band_filtered\n",
    "\n",
    "def apply_lee_filter(band_1_linear, band_2_linear, window_var_index = 0, noise_var_index = 0):\n",
    "    windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "    noise_var = np.array([1, 2, 4])\n",
    "    noise_var_1 = np.round(np.var(band_1_linear) * noise_var, 10)\n",
    "    noise_var_2 = np.round(np.var(band_2_linear) * noise_var, 10)\n",
    "    band_1_linear_filtered = lee_filter(band_1_linear, windows[window_var_index], noise_var_1[noise_var_index])\n",
    "    band_2_linear_filtered = lee_filter(band_2_linear, windows[window_var_index], noise_var_2[noise_var_index])\n",
    "    return band_1_linear_filtered, band_2_linear_filtered\n",
    "\n",
    "def apply_lee_filter_single(band_linear, window_var_index = 0, noise_var_index = 0):\n",
    "    windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "    noise_var = np.array([1, 2, 4])\n",
    "    noise_var = np.round(np.var(band_linear) * noise_var, 10)\n",
    "    band_linear_filtered = lee_filter(band_linear, windows[window_var_index], noise_var[noise_var_index])\n",
    "    return band_linear_filtered\n",
    "\n",
    "def np_get_scaled_band(band_list):\n",
    "    imgs = []\n",
    "    for band in band_list:        \n",
    "        imgs.append((band - band.mean()) / band.std())\n",
    "    return np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(\"E:/kaggle/iceberg/train.json/data/processed/train.json\")\n",
    "Y_train=train['is_iceberg']\n",
    "test = pd.read_json(\"E:/kaggle/iceberg/test.json/data/processed/test.json\")\n",
    "\n",
    "train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\n",
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "train['inc_angle']=train['inc_angle'].fillna(method='pad')\n",
    "test['inc_angle']=test['inc_angle'].fillna(method='pad')\n",
    "X_angle=train['inc_angle']\n",
    "X_test_angle=test['inc_angle']\n",
    "\n",
    "def iso(arr):\n",
    "    arr = np.reshape(arr, (75,75))\n",
    "    p = arr > (np.mean(arr) + 2 * np.std(arr))\n",
    "    return p * arr\n",
    "\n",
    "def size(arr):     \n",
    "    return float(np.sum(arr < -5)) / (75 * 75)\n",
    "\n",
    "train['iso_1'] = train.band_1.apply(iso)\n",
    "train['iso_2'] = train.band_2.apply(iso)\n",
    "train['size_1'] = train.iso_1.apply(size)\n",
    "train['size_2'] = train.iso_2.apply(size)\n",
    "X_size_1 = np.array(train['size_1'])\n",
    "X_size_2 = np.array(train['size_2'])\n",
    "\n",
    "test['iso_1'] = test.band_1.apply(iso)\n",
    "test['iso_2'] = test.band_2.apply(iso)\n",
    "test['size_1'] = test.iso_1.apply(size)\n",
    "test['size_2'] = test.iso_2.apply(size)\n",
    "test_size_1 = np.array(test['size_1'])\n",
    "test_size_2 = np.array(test['size_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the training data\n",
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "#apply filter\n",
    "X_band_1_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_1])\n",
    "X_band_2_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_2])\n",
    "X_band_1_filtered = linear_to_decibel(X_band_1_filtered)\n",
    "X_band_2_filtered = linear_to_decibel(X_band_2_filtered)\n",
    "X_band_1 = X_band_1_filtered\n",
    "X_band_2 = X_band_2_filtered\n",
    "X_band_mean = (X_band_1 + X_band_2) / 2\n",
    "# construct bands\n",
    "X_band_3=np.fabs(np.subtract(X_band_1,X_band_2))\n",
    "X_band_4=np.maximum(X_band_1,X_band_2)\n",
    "X_band_5=np.minimum(X_band_1,X_band_2)\n",
    "# subtract mean\n",
    "X_band_3 = np_get_scaled_band(X_band_3)\n",
    "X_band_4 = np_get_scaled_band(X_band_4)\n",
    "X_band_5 = np_get_scaled_band(X_band_5)\n",
    "\n",
    "# X_train = np.concatenate([X_band_3[:, :, :, np.newaxis],X_band_4[:, :, :, np.newaxis],X_band_5[:, :, :, np.newaxis]], axis=-1)\n",
    "X_train = np.concatenate([X_band_1[:, :, :, np.newaxis],X_band_2[:, :, :, np.newaxis],X_band_mean[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "#apply filter\n",
    "X_band_test_1_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_test_1])\n",
    "X_band_test_2_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_test_2])\n",
    "X_band_test_1_filtered = linear_to_decibel(X_band_test_1_filtered)\n",
    "X_band_test_2_filtered = linear_to_decibel(X_band_test_2_filtered)\n",
    "X_band_test_1 = X_band_test_1_filtered\n",
    "X_band_test_2 = X_band_test_2_filtered\n",
    "X_band_test_mean = (X_band_test_1 + X_band_test_2) / 2\n",
    "# construct bands\n",
    "X_band_test_3=np.fabs(np.subtract(X_band_test_1,X_band_test_2))\n",
    "X_band_test_4=np.maximum(X_band_test_1,X_band_test_2)\n",
    "X_band_test_5=np.minimum(X_band_test_1,X_band_test_2)\n",
    "# subtract mean\n",
    "X_band_test_3 = np_get_scaled_band(X_band_test_3)\n",
    "X_band_test_4 = np_get_scaled_band(X_band_test_4)\n",
    "X_band_test_5 = np_get_scaled_band(X_band_test_5)\n",
    "\n",
    "# X_test = np.concatenate([X_band_test_3[:, :, :, np.newaxis], X_band_test_4[:, :, :, np.newaxis],X_band_test_5[:, :, :, np.newaxis]],axis=-1)\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis], X_band_test_2[:, :, :, np.newaxis],X_band_test_mean[:, :, :, np.newaxis]],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resize_shape = tuple(np.array(X_train.shape[1:3]) * 2)\n",
    "# X_train = np.array([cv2.resize(img, resize_shape) for img in X_train])\n",
    "# X_test = np.array([cv2.resize(img, resize_shape) for img in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 75, 75, 3) (1604,) (1604,) (1604,) (8424, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape, X_angle.shape, X_size_1.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    img_input = Input(shape=X_train.shape[1:], name=\"images\")\n",
    "    angle_input = Input(shape=[1], name=\"angle\")\n",
    "    \n",
    "    # ==================== flow1 ====================\n",
    "    flow1_x = img_input\n",
    "    # -------------------- block1 -------------------\n",
    "    flow1_shortcut1 = flow1_x\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='flow1_block1_conv1')(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='flow1_block1_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='flow1_block1_pool')(flow1_x)\n",
    "    # -------------------- shortcut1 -------------------\n",
    "    flow1_shortcut1 = Conv2D(256, (3, 3), strides=(2, 2), activation='relu', name='flow1_shortcut1')(flow1_shortcut1)\n",
    "    flow1_x = Add(name='flow1_shortcut1_add')([flow1_shortcut1, flow1_x])\n",
    "    flow1_x = Dropout(0.2)(flow1_x)\n",
    "    # -------------------- block2 -------------------\n",
    "    flow1_shortcut2 = flow1_x\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='flow1_block2_conv1')(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='flow1_block2_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='flow1_block2_pool')(flow1_x)\n",
    "    # -------------------- shortcut2 -------------------\n",
    "    flow1_shortcut2 = Conv2D(256, (3, 3), strides=(2, 2), activation='relu', name='flow1_shortcut2')(flow1_shortcut2)\n",
    "    flow1_x = Add(name='flow1_shortcut2_add')([flow1_shortcut2, flow1_x])\n",
    "    flow1_x = Dropout(0.2)(flow1_x)\n",
    "    # -------------------- block3 -------------------\n",
    "    flow1_shortcut3 = flow1_x\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='flow1_block3_conv1')(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='flow1_block3_conv2')(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='flow1_block3_conv3')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='flow1_block3_pool')(flow1_x)\n",
    "    # -------------------- shortcut3 -------------------\n",
    "    flow1_shortcut3 = Conv2D(256, (3, 3), strides=(2, 2), activation='relu', padding='same', name='flow1_shortcut3')(flow1_shortcut3)\n",
    "    flow1_x = Add(name='flow1_shortcut3_add')([flow1_shortcut3, flow1_x])\n",
    "    flow1_x = Dropout(0.2)(flow1_x)\n",
    "    # -------------------- block4 -------------------\n",
    "    flow1_shortcut4 = flow1_x\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='flow1_block4_conv1')(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='flow1_block4_conv2')(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='flow1_block4_conv3')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='flow1_block4_pool')(flow1_x)\n",
    "    # -------------------- shortcut4 -------------------\n",
    "    flow1_shortcut4 = Conv2D(512, (3, 3), strides=(2, 2), activation='relu', name='flow1_shortcut4')(flow1_shortcut4)\n",
    "    flow1_x = Add(name='flow1_shortcut4_add')([flow1_shortcut4, flow1_x])\n",
    "    flow1_x = Dropout(0.2)(flow1_x)\n",
    "    # -------------------- block5 -------------------\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='flow1_block5_conv1')(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='flow1_block5_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='flow1_block5_pool')(flow1_x)\n",
    "    # -------------------- block6 -------------------\n",
    "    flow1_x = GlobalAveragePooling2D(name='flow1_block6_global_avg')(flow1_x)\n",
    "    flow1_x = Dropout(0.1)(flow1_x)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid', name='predictions')(flow1_x)\n",
    "    model = Model(inputs=img_input, outputs=predictions)\n",
    "    optimizer = Adam(lr=1e-4)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "def getVgg19PlusModel():\n",
    "    img_input = Input(shape=X_train.shape[1:], name=\"images\")\n",
    "    angle_input = Input(shape=[1], name=\"angle\")\n",
    "    size_1 = Input(shape=[1], name=\"size_1\")\n",
    "    size_2 = Input(shape=[1], name=\"size_2\")\n",
    "    \n",
    "    # ==================== flow1 ====================\n",
    "    flow1_x = img_input\n",
    "    # -------------------- block1 -------------------\n",
    "    flow1_x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(flow1_x)\n",
    "    flow1_x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(flow1_x)\n",
    "#     flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(flow1_x)\n",
    "    # -------------------- block2 -------------------\n",
    "    flow1_x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(flow1_x)\n",
    "    flow1_x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(flow1_x)\n",
    "#     flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(flow1_x)\n",
    "    # -------------------- block3 -------------------\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(flow1_x)\n",
    "#     flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(flow1_x)\n",
    "    # -------------------- block4 -------------------\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(flow1_x)\n",
    "#     flow1_x = BatchNormalization()(flow1_x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(flow1_x)\n",
    "    # -------------------- block5 -------------------\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv4')(flow1_x)\n",
    "#     flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(flow1_x)\n",
    "    # -------------------- block6 -------------------\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block6_conv1')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block6_conv2')(flow1_x)\n",
    "    flow1_x = Dropout(0.1)(flow1_x)\n",
    "    \n",
    "    flow1_x = GlobalAveragePooling2D()(flow1_x)\n",
    "    flow1_x = Dropout(0.1)(flow1_x)\n",
    "    flow1_x = Concatenate()([flow1_x, size_1])\n",
    "    predictions = Dense(1, activation='sigmoid', name='predictions')(flow1_x)\n",
    "    model = Model(inputs=[img_input, size_1], outputs=predictions)\n",
    "    weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5', None, cache_subdir='models')\n",
    "    model.load_weights(weights_path, by_name=True)\n",
    "    optimizer = Adam(lr=1e-4)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "getModel = getVgg19PlusModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "images (InputLayer)              (None, 75, 75, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)            (None, 75, 75, 64)    1792        images[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)            (None, 75, 75, 64)    36928       block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)       (None, 37, 37, 64)    0           block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)            (None, 37, 37, 128)   73856       block1_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)            (None, 37, 37, 128)   147584      block2_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNor (None, 37, 37, 128)   512         block2_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, 18, 18, 128)   0           batch_normalization_46[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)            (None, 18, 18, 256)   295168      block2_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)            (None, 18, 18, 256)   590080      block3_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)            (None, 18, 18, 256)   590080      block3_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv4 (Conv2D)            (None, 18, 18, 256)   590080      block3_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNor (None, 18, 18, 256)   1024        block3_conv4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, 9, 9, 256)     0           batch_normalization_47[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)            (None, 9, 9, 512)     1180160     block3_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)            (None, 9, 9, 512)     2359808     block4_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)            (None, 9, 9, 512)     2359808     block4_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv4 (Conv2D)            (None, 9, 9, 512)     2359808     block4_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNor (None, 9, 9, 512)     2048        block4_conv4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)            (None, 9, 9, 512)     2359808     batch_normalization_48[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)            (None, 9, 9, 512)     2359808     block5_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)            (None, 9, 9, 512)     2359808     block5_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv4 (Conv2D)            (None, 9, 9, 512)     2359808     block5_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNor (None, 9, 9, 512)     2048        block5_conv4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)       (None, 4, 4, 512)     0           batch_normalization_49[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "block6_conv1 (Conv2D)            (None, 4, 4, 512)     2359808     block5_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNor (None, 4, 4, 512)     2048        block6_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block6_conv2 (Conv2D)            (None, 4, 4, 128)     589952      batch_normalization_50[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4, 4, 128)     0           block6_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_10 (Glo (None, 128)           0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 128)           0           global_average_pooling2d_10[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "size_1 (InputLayer)              (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)      (None, 129)           0           dropout_2[0][0]                  \n",
      "                                                                   size_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "predictions (Dense)              (None, 1)             130         concatenate_9[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 22,981,954\n",
      "Trainable params: 22,978,114\n",
      "Non-trainable params: 3,840\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = getModel()\n",
    "model.summary()\n",
    "plot_model(model, to_file=\"k_scale_net.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = True,\n",
    "                         width_shift_range = 0.1,\n",
    "                         height_shift_range = 0.1,\n",
    "                         channel_shift_range=0,\n",
    "                         zoom_range = 0.5,\n",
    "                         rotation_range = 10)\n",
    "\n",
    "def get_callbacks(filepath):\n",
    "    es = EarlyStopping('val_loss', patience=20, mode=\"min\")\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave, reduce_lr_loss]\n",
    "\n",
    "def flow_x1_x2_y(X1, X2, Y, batch_size, seed):\n",
    "    X1Y = gen.flow(X1, Y, batch_size=batch_size, seed=SEED)\n",
    "    X1X2 = gen.flow(X1, X2, batch_size=batch_size, seed=SEED)\n",
    "    while True:\n",
    "        X1, Y = X1Y.next()\n",
    "        _, X2 = X1X2.next()\n",
    "        yield [X1, X2], Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K=3\n",
    "epochs = 150\n",
    "batch_size = 32\n",
    "def Train_StratifiedKFold():\n",
    "    Kfolds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED).split(X_train, Y_train))\n",
    "    for j, (train_idx, test_idx) in enumerate(Kfolds):\n",
    "        print('\\n==========FOLD %s=========='% j)\n",
    "        Xtrain_cv = X_train[train_idx]\n",
    "        Ytrain_cv = Y_train[train_idx]\n",
    "        Xangle_cv = X_angle[train_idx]\n",
    "        Xsize1_cv = X_size_1[train_idx]\n",
    "\n",
    "        Xtrain_val = X_train[test_idx]\n",
    "        Ytrain_val = Y_train[test_idx]\n",
    "        Xangle_val = X_angle[test_idx]\n",
    "        Xsize1_val = X_size_1[test_idx]\n",
    "\n",
    "        Xtrain_input = [X_train, X_size_1]\n",
    "        Xval_input = [Xtrain_val, Xsize1_val]\n",
    "\n",
    "        model_file = 'k_scale_net_%s.hdf5' % j\n",
    "        model = getModel()\n",
    "\n",
    "        steps = np.ceil(len(Xtrain_cv) / batch_size) * 3\n",
    "        model.fit_generator(\n",
    "            flow_x1_x2_y(Xtrain_cv, Xsize1_cv, Ytrain_cv, batch_size=batch_size, seed=SEED), \n",
    "            steps_per_epoch=steps, epochs=epochs, verbose=1, shuffle=True, \n",
    "            callbacks=get_callbacks(model_file), validation_data=(Xval_input, Ytrain_val))\n",
    "\n",
    "        model.load_weights(filepath = model_file)    \n",
    "\n",
    "        score = model.evaluate(Xtrain_input, Y_train, verbose=1)\n",
    "        print('Train loss:', score[0])\n",
    "        print('Train accuracy:', score[1])\n",
    "        score = model.evaluate(Xval_input, Ytrain_val, verbose=1)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "def Train_KFold(j=0):\n",
    "    Xtrain_cv, Xtrain_val, Ytrain_cv, Ytrain_val, Xangle_cv, Xangle_val, Xsize1_cv, Xsize1_val = train_test_split(X_train, Y_train, X_angle, X_size_1, test_size=0.3, shuffle = True, random_state=SEED)\n",
    "    Xtrain_input = [X_train, X_size_1]\n",
    "    Xval_input = [Xtrain_val, Xsize1_val]\n",
    "\n",
    "    model_file = 'k_scale_net_%s.hdf5' % j\n",
    "    model = getModel()\n",
    "\n",
    "    steps = np.ceil(len(Xtrain_cv) / batch_size) * 3\n",
    "    model.fit_generator(\n",
    "        flow_x1_x2_y(Xtrain_cv, Xsize1_cv, Ytrain_cv, batch_size=batch_size, seed=SEED), \n",
    "        steps_per_epoch=steps, epochs=epochs, verbose=1, shuffle=True, \n",
    "        callbacks=get_callbacks(model_file), validation_data=(Xval_input, Ytrain_val))\n",
    "\n",
    "    model.load_weights(filepath = model_file)    \n",
    "\n",
    "    score = model.evaluate(Xtrain_input, Y_train, verbose=1)\n",
    "    print('Train loss:', score[0])\n",
    "    print('Train accuracy:', score[1])\n",
    "    score = model.evaluate(Xval_input, Ytrain_val, verbose=1)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "108/108 [==============================] - 29s - loss: 0.4031 - acc: 0.8160 - val_loss: 0.8293 - val_acc: 0.6577\n",
      "Epoch 2/150\n",
      "108/108 [==============================] - 20s - loss: 0.2926 - acc: 0.8773 - val_loss: 0.4859 - val_acc: 0.8029\n",
      "Epoch 3/150\n",
      "108/108 [==============================] - 20s - loss: 0.2892 - acc: 0.8739 - val_loss: 0.3076 - val_acc: 0.8465\n",
      "Epoch 4/150\n",
      "108/108 [==============================] - 20s - loss: 0.3244 - acc: 0.8592 - val_loss: 0.2748 - val_acc: 0.8838\n",
      "Epoch 5/150\n",
      "108/108 [==============================] - 19s - loss: 0.4059 - acc: 0.8233 - val_loss: 0.2810 - val_acc: 0.8651\n",
      "Epoch 6/150\n",
      "108/108 [==============================] - 19s - loss: 0.3983 - acc: 0.8282 - val_loss: 0.4748 - val_acc: 0.8340\n",
      "Epoch 7/150\n",
      "108/108 [==============================] - 19s - loss: 0.2975 - acc: 0.8788 - val_loss: 0.3579 - val_acc: 0.8527\n",
      "Epoch 8/150\n",
      "108/108 [==============================] - 19s - loss: 0.2688 - acc: 0.8932 - val_loss: 0.8762 - val_acc: 0.6473\n",
      "Epoch 9/150\n",
      "108/108 [==============================] - 20s - loss: 0.2873 - acc: 0.8774 - val_loss: 0.2467 - val_acc: 0.9046\n",
      "Epoch 10/150\n",
      "108/108 [==============================] - 19s - loss: 0.2974 - acc: 0.8777 - val_loss: 0.3326 - val_acc: 0.8506\n",
      "Epoch 11/150\n",
      "108/108 [==============================] - 19s - loss: 0.2640 - acc: 0.8932 - val_loss: 0.3294 - val_acc: 0.8714\n",
      "Epoch 12/150\n",
      "108/108 [==============================] - 19s - loss: 0.2247 - acc: 0.9045 - val_loss: 0.2588 - val_acc: 0.8963\n",
      "Epoch 13/150\n",
      "108/108 [==============================] - 19s - loss: 0.2362 - acc: 0.9011 - val_loss: 0.3731 - val_acc: 0.8195\n",
      "Epoch 14/150\n",
      "108/108 [==============================] - 19s - loss: 0.2176 - acc: 0.9132 - val_loss: 0.2917 - val_acc: 0.8797\n",
      "Epoch 15/150\n",
      "108/108 [==============================] - 19s - loss: 0.2204 - acc: 0.9112 - val_loss: 0.2765 - val_acc: 0.8921\n",
      "Epoch 16/150\n",
      "108/108 [==============================] - 19s - loss: 0.2248 - acc: 0.9126 - val_loss: 0.3116 - val_acc: 0.8693\n",
      "Epoch 17/150\n",
      "107/108 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9033\n",
      "Epoch 00016: reducing learning rate to 9.999999747378752e-06.\n",
      "108/108 [==============================] - 20s - loss: 0.2406 - acc: 0.8996 - val_loss: 0.2947 - val_acc: 0.8755\n",
      "Epoch 18/150\n",
      "108/108 [==============================] - 19s - loss: 0.1992 - acc: 0.9230 - val_loss: 0.2586 - val_acc: 0.9046\n",
      "Epoch 19/150\n",
      "108/108 [==============================] - 20s - loss: 0.2093 - acc: 0.9179 - val_loss: 0.2415 - val_acc: 0.9046\n",
      "Epoch 20/150\n",
      "108/108 [==============================] - 19s - loss: 0.2206 - acc: 0.9159 - val_loss: 0.2429 - val_acc: 0.9087\n",
      "Epoch 21/150\n",
      "108/108 [==============================] - 19s - loss: 0.1867 - acc: 0.9250 - val_loss: 0.2457 - val_acc: 0.9066\n",
      "Epoch 22/150\n",
      "108/108 [==============================] - 19s - loss: 0.1759 - acc: 0.9340 - val_loss: 0.2593 - val_acc: 0.9066\n",
      "Epoch 23/150\n",
      "108/108 [==============================] - 19s - loss: 0.1859 - acc: 0.9231 - val_loss: 0.2441 - val_acc: 0.9087\n",
      "Epoch 24/150\n",
      "108/108 [==============================] - 19s - loss: 0.1923 - acc: 0.9205 - val_loss: 0.2484 - val_acc: 0.9087\n",
      "Epoch 25/150\n",
      "108/108 [==============================] - 19s - loss: 0.1917 - acc: 0.9190 - val_loss: 0.2495 - val_acc: 0.9046\n",
      "Epoch 26/150\n",
      "108/108 [==============================] - 19s - loss: 0.1952 - acc: 0.9196 - val_loss: 0.2559 - val_acc: 0.9004\n",
      "Epoch 27/150\n",
      "107/108 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9165\n",
      "Epoch 00026: reducing learning rate to 9.999999747378752e-07.\n",
      "108/108 [==============================] - 19s - loss: 0.1989 - acc: 0.9173 - val_loss: 0.2590 - val_acc: 0.9046\n",
      "Epoch 28/150\n",
      "108/108 [==============================] - 19s - loss: 0.1766 - acc: 0.9294 - val_loss: 0.2471 - val_acc: 0.9066\n",
      "Epoch 29/150\n",
      "108/108 [==============================] - 19s - loss: 0.1726 - acc: 0.9314 - val_loss: 0.2453 - val_acc: 0.9066\n",
      "Epoch 30/150\n",
      "108/108 [==============================] - 19s - loss: 0.1808 - acc: 0.9251 - val_loss: 0.2441 - val_acc: 0.9066\n",
      "Epoch 31/150\n",
      "108/108 [==============================] - 19s - loss: 0.1759 - acc: 0.9312 - val_loss: 0.2445 - val_acc: 0.9046\n",
      "Epoch 32/150\n",
      "108/108 [==============================] - 19s - loss: 0.1657 - acc: 0.9363 - val_loss: 0.2479 - val_acc: 0.9066\n",
      "Epoch 33/150\n",
      "108/108 [==============================] - 19s - loss: 0.1844 - acc: 0.9260 - val_loss: 0.2465 - val_acc: 0.9066\n",
      "Epoch 34/150\n",
      "107/108 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9252\n",
      "Epoch 00033: reducing learning rate to 9.999999974752428e-08.\n",
      "108/108 [==============================] - 19s - loss: 0.1745 - acc: 0.9259 - val_loss: 0.2467 - val_acc: 0.9046\n",
      "Epoch 35/150\n",
      "108/108 [==============================] - 19s - loss: 0.1645 - acc: 0.9349 - val_loss: 0.2471 - val_acc: 0.9066\n",
      "Epoch 36/150\n",
      "108/108 [==============================] - 19s - loss: 0.1679 - acc: 0.9303 - val_loss: 0.2482 - val_acc: 0.9087\n",
      "Epoch 37/150\n",
      "108/108 [==============================] - 19s - loss: 0.1805 - acc: 0.9234 - val_loss: 0.2476 - val_acc: 0.9066\n",
      "Epoch 38/150\n",
      "108/108 [==============================] - 19s - loss: 0.2198 - acc: 0.9177 - val_loss: 0.2454 - val_acc: 0.9087\n",
      "Epoch 39/150\n",
      "108/108 [==============================] - 19s - loss: 0.1689 - acc: 0.9297 - val_loss: 0.2468 - val_acc: 0.9087\n",
      "Epoch 40/150\n",
      "108/108 [==============================] - 19s - loss: 0.1822 - acc: 0.9269 - val_loss: 0.2480 - val_acc: 0.9087\n",
      "1604/1604 [==============================] - 2s     \n",
      "Train loss: 0.16821044546\n",
      "Train accuracy: 0.936408977556\n",
      "482/482 [==============================] - 0s     \n",
      "Test loss: 0.241464865158\n",
      "Test accuracy: 0.904564315353\n"
     ]
    }
   ],
   "source": [
    "Train_KFold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_StratifiedKFold():\n",
    "    test_randround = 3\n",
    "    test_pred = 0\n",
    "    for j in range(K):\n",
    "        Xtest_input = [X_test, test_size_1]\n",
    "        model_file = 'k_scale_net_%s.hdf5' % j\n",
    "        model = getModel()\n",
    "        model.load_weights(filepath = model_file)    \n",
    "        for i in range(test_randround):\n",
    "            test_steps = np.ceil(X_test.shape[0] / batch_size)\n",
    "            test_pred += model.predict_generator(flow_x1_x2_y(*Xtest_input, range(X_test.shape[0]), batch_size=batch_size, seed=i), steps=test_steps, verbose=1).reshape(X_test.shape[0])\n",
    "            print(test_pred.shape)\n",
    "    test_pred /= K * test_randround\n",
    "\n",
    "def predict_KFold(j=0, K=1, test_randround=3):\n",
    "    test_pred = 0\n",
    "    for _ in range(K):\n",
    "        Xtest_input = [X_test, test_size_1]\n",
    "        model_file = 'k_scale_net_%s.hdf5' % j\n",
    "        model = getModel()\n",
    "        model.load_weights(filepath = model_file)    \n",
    "        for i in range(test_randround):\n",
    "            test_steps = np.ceil(X_test.shape[0] / batch_size)\n",
    "            test_pred += model.predict_generator(flow_x1_x2_y(*Xtest_input, range(X_test.shape[0]), batch_size=batch_size, seed=i), steps=test_steps, verbose=1).reshape(X_test.shape[0])\n",
    "            print(test_pred.shape)\n",
    "    test_pred /= K * test_randround\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/264 [============================>.] - ETA: 0s(8424,)\n"
     ]
    }
   ],
   "source": [
    "test_pred = predict_KFold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': test_pred})\n",
    "print(submission.count(), Xtest.shape[0])\n",
    "\n",
    "submission.to_csv('submission-k-scale-net.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
