{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import importlib\n",
    "SEED = 1234\n",
    "np.random.seed(SEED) \n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D, AveragePooling2D, Concatenate, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Add, GRUCell\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras import backend as KB\n",
    "from keras.engine import Layer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth = True\n",
    "keras.backend.tensorflow_backend.set_session(tf.Session(config=config)) \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import uniform_filter\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.losses import binary_crossentropy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%cd E:\\kaggle\\iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_img(band_1, band_2, is_iceberg, angle = None):\n",
    "    if angle is None:\n",
    "        title_str = 'Iceberg' if is_iceberg == 1 else 'Ship'\n",
    "    else:\n",
    "        title_str = 'Iceberg-' + str(angle) if is_iceberg == 1 else 'Ship-' + str(angle)\n",
    "    fig = plt.figure(0, figsize=(10,10))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    ax.set_title(title_str + ' - Band 1')\n",
    "    ax.imshow(band_1,cmap='jet')\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    ax.set_title(title_str + ' - Band 2')\n",
    "    ax.imshow(band_2,cmap='jet')\n",
    "    plt.show()\n",
    "\n",
    "# implement functions to convert SAR data from decibel units to linear units and back again\n",
    "def decibel_to_linear(band):\n",
    "     # convert to linear units\n",
    "    return np.power(10,np.array(band)/10)\n",
    "\n",
    "def linear_to_decibel(band):\n",
    "    return 10*np.log10(band)\n",
    "\n",
    "# implement the Lee Filter for a band in an image already reshaped into the proper dimensions\n",
    "def lee_filter(band, window, var_noise = 0.25):\n",
    "    # band: SAR data to be despeckled (already reshaped into image dimensions)\n",
    "    # window: descpeckling filter window (tuple)\n",
    "    # default noise variance = 0.25\n",
    "    # assumes noise mean = 0\n",
    "    \n",
    "    mean_window = uniform_filter(band, window)\n",
    "    mean_sqr_window = uniform_filter(band**2, window)\n",
    "    var_window = mean_sqr_window - mean_window**2\n",
    "\n",
    "    weights = var_window / (var_window + var_noise)\n",
    "    band_filtered = mean_window + weights*(band - mean_window)\n",
    "    return band_filtered\n",
    "\n",
    "def apply_lee_filter(band_1_linear, band_2_linear, window_var_index = 0, noise_var_index = 0):\n",
    "    windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "    noise_var = np.array([1, 2, 4])\n",
    "    noise_var_1 = np.round(np.var(band_1_linear) * noise_var, 10)\n",
    "    noise_var_2 = np.round(np.var(band_2_linear) * noise_var, 10)\n",
    "    band_1_linear_filtered = lee_filter(band_1_linear, windows[window_var_index], noise_var_1[noise_var_index])\n",
    "    band_2_linear_filtered = lee_filter(band_2_linear, windows[window_var_index], noise_var_2[noise_var_index])\n",
    "    return band_1_linear_filtered, band_2_linear_filtered\n",
    "\n",
    "def apply_lee_filter_single(band_linear, window_var_index = 0, noise_var_index = 0):\n",
    "    windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "    noise_var = np.array([1, 2, 4])\n",
    "    noise_var = np.round(np.var(band_linear) * noise_var, 10)\n",
    "    band_linear_filtered = lee_filter(band_linear, windows[window_var_index], noise_var[noise_var_index])\n",
    "    return band_linear_filtered\n",
    "\n",
    "def np_get_scaled_band(band_list):\n",
    "    imgs = []\n",
    "    for band in band_list:        \n",
    "        imgs.append((band - band.mean()) / band.std())\n",
    "    return np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(\"E:/kaggle/iceberg/train.json/data/processed/train.json\")\n",
    "Y_train = np.array(train['is_iceberg'])\n",
    "test = pd.read_json(\"E:/kaggle/iceberg/test.json/data/processed/test.json\")\n",
    "\n",
    "train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\n",
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "train['inc_angle']=train['inc_angle'].fillna(method='pad')\n",
    "test['inc_angle']=test['inc_angle'].fillna(method='pad')\n",
    "X_angle=train['inc_angle']\n",
    "X_test_angle=test['inc_angle']\n",
    "\n",
    "def iso(arr):\n",
    "    arr = np.reshape(arr, (75,75))\n",
    "    p = arr > (np.mean(arr) + 2 * np.std(arr))\n",
    "    return p * arr\n",
    "\n",
    "def size(arr):     \n",
    "    return float(np.sum(arr < -5)) / (75 * 75)\n",
    "\n",
    "train['iso_1'] = train.band_1.apply(iso)\n",
    "train['iso_2'] = train.band_2.apply(iso)\n",
    "train['size_1'] = train.iso_1.apply(size)\n",
    "train['size_2'] = train.iso_2.apply(size)\n",
    "X_size_1 = np.array(train['size_1'])\n",
    "X_size_2 = np.array(train['size_2'])\n",
    "\n",
    "test['iso_1'] = test.band_1.apply(iso)\n",
    "test['iso_2'] = test.band_2.apply(iso)\n",
    "test['size_1'] = test.iso_1.apply(size)\n",
    "test['size_2'] = test.iso_2.apply(size)\n",
    "test_size_1 = np.array(test['size_1'])\n",
    "test_size_2 = np.array(test['size_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the training data\n",
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "#apply filter\n",
    "X_band_1_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_1])\n",
    "X_band_2_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_2])\n",
    "X_band_1_filtered = linear_to_decibel(X_band_1_filtered)\n",
    "X_band_2_filtered = linear_to_decibel(X_band_2_filtered)\n",
    "X_band_1 = X_band_1_filtered\n",
    "X_band_2 = X_band_2_filtered\n",
    "X_band_mean = (X_band_1 + X_band_2) / 2\n",
    "# construct bands\n",
    "X_band_3=np.fabs(np.subtract(X_band_1,X_band_2))\n",
    "X_band_4=np.maximum(X_band_1,X_band_2)\n",
    "X_band_5=np.minimum(X_band_1,X_band_2)\n",
    "# subtract mean\n",
    "X_band_3 = np_get_scaled_band(X_band_3)\n",
    "X_band_4 = np_get_scaled_band(X_band_4)\n",
    "X_band_5 = np_get_scaled_band(X_band_5)\n",
    "\n",
    "X_train = np.concatenate([X_band_3[:, :, :, np.newaxis],X_band_4[:, :, :, np.newaxis],X_band_5[:, :, :, np.newaxis]], axis=-1)\n",
    "# X_train = np.concatenate([X_band_1[:, :, :, np.newaxis],X_band_2[:, :, :, np.newaxis],X_band_mean[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "#apply filter\n",
    "X_band_test_1_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_test_1])\n",
    "X_band_test_2_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_test_2])\n",
    "X_band_test_1_filtered = linear_to_decibel(X_band_test_1_filtered)\n",
    "X_band_test_2_filtered = linear_to_decibel(X_band_test_2_filtered)\n",
    "X_band_test_1 = X_band_test_1_filtered\n",
    "X_band_test_2 = X_band_test_2_filtered\n",
    "X_band_test_mean = (X_band_test_1 + X_band_test_2) / 2\n",
    "# construct bands\n",
    "X_band_test_3=np.fabs(np.subtract(X_band_test_1,X_band_test_2))\n",
    "X_band_test_4=np.maximum(X_band_test_1,X_band_test_2)\n",
    "X_band_test_5=np.minimum(X_band_test_1,X_band_test_2)\n",
    "# subtract mean\n",
    "X_band_test_3 = np_get_scaled_band(X_band_test_3)\n",
    "X_band_test_4 = np_get_scaled_band(X_band_test_4)\n",
    "X_band_test_5 = np_get_scaled_band(X_band_test_5)\n",
    "\n",
    "X_test = np.concatenate([X_band_test_3[:, :, :, np.newaxis], X_band_test_4[:, :, :, np.newaxis],X_band_test_5[:, :, :, np.newaxis]],axis=-1)\n",
    "# X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis], X_band_test_2[:, :, :, np.newaxis],X_band_test_mean[:, :, :, np.newaxis]],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resize_shape = tuple(np.array(X_train.shape[1:3]) * 2)\n",
    "# X_train = np.array([cv2.resize(img, resize_shape) for img in X_train])\n",
    "# X_test = np.array([cv2.resize(img, resize_shape) for img in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, Y_train.shape, X_angle.shape, X_size_1.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    img_input = Input(shape=X_train.shape[1:], name=\"images\")\n",
    "    angle_input = Input(shape=[1], name=\"angle\")\n",
    "    \n",
    "    # ==================== flow1 ====================\n",
    "    flow1_x = img_input\n",
    "    # -------------------- block1 -------------------\n",
    "    flow1_shortcut1 = flow1_x\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='flow1_block1_conv1')(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='flow1_block1_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='flow1_block1_pool')(flow1_x)\n",
    "    # -------------------- shortcut1 -------------------\n",
    "    flow1_shortcut1 = Conv2D(256, (3, 3), strides=(2, 2), activation='relu', name='flow1_shortcut1')(flow1_shortcut1)\n",
    "    flow1_x = Add(name='flow1_shortcut1_add')([flow1_shortcut1, flow1_x])\n",
    "    flow1_x = Dropout(0.2)(flow1_x)\n",
    "    # -------------------- block2 -------------------\n",
    "    flow1_shortcut2 = flow1_x\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='flow1_block2_conv1')(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='flow1_block2_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='flow1_block2_pool')(flow1_x)\n",
    "    # -------------------- shortcut2 -------------------\n",
    "    flow1_shortcut2 = Conv2D(256, (3, 3), strides=(2, 2), activation='relu', name='flow1_shortcut2')(flow1_shortcut2)\n",
    "    flow1_x = Add(name='flow1_shortcut2_add')([flow1_shortcut2, flow1_x])\n",
    "    flow1_x = Dropout(0.2)(flow1_x)\n",
    "    # -------------------- block3 -------------------\n",
    "    flow1_shortcut3 = flow1_x\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='flow1_block3_conv1')(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='flow1_block3_conv2')(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='flow1_block3_conv3')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='flow1_block3_pool')(flow1_x)\n",
    "    # -------------------- shortcut3 -------------------\n",
    "    flow1_shortcut3 = Conv2D(256, (3, 3), strides=(2, 2), activation='relu', padding='same', name='flow1_shortcut3')(flow1_shortcut3)\n",
    "    flow1_x = Add(name='flow1_shortcut3_add')([flow1_shortcut3, flow1_x])\n",
    "    flow1_x = Dropout(0.2)(flow1_x)\n",
    "    # -------------------- block4 -------------------\n",
    "    flow1_shortcut4 = flow1_x\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='flow1_block4_conv1')(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='flow1_block4_conv2')(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='flow1_block4_conv3')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='flow1_block4_pool')(flow1_x)\n",
    "    # -------------------- shortcut4 -------------------\n",
    "    flow1_shortcut4 = Conv2D(512, (3, 3), strides=(2, 2), activation='relu', name='flow1_shortcut4')(flow1_shortcut4)\n",
    "    flow1_x = Add(name='flow1_shortcut4_add')([flow1_shortcut4, flow1_x])\n",
    "    flow1_x = Dropout(0.2)(flow1_x)\n",
    "    # -------------------- block5 -------------------\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='flow1_block5_conv1')(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='flow1_block5_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='flow1_block5_pool')(flow1_x)\n",
    "    # -------------------- block6 -------------------\n",
    "    flow1_x = GlobalAveragePooling2D(name='flow1_block6_global_avg')(flow1_x)\n",
    "    flow1_x = Dropout(0.1)(flow1_x)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid', name='predictions')(flow1_x)\n",
    "    model = Model(inputs=img_input, outputs=predictions)\n",
    "    optimizer = Adam(lr=1e-4)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    Y_train_bak = Y_train\n",
    "    Y_train = OneHotEncoder().fit_transform(Y_train.reshape((-1, 1))).toarray()\n",
    "\n",
    "def getVgg19PlusModel():\n",
    "    img_input = Input(shape=X_train.shape[1:], name=\"images\")\n",
    "    angle_input = Input(shape=[1], name=\"angle\")\n",
    "    size_1 = Input(shape=[1], name=\"size_1\")\n",
    "    size_2 = Input(shape=[1], name=\"size_2\")\n",
    "    \n",
    "    # ==================== flow1 ====================\n",
    "    flow1_x = img_input\n",
    "    # -------------------- block1 -------------------\n",
    "    flow1_x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(flow1_x)\n",
    "    # -------------------- block2 -------------------\n",
    "    flow1_x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(flow1_x)\n",
    "    # -------------------- block3 -------------------\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(flow1_x)\n",
    "    # -------------------- block4 -------------------\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(flow1_x)\n",
    "    # -------------------- block5 -------------------\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv4')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(flow1_x)\n",
    "    # -------------------- block6 -------------------\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block6_conv1')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block6_conv2')(flow1_x)\n",
    "\n",
    "    \n",
    "    flow1_x = GlobalAveragePooling2D()(flow1_x)\n",
    "    flow1_x = Concatenate()([flow1_x, size_1])\n",
    "    predictions = Dense(2, activation='softmax', name='predictions')(flow1_x)\n",
    "    model = Model(inputs=[img_input, size_1], outputs=predictions)\n",
    "    weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5', None, cache_subdir='models')\n",
    "    model.load_weights(weights_path, by_name=True)\n",
    "    optimizer = Adam(lr=1e-4)\n",
    "    lg = binary_crossentropy\n",
    "    lg.__name__ = \"lg\"\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', lg])\n",
    "    \n",
    "    return model\n",
    "getModel = getVgg19PlusModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionRNNCell_Type1(GRUCell):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.init_args = args\n",
    "        self.init_kwargs = kwargs\n",
    "        if \"units\" in self.init_kwargs:\n",
    "            del self.init_kwargs[\"units\"]\n",
    "        \n",
    "    def init_context(self, ctx_block):\n",
    "        H, W, C = [int(i) for i in ctx_block.shape[1:]]\n",
    "        units = H * W\n",
    "        ctx_map = tf.reshape(tf.transpose(ctx_block, (0, 3, 1, 2)), (-1, C, units)) # (-1, C, units)\n",
    "        self.ctx_map = ctx_map\n",
    "        self.attention_units = C\n",
    "        super(AttentionRNNCell_Type1, self).__init__(units, *self.init_args, **self.init_kwargs)\n",
    "    \n",
    "    def build(self):\n",
    "        self.ctx_weight = self.add_weight(shape=(1, 1, self.units),\n",
    "                                      name='ctx_weight',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        self.ctx_bias = self.add_weight(shape=(1, 1, 1),\n",
    "                                      name='ctx_bias',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        self.state_weight = self.add_weight(shape=(1, self.units),\n",
    "                                      name='state_weight',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        self.state_bias = self.add_weight(shape=(1, 1),\n",
    "                                      name='state_bias',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        self.transform_weight = self.add_weight(shape=(1, 1, self.units),\n",
    "                                      name='transform_weight',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        self.transform_bias = self.add_weight(shape=(1, 1, 1),\n",
    "                                      name='transform_bias',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        self.initial_ctx_weight = tf.ones((1, 1, self.units))\n",
    "        super(AttentionRNNCell_Type1, self).build((None, self.units))\n",
    "    \n",
    "    def call(self, inputs, states, time_step, training=None):\n",
    "        h_tm1 = states[0]\n",
    "        ctx_weight = tf.where(tf.equal(time_step, 0), self.initial_ctx_weight, self.ctx_weight)\n",
    "        ctx_attention_term = self.ctx_map * ctx_weight + self.ctx_bias # shape: (-1, C, units)\n",
    "        state_guidance_term = tf.reshape(h_tm1 * self.state_weight + self.state_bias, (-1, 1, self.units)) # shape: (-1, 1, units) \n",
    "        g = KB.tanh(ctx_attention_term + state_guidance_term) # shape: (-1, C, units)\n",
    "        alpha = KB.softmax(tf.reduce_sum(g * self.transform_weight + self.transform_bias, axis=2)) # shape: (-1, C)\n",
    "        alpha = KB.expand_dims(alpha)# shape: (-1, C, 1)\n",
    "        attention_c = tf.reduce_sum(self.ctx_map * alpha, axis=1) # shape: (-1, units)\n",
    "        return super(AttentionRNNCell_Type1, self).call(attention_c, states, training=None)\n",
    "\n",
    "class AttentionRNN(Layer):\n",
    "    def __init__(self, cell_cls, steps, *args, **kwargs):\n",
    "        super(AttentionRNN, self).__init__(**kwargs)\n",
    "        self.cell_cls = cell_cls\n",
    "        self.cell = cell_cls()\n",
    "        self.steps = steps\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super(AttentionRNN, self).build(input_shape)\n",
    "    \n",
    "    def call(self, context):\n",
    "        # initialize cell\n",
    "        self.cell.init_context(context)\n",
    "        self.units = self.cell.units\n",
    "        if not self.cell.built:\n",
    "            self.cell.build()\n",
    "        # initial states\n",
    "        initial_states = KB.zeros_like(context) # (sample, H, W, C)\n",
    "        initial_states = KB.sum(initial_states, axis=(1, 2, 3)) # (sample, )\n",
    "        initial_states = KB.expand_dims(initial_states)  # (samples, 1)\n",
    "        initial_states = KB.tile(initial_states, (1, self.units))\n",
    "        initial_states = [initial_states]\n",
    "        # basic params and functions\n",
    "        time = tf.constant(0, dtype='int32', name='time')\n",
    "        def step_function(states, time_step):\n",
    "            return self.cell.call(None, states, time_step)\n",
    "        def _step(time_step, output_ta, states):\n",
    "            output, states = step_function(states, time_step)\n",
    "            output_ta = output_ta.write(time_step, output)\n",
    "            return time_step + 1, output_ta, states\n",
    "        outputs, _ = step_function(initial_states, time)\n",
    "        output_ta = tensor_array_ops.TensorArray(dtype=outputs.dtype, size=self.steps, tensor_array_name='output_ta')\n",
    "        # while loop\n",
    "        final_outputs = tf.while_loop(lambda time, *_: time < self.steps, _step, [time, output_ta, initial_states], parallel_iterations=32, swap_memory=True)\n",
    "        last_time = final_outputs[0]\n",
    "        output_ta = final_outputs[1]\n",
    "        new_states = final_outputs[2]\n",
    "        # deal with outputs\n",
    "        outputs = output_ta.stack() # time_step, batch_size, units\n",
    "        outputs = tf.transpose(outputs, (1, 0, 2)) # batch_size, time_step, units\n",
    "        outputs = tf.reshape(outputs, (-1, self.cell.units * self.steps)) # batch_size, time_step * units\n",
    "        # deal with last output\n",
    "        last_output = output_ta.read(last_time - 1)\n",
    "        return outputs\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.cell is None or not self.cell.built:\n",
    "            raise Exception(\"Call compute_output_shape after rnn cell are built(after invoking __call__).\")\n",
    "        return (input_shape[0], self.cell.units * self.steps)\n",
    "\n",
    "def getVgg16RecurrentModel():\n",
    "    img_input = Input(shape=X_train.shape[1:], name=\"images\")\n",
    "    angle_input = Input(shape=[1], name=\"angle\")\n",
    "    size_1 = Input(shape=[1], name=\"size_1\")\n",
    "    size_2 = Input(shape=[1], name=\"size_2\")\n",
    "    \n",
    "    # ==================== flow1 ====================\n",
    "    flow1_x = img_input\n",
    "    # -------------------- block1 -------------------\n",
    "    flow1_x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(flow1_x)\n",
    "    # -------------------- block2 -------------------\n",
    "    flow1_x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(flow1_x)\n",
    "    # -------------------- block3 -------------------\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(flow1_x)\n",
    "    # -------------------- block4 -------------------\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(flow1_x)\n",
    "    # -------------------- block5 -------------------\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(flow1_x)\n",
    "    flow1_x = BatchNormalization()(flow1_x)\n",
    "    flow1_x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(flow1_x)\n",
    "    flow1_x = AttentionRNN(AttentionRNNCell_Type1, 10)(flow1_x)\n",
    "\n",
    "#     flow1_x = Concatenate()([flow1_x, size_1])\n",
    "    predictions = Dense(1, activation='sigmoid', name='predictions')(flow1_x)\n",
    "    model = Model(inputs=img_input, outputs=predictions)\n",
    "    optimizer = Adam(lr=1e-4)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "getModel = getVgg16RecurrentModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = getModel()\n",
    "model.summary()\n",
    "plot_model(model, to_file=\"k_scale_net.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = True,\n",
    "                         width_shift_range = 0.0,\n",
    "                         height_shift_range = 0.0,\n",
    "                         channel_shift_range=0,\n",
    "                         zoom_range = 0.5,\n",
    "                         rotation_range = 10)\n",
    "\n",
    "def get_callbacks(filepath):\n",
    "    es = EarlyStopping('val_loss', patience=20, mode=\"min\")\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave, reduce_lr_loss]\n",
    "\n",
    "def flow_x1_x2_y(X1, X2, Y, batch_size, seed):\n",
    "    X1Y = gen.flow(X1, Y, batch_size=batch_size, seed=SEED)\n",
    "    X1X2 = gen.flow(X1, X2, batch_size=batch_size, seed=SEED)\n",
    "    while True:\n",
    "        X1, Y = X1Y.next()\n",
    "        _, X2 = X1X2.next()\n",
    "        yield [X1, X2], Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K=3\n",
    "epochs = 150\n",
    "batch_size = 32\n",
    "def Train_StratifiedKFold():\n",
    "    Kfolds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED).split(X_train, Y_train))\n",
    "    for j, (train_idx, test_idx) in enumerate(Kfolds):\n",
    "        print('\\n==========FOLD %s=========='% j)\n",
    "        Xtrain_cv = X_train[train_idx]\n",
    "        Ytrain_cv = Y_train[train_idx]\n",
    "        Xangle_cv = X_angle[train_idx]\n",
    "        Xsize1_cv = X_size_1[train_idx]\n",
    "\n",
    "        Xtrain_val = X_train[test_idx]\n",
    "        Ytrain_val = Y_train[test_idx]\n",
    "        Xangle_val = X_angle[test_idx]\n",
    "        Xsize1_val = X_size_1[test_idx]\n",
    "\n",
    "        Xtrain_input = [X_train, X_size_1]\n",
    "        Xval_input = [Xtrain_val, Xsize1_val]\n",
    "\n",
    "        model_file = 'k_scale_net_%s.hdf5' % j\n",
    "        model = getModel()\n",
    "\n",
    "        steps = np.ceil(len(Xtrain_cv) / batch_size) * 3\n",
    "        model.fit_generator(\n",
    "            flow_x1_x2_y(Xtrain_cv, Xsize1_cv, Ytrain_cv, batch_size=batch_size, seed=SEED), \n",
    "            steps_per_epoch=steps, epochs=epochs, verbose=1, shuffle=True, \n",
    "            callbacks=get_callbacks(model_file), validation_data=(Xval_input, Ytrain_val))\n",
    "\n",
    "        model.load_weights(filepath = model_file)    \n",
    "\n",
    "        score = model.evaluate(Xtrain_input, Y_train, verbose=1)\n",
    "        print('Train loss:', score[0])\n",
    "        print('Train accuracy:', score[1])\n",
    "        score = model.evaluate(Xval_input, Ytrain_val, verbose=1)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "def Train_KFold(j=0):\n",
    "    Xtrain_cv, Xtrain_val, Ytrain_cv, Ytrain_val, Xangle_cv, Xangle_val, Xsize1_cv, Xsize1_val = train_test_split(X_train, Y_train, X_angle, X_size_1, test_size=0.3, shuffle = True, random_state=SEED)\n",
    "    Xtrain_input = X_train # [X_train, X_size_1]\n",
    "    Xval_input = Xtrain_val # [Xtrain_val, Xsize1_val]\n",
    "\n",
    "    model_file = 'k_scale_net_%s.hdf5' % j\n",
    "    model = getModel()\n",
    "\n",
    "    steps = np.ceil(len(Xtrain_cv) / batch_size) * 3\n",
    "    model.fit_generator(\n",
    "        gen.flow(Xtrain_cv, Ytrain_cv, batch_size=batch_size, seed=SEED),# flow_x1_x2_y(Xtrain_cv, Xsize1_cv, Ytrain_cv, batch_size=batch_size, seed=SEED), \n",
    "        steps_per_epoch=steps, epochs=epochs, verbose=1, shuffle=True, \n",
    "        callbacks=get_callbacks(model_file), validation_data=(Xval_input, Ytrain_val))\n",
    "\n",
    "    model.load_weights(filepath = model_file)    \n",
    "\n",
    "    score = model.evaluate(Xtrain_input, Y_train, verbose=1)\n",
    "    print('Train loss:', score[0])\n",
    "    print('Train accuracy:', score[1])\n",
    "    score = model.evaluate(Xval_input, Ytrain_val, verbose=1)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Train_KFold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_StratifiedKFold():\n",
    "    test_randround = 3\n",
    "    test_pred = 0\n",
    "    for j in range(K):\n",
    "        Xtest_input = [X_test, test_size_1]\n",
    "        model_file = 'k_scale_net_%s.hdf5' % j\n",
    "        model = getModel()\n",
    "        model.load_weights(filepath = model_file)    \n",
    "        for i in range(test_randround):\n",
    "            test_steps = np.ceil(X_test.shape[0] / batch_size)\n",
    "            test_pred += model.predict_generator(flow_x1_x2_y(*Xtest_input, range(X_test.shape[0]), batch_size=batch_size, seed=i), steps=test_steps, verbose=1).reshape(X_test.shape[0])\n",
    "            print(test_pred.shape)\n",
    "    test_pred /= K * test_randround\n",
    "\n",
    "def predict_KFold(j=0, K=1, test_randround=3):\n",
    "    test_pred = 0\n",
    "    for _ in range(K):\n",
    "        Xtest_input = X_test # [X_test, test_size_1]\n",
    "        model_file = 'k_scale_net_%s.hdf5' % j\n",
    "        model = getModel()\n",
    "        model.load_weights(filepath = model_file)    \n",
    "        for i in range(test_randround):\n",
    "            test_steps = np.ceil(X_test.shape[0] / batch_size)\n",
    "            test_pred += model.predict_generator(flow_x1_x2_y(*Xtest_input, range(X_test.shape[0]), batch_size=batch_size, seed=i), steps=test_steps, verbose=1).reshape(X_test.shape[0])\n",
    "            print(test_pred.shape)\n",
    "    test_pred /= K * test_randround\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = predict_KFold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': test_pred})\n",
    "print(submission.count(), Xtest.shape[0])\n",
    "\n",
    "submission.to_csv('submission-k-scale-net.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
