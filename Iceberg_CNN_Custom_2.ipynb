{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import importlib\n",
    "SEED = 1234\n",
    "np.random.seed(SEED) \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D, AveragePooling2D, Concatenate, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import mylibs.ResNet as ResNet\n",
    "import mylibs.SENet as SENet\n",
    "importlib.reload(ResNet)\n",
    "importlib.reload(SENet)\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import uniform_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras.backend.tensorflow_backend import set_session  \n",
    "# config = tf.ConfigProto()  \n",
    "# config.gpu_options.allow_growth = True\n",
    "# set_session(tf.Session(config=config)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\kaggle\\iceberg\n"
     ]
    }
   ],
   "source": [
    "%cd E:\\kaggle\\iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scaled_imgs(df):\n",
    "    imgs = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        #make 75x75 image\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 + band_2 # plus since log(x*y) = log(x) + log(y)\n",
    "        \n",
    "        # Rescale\n",
    "        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
    "        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "\n",
    "        imgs.append(np.dstack((a, b, c)))\n",
    "\n",
    "    return np.array(imgs)\n",
    "\n",
    "def get_more_images(imgs):\n",
    "    more_images = []\n",
    "    vert_flip_imgs = []\n",
    "    hori_flip_imgs = []\n",
    "    vh_flip_imgs = []\n",
    "      \n",
    "    for i in range(0,imgs.shape[0]):\n",
    "        vert_flip_imgs.append(cv2.flip(imgs[i], 1))\n",
    "        hori_flip_imgs.append(cv2.flip(imgs[i], 0))\n",
    "        vh_flip_imgs.append(cv2.flip(imgs[i], -1))\n",
    "      \n",
    "    v = np.array(vert_flip_imgs)\n",
    "    h = np.array(hori_flip_imgs)\n",
    "    vh = np.array(vh_flip_imgs)\n",
    "       \n",
    "    more_images = np.concatenate((imgs,v,h, vh))\n",
    "    \n",
    "    return more_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_img(band_1, band_2, is_iceberg, angle = None):\n",
    "    if angle is None:\n",
    "        title_str = 'Iceberg' if is_iceberg == 1 else 'Ship'\n",
    "    else:\n",
    "        title_str = 'Iceberg-' + str(angle) if is_iceberg == 1 else 'Ship-' + str(angle)\n",
    "    fig = plt.figure(0, figsize=(10,10))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    ax.set_title(title_str + ' - Band 1')\n",
    "    ax.imshow(band_1,cmap='jet')\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    ax.set_title(title_str + ' - Band 2')\n",
    "    ax.imshow(band_2,cmap='jet')\n",
    "    plt.show()\n",
    "\n",
    "# implement functions to convert SAR data from decibel units to linear units and back again\n",
    "def decibel_to_linear(band):\n",
    "     # convert to linear units\n",
    "    return np.power(10,np.array(band)/10)\n",
    "\n",
    "def linear_to_decibel(band):\n",
    "    return 10*np.log10(band)\n",
    "\n",
    "# implement the Lee Filter for a band in an image already reshaped into the proper dimensions\n",
    "def lee_filter(band, window, var_noise = 0.25):\n",
    "    # band: SAR data to be despeckled (already reshaped into image dimensions)\n",
    "    # window: descpeckling filter window (tuple)\n",
    "    # default noise variance = 0.25\n",
    "    # assumes noise mean = 0\n",
    "    \n",
    "    mean_window = uniform_filter(band, window)\n",
    "    mean_sqr_window = uniform_filter(band**2, window)\n",
    "    var_window = mean_sqr_window - mean_window**2\n",
    "\n",
    "    weights = var_window / (var_window + var_noise)\n",
    "    band_filtered = mean_window + weights*(band - mean_window)\n",
    "    return band_filtered\n",
    "\n",
    "def apply_lee_filter(band_1_linear, band_2_linear, window_var_index = 0, noise_var_index = 0):\n",
    "    windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "    noise_var = np.array([1, 2, 4])\n",
    "    noise_var_1 = np.round(np.var(band_1_linear) * noise_var, 10)\n",
    "    noise_var_2 = np.round(np.var(band_2_linear) * noise_var, 10)\n",
    "    band_1_linear_filtered = lee_filter(band_1_linear, windows[window_var_index], noise_var_1[noise_var_index])\n",
    "    band_2_linear_filtered = lee_filter(band_2_linear, windows[window_var_index], noise_var_2[noise_var_index])\n",
    "    return band_1_linear_filtered, band_2_linear_filtered\n",
    "\n",
    "def apply_lee_filter_single(band_linear, window_var_index = 0, noise_var_index = 0):\n",
    "    windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "    noise_var = np.array([1, 2, 4])\n",
    "    noise_var = np.round(np.var(band_linear) * noise_var, 10)\n",
    "    band_linear_filtered = lee_filter(band_linear, windows[window_var_index], noise_var[noise_var_index])\n",
    "    return band_linear_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_custom_augmentation = False\n",
    "if use_custom_augmentation:\n",
    "    df_train = pd.read_json('E:/kaggle/iceberg/train.json/data/processed/train.json')\n",
    "    df_test = pd.read_json('E:/kaggle/iceberg/test.json/data/processed/test.json')\n",
    "    Xtrain = get_scaled_imgs(df_train)\n",
    "    Xtest = get_scaled_imgs(df_test)\n",
    "    Ytrain = np.array(df_train['is_iceberg'])\n",
    "    \n",
    "    df_train[\"inc_angle\"] = df_train[\"inc_angle\"].replace('na',0)\n",
    "    df_test[\"inc_angle\"] = df_test[\"inc_angle\"].replace('na',0)\n",
    "    idx_tr = np.where(df_train[\"inc_angle\"]>0)\n",
    "    Xtrain = Xtrain[idx_tr[0]]\n",
    "    Ytrain = Ytrain[idx_tr[0]]\n",
    "    \n",
    "    Xtrain = get_more_images(Xtrain) \n",
    "    Ytrain = np.concatenate((Ytrain,Ytrain,Ytrain, Ytrain))\n",
    "else:\n",
    "    train = pd.read_json(\"E:/kaggle/iceberg/train.json/data/processed/train.json\")\n",
    "    target_train=train['is_iceberg']\n",
    "    test = pd.read_json(\"E:/kaggle/iceberg/test.json/data/processed/test.json\")\n",
    "    \n",
    "    train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\n",
    "    test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "    train['inc_angle']=train['inc_angle'].fillna(method='pad')\n",
    "    test['inc_angle']=test['inc_angle'].fillna(method='pad')\n",
    "    X_angle=train['inc_angle']\n",
    "    X_test_angle=test['inc_angle']\n",
    "    \n",
    "    #Generate the training data\n",
    "    X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "    X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "    #apply filter\n",
    "    X_band_1_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_1])\n",
    "    X_band_2_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_2])\n",
    "    X_band_1_filtered = linear_to_decibel(X_band_1_filtered)\n",
    "    X_band_2_filtered = linear_to_decibel(X_band_2_filtered)\n",
    "    X_band_1 = X_band_1_filtered\n",
    "    X_band_2 = X_band_2_filtered\n",
    "\n",
    "    X_band_3=np.fabs(np.subtract(X_band_1,X_band_2))\n",
    "    X_band_4=np.maximum(X_band_1,X_band_2)\n",
    "    X_band_5=np.minimum(X_band_1,X_band_2)\n",
    "    X_train = np.concatenate([X_band_3[:, :, :, np.newaxis],X_band_4[:, :, :, np.newaxis],X_band_5[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "    X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "    X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "    #apply filter\n",
    "    X_band_test_1_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_test_1])\n",
    "    X_band_test_2_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_test_2])\n",
    "    X_band_test_1_filtered = linear_to_decibel(X_band_test_1_filtered)\n",
    "    X_band_test_2_filtered = linear_to_decibel(X_band_test_2_filtered)\n",
    "    X_band_test_1 = X_band_test_1_filtered\n",
    "    X_band_test_2 = X_band_test_2_filtered\n",
    "\n",
    "    X_band_test_3=np.fabs(np.subtract(X_band_test_1,X_band_test_2))\n",
    "    X_band_test_4=np.maximum(X_band_test_1,X_band_test_2)\n",
    "    X_band_test_5=np.minimum(X_band_test_1,X_band_test_2)\n",
    "    X_test = np.concatenate([X_band_test_3[:, :, :, np.newaxis], X_band_test_4[:, :, :, np.newaxis],X_band_test_5[:, :, :, np.newaxis]],axis=-1)\n",
    "    \n",
    "    X_train = get_more_images(X_train)\n",
    "    target_train = np.concatenate((target_train, target_train, target_train, target_train))\n",
    "    X_angle = np.concatenate((X_angle, X_angle, X_angle, X_angle))\n",
    "    \n",
    "    Xtrain = X_train\n",
    "    Ytrain = target_train\n",
    "    Xtest = X_test\n",
    "    Xangle = X_angle\n",
    "    Xangle_test = X_test_angle\n",
    "    df_train = train\n",
    "    df_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6416, 75, 75, 3) (6416,) (6416,) (8424, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape, Ytrain.shape, Xangle.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEtCAYAAAAsgeXEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ba22182828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_img(X_band_1[0], X_band_2[0], target_train[0], X_angle[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VGG16, ResNet50, SENet50\n",
    "baseModelName = \"ResNet50\"\n",
    "useAngle = False\n",
    "def getModel(baseModelName):\n",
    "    global useAngle\n",
    "    angle_input = Input(shape=[1], name=\"angle\")\n",
    "    angle_layer = Dense(1)(angle_input)\n",
    "    if baseModelName == \"VGG16\":\n",
    "        baseModel = VGG16(weights='imagenet', include_top=False, input_shape=Xtrain.shape[1:], pooling = \"avg\")\n",
    "        cnnOutput = baseModel.output\n",
    "    elif baseModelName == \"ResNet50\":\n",
    "        baseModel = ResNet.ResNet50(weights='imagenet', include_top=False, input_shape=Xtrain.shape[1:], pooling = \"avg\")\n",
    "        cnnOutput = baseModel.output\n",
    "#         cnnOutput = baseModel.get_layer(\"final\").output\n",
    "#         cnnOutput = AveragePooling2D((3, 3), name='avg_pool')(cnnOutput)\n",
    "#         cnnOutput = GlobalAveragePooling2D(name = \"global_avg_pool\")(cnnOutput)\n",
    "    elif baseModelName == \"SENet50\":\n",
    "        baseModel = SENet.SENet50(weights='imagenet', include_top=False, input_shape=Xtrain.shape[1:], pooling = \"avg\",\n",
    "#                                   kernel_regularizer = \"l2\",\n",
    "#                                   bias_regularizer = \"l2\",\n",
    "#                                   activity_regularizer = None,\n",
    "#                                   regularizer_value = 1e-4\n",
    "                                 )\n",
    "#         cnnOutput = baseModel.output\n",
    "        cnnOutput = baseModel.get_layer(\"final\").output\n",
    "        cnnOutput = AveragePooling2D((3, 3), name='avg_pool')(cnnOutput)\n",
    "        cnnOutput = GlobalAveragePooling2D(name = \"global_avg_pool\")(cnnOutput)\n",
    "\n",
    "    if baseModelName == \"VGG16\":\n",
    "        fcOutput = Dropout(0.6)(cnnOutput)\n",
    "        predictions = Dense(1, activation=\"sigmoid\")(fcOutput)\n",
    "        model = Model(inputs=baseModel.input, outputs=predictions)\n",
    "    elif baseModelName == \"ResNet50\":\n",
    "        useAngle = True\n",
    "        predictions = Dense(1, activation=\"sigmoid\")(Concatenate()([Dropout(0.6)(cnnOutput), angle_layer]))\n",
    "        model = Model(inputs=[baseModel.input, angle_input], outputs=predictions)\n",
    "    elif baseModelName == \"SENet50\":\n",
    "        predictions = Dense(1, activation=\"sigmoid\")(cnnOutput)\n",
    "        model = Model(inputs=baseModel.input, outputs=predictions)\n",
    "    \n",
    "    optimizer = Adam(lr=1e-4)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_7 (InputLayer)             (None, 75, 75, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, 38, 38, 64)    9472        input_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)    (None, 38, 38, 64)    256         conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_289 (Activation)      (None, 38, 38, 64)    0           bn_conv1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)   (None, 18, 18, 64)    0           activation_289[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)          (None, 18, 18, 64)    4160        max_pooling2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizatio (None, 18, 18, 64)    256         res2a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_290 (Activation)      (None, 18, 18, 64)    0           bn2a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)          (None, 18, 18, 64)    36928       activation_290[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizatio (None, 18, 18, 64)    256         res2a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_291 (Activation)      (None, 18, 18, 64)    0           bn2a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)          (None, 18, 18, 256)   16640       activation_291[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)           (None, 18, 18, 256)   16640       max_pooling2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizatio (None, 18, 18, 256)   1024        res2a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalization (None, 18, 18, 256)   1024        res2a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_97 (Add)                     (None, 18, 18, 256)   0           bn2a_branch2c[0][0]              \n",
      "                                                                   bn2a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_292 (Activation)      (None, 18, 18, 256)   0           add_97[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)          (None, 18, 18, 64)    16448       activation_292[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizatio (None, 18, 18, 64)    256         res2b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_293 (Activation)      (None, 18, 18, 64)    0           bn2b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)          (None, 18, 18, 64)    36928       activation_293[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizatio (None, 18, 18, 64)    256         res2b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_294 (Activation)      (None, 18, 18, 64)    0           bn2b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)          (None, 18, 18, 256)   16640       activation_294[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizatio (None, 18, 18, 256)   1024        res2b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_98 (Add)                     (None, 18, 18, 256)   0           bn2b_branch2c[0][0]              \n",
      "                                                                   activation_292[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_295 (Activation)      (None, 18, 18, 256)   0           add_98[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)          (None, 18, 18, 64)    16448       activation_295[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizatio (None, 18, 18, 64)    256         res2c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_296 (Activation)      (None, 18, 18, 64)    0           bn2c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)          (None, 18, 18, 64)    36928       activation_296[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizatio (None, 18, 18, 64)    256         res2c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_297 (Activation)      (None, 18, 18, 64)    0           bn2c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)          (None, 18, 18, 256)   16640       activation_297[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizatio (None, 18, 18, 256)   1024        res2c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_99 (Add)                     (None, 18, 18, 256)   0           bn2c_branch2c[0][0]              \n",
      "                                                                   activation_295[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_298 (Activation)      (None, 18, 18, 256)   0           add_99[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)          (None, 9, 9, 128)     32896       activation_298[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizatio (None, 9, 9, 128)     512         res3a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_299 (Activation)      (None, 9, 9, 128)     0           bn3a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)          (None, 9, 9, 128)     147584      activation_299[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizatio (None, 9, 9, 128)     512         res3a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_300 (Activation)      (None, 9, 9, 128)     0           bn3a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)          (None, 9, 9, 512)     66048       activation_300[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)           (None, 9, 9, 512)     131584      activation_298[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizatio (None, 9, 9, 512)     2048        res3a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalization (None, 9, 9, 512)     2048        res3a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_100 (Add)                    (None, 9, 9, 512)     0           bn3a_branch2c[0][0]              \n",
      "                                                                   bn3a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_301 (Activation)      (None, 9, 9, 512)     0           add_100[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)          (None, 9, 9, 128)     65664       activation_301[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizatio (None, 9, 9, 128)     512         res3b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_302 (Activation)      (None, 9, 9, 128)     0           bn3b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)          (None, 9, 9, 128)     147584      activation_302[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizatio (None, 9, 9, 128)     512         res3b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_303 (Activation)      (None, 9, 9, 128)     0           bn3b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)          (None, 9, 9, 512)     66048       activation_303[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizatio (None, 9, 9, 512)     2048        res3b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_101 (Add)                    (None, 9, 9, 512)     0           bn3b_branch2c[0][0]              \n",
      "                                                                   activation_301[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_304 (Activation)      (None, 9, 9, 512)     0           add_101[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)          (None, 9, 9, 128)     65664       activation_304[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizatio (None, 9, 9, 128)     512         res3c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_305 (Activation)      (None, 9, 9, 128)     0           bn3c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)          (None, 9, 9, 128)     147584      activation_305[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizatio (None, 9, 9, 128)     512         res3c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_306 (Activation)      (None, 9, 9, 128)     0           bn3c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)          (None, 9, 9, 512)     66048       activation_306[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizatio (None, 9, 9, 512)     2048        res3c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_102 (Add)                    (None, 9, 9, 512)     0           bn3c_branch2c[0][0]              \n",
      "                                                                   activation_304[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_307 (Activation)      (None, 9, 9, 512)     0           add_102[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)          (None, 9, 9, 128)     65664       activation_307[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizatio (None, 9, 9, 128)     512         res3d_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_308 (Activation)      (None, 9, 9, 128)     0           bn3d_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)          (None, 9, 9, 128)     147584      activation_308[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizatio (None, 9, 9, 128)     512         res3d_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_309 (Activation)      (None, 9, 9, 128)     0           bn3d_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)          (None, 9, 9, 512)     66048       activation_309[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizatio (None, 9, 9, 512)     2048        res3d_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_103 (Add)                    (None, 9, 9, 512)     0           bn3d_branch2c[0][0]              \n",
      "                                                                   activation_307[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_310 (Activation)      (None, 9, 9, 512)     0           add_103[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)          (None, 5, 5, 256)     131328      activation_310[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizatio (None, 5, 5, 256)     1024        res4a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_311 (Activation)      (None, 5, 5, 256)     0           bn4a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)          (None, 5, 5, 256)     590080      activation_311[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizatio (None, 5, 5, 256)     1024        res4a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_312 (Activation)      (None, 5, 5, 256)     0           bn4a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)          (None, 5, 5, 1024)    263168      activation_312[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)           (None, 5, 5, 1024)    525312      activation_310[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizatio (None, 5, 5, 1024)    4096        res4a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalization (None, 5, 5, 1024)    4096        res4a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_104 (Add)                    (None, 5, 5, 1024)    0           bn4a_branch2c[0][0]              \n",
      "                                                                   bn4a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_313 (Activation)      (None, 5, 5, 1024)    0           add_104[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)          (None, 5, 5, 256)     262400      activation_313[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizatio (None, 5, 5, 256)     1024        res4b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_314 (Activation)      (None, 5, 5, 256)     0           bn4b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)          (None, 5, 5, 256)     590080      activation_314[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizatio (None, 5, 5, 256)     1024        res4b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_315 (Activation)      (None, 5, 5, 256)     0           bn4b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)          (None, 5, 5, 1024)    263168      activation_315[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizatio (None, 5, 5, 1024)    4096        res4b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_105 (Add)                    (None, 5, 5, 1024)    0           bn4b_branch2c[0][0]              \n",
      "                                                                   activation_313[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_316 (Activation)      (None, 5, 5, 1024)    0           add_105[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)          (None, 5, 5, 256)     262400      activation_316[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizatio (None, 5, 5, 256)     1024        res4c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_317 (Activation)      (None, 5, 5, 256)     0           bn4c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)          (None, 5, 5, 256)     590080      activation_317[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizatio (None, 5, 5, 256)     1024        res4c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_318 (Activation)      (None, 5, 5, 256)     0           bn4c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)          (None, 5, 5, 1024)    263168      activation_318[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizatio (None, 5, 5, 1024)    4096        res4c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_106 (Add)                    (None, 5, 5, 1024)    0           bn4c_branch2c[0][0]              \n",
      "                                                                   activation_316[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_319 (Activation)      (None, 5, 5, 1024)    0           add_106[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)          (None, 5, 5, 256)     262400      activation_319[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizatio (None, 5, 5, 256)     1024        res4d_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_320 (Activation)      (None, 5, 5, 256)     0           bn4d_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)          (None, 5, 5, 256)     590080      activation_320[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizatio (None, 5, 5, 256)     1024        res4d_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_321 (Activation)      (None, 5, 5, 256)     0           bn4d_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)          (None, 5, 5, 1024)    263168      activation_321[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizatio (None, 5, 5, 1024)    4096        res4d_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_107 (Add)                    (None, 5, 5, 1024)    0           bn4d_branch2c[0][0]              \n",
      "                                                                   activation_319[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_322 (Activation)      (None, 5, 5, 1024)    0           add_107[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)          (None, 5, 5, 256)     262400      activation_322[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizatio (None, 5, 5, 256)     1024        res4e_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_323 (Activation)      (None, 5, 5, 256)     0           bn4e_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)          (None, 5, 5, 256)     590080      activation_323[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizatio (None, 5, 5, 256)     1024        res4e_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_324 (Activation)      (None, 5, 5, 256)     0           bn4e_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)          (None, 5, 5, 1024)    263168      activation_324[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizatio (None, 5, 5, 1024)    4096        res4e_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_108 (Add)                    (None, 5, 5, 1024)    0           bn4e_branch2c[0][0]              \n",
      "                                                                   activation_322[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_325 (Activation)      (None, 5, 5, 1024)    0           add_108[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)          (None, 5, 5, 256)     262400      activation_325[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizatio (None, 5, 5, 256)     1024        res4f_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_326 (Activation)      (None, 5, 5, 256)     0           bn4f_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)          (None, 5, 5, 256)     590080      activation_326[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizatio (None, 5, 5, 256)     1024        res4f_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_327 (Activation)      (None, 5, 5, 256)     0           bn4f_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)          (None, 5, 5, 1024)    263168      activation_327[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizatio (None, 5, 5, 1024)    4096        res4f_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_109 (Add)                    (None, 5, 5, 1024)    0           bn4f_branch2c[0][0]              \n",
      "                                                                   activation_325[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "final (Activation)               (None, 5, 5, 1024)    0           add_109[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)          (None, 3, 3, 512)     524800      final[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizatio (None, 3, 3, 512)     2048        res5a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_328 (Activation)      (None, 3, 3, 512)     0           bn5a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)          (None, 3, 3, 512)     2359808     activation_328[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizatio (None, 3, 3, 512)     2048        res5a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_329 (Activation)      (None, 3, 3, 512)     0           bn5a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)          (None, 3, 3, 2048)    1050624     activation_329[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)           (None, 3, 3, 2048)    2099200     final[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizatio (None, 3, 3, 2048)    8192        res5a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalization (None, 3, 3, 2048)    8192        res5a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_110 (Add)                    (None, 3, 3, 2048)    0           bn5a_branch2c[0][0]              \n",
      "                                                                   bn5a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_330 (Activation)      (None, 3, 3, 2048)    0           add_110[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)          (None, 3, 3, 512)     1049088     activation_330[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizatio (None, 3, 3, 512)     2048        res5b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_331 (Activation)      (None, 3, 3, 512)     0           bn5b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)          (None, 3, 3, 512)     2359808     activation_331[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizatio (None, 3, 3, 512)     2048        res5b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_332 (Activation)      (None, 3, 3, 512)     0           bn5b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)          (None, 3, 3, 2048)    1050624     activation_332[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizatio (None, 3, 3, 2048)    8192        res5b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_111 (Add)                    (None, 3, 3, 2048)    0           bn5b_branch2c[0][0]              \n",
      "                                                                   activation_330[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_333 (Activation)      (None, 3, 3, 2048)    0           add_111[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)          (None, 3, 3, 512)     1049088     activation_333[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizatio (None, 3, 3, 512)     2048        res5c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_334 (Activation)      (None, 3, 3, 512)     0           bn5c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)          (None, 3, 3, 512)     2359808     activation_334[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizatio (None, 3, 3, 512)     2048        res5c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_335 (Activation)      (None, 3, 3, 512)     0           bn5c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)          (None, 3, 3, 2048)    1050624     activation_335[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizatio (None, 3, 3, 2048)    8192        res5c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_112 (Add)                    (None, 3, 3, 2048)    0           bn5c_branch2c[0][0]              \n",
      "                                                                   activation_333[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_336 (Activation)      (None, 3, 3, 2048)    0           add_112[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)      (None, 1, 1, 2048)    0           activation_336[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "global_avg_pool (GlobalAveragePo (None, 2048)          0           avg_pool[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "angle (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 2048)          0           global_avg_pool[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 1)             2           angle[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)      (None, 2049)          0           dropout_7[0][0]                  \n",
      "                                                                   dense_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 1)             2050        concatenate_7[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 23,589,764\n",
      "Trainable params: 23,536,644\n",
      "Non-trainable params: 53,120\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = getModel(baseModelName)\n",
    "model.summary()\n",
    "plot_model(model, to_file=baseModelName.lower() + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for layer in model.layers:\n",
    "#     print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=20, mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=8, verbose=1, epsilon=1e-4, mode='min')\n",
    "tensorboard = TensorBoard(log_dir='./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================FOLD= 0\n",
      "Train on 4277 samples, validate on 2139 samples\n",
      "Epoch 1/100\n",
      "4277/4277 [==============================] - 122s - loss: 0.5511 - acc: 0.7314 - val_loss: 0.9421 - val_acc: 0.5470\n",
      "Epoch 2/100\n",
      "4277/4277 [==============================] - 19s - loss: 0.2126 - acc: 0.9114 - val_loss: 0.6177 - val_acc: 0.6526\n",
      "Epoch 3/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0701 - acc: 0.9787 - val_loss: 0.6395 - val_acc: 0.6307\n",
      "Epoch 4/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0310 - acc: 0.9913 - val_loss: 0.6606 - val_acc: 0.6166\n",
      "Epoch 5/100\n",
      "4277/4277 [==============================] - 19s - loss: 0.0223 - acc: 0.9930 - val_loss: 0.5167 - val_acc: 0.7597\n",
      "Epoch 6/100\n",
      "4277/4277 [==============================] - 19s - loss: 0.0224 - acc: 0.9942 - val_loss: 0.4413 - val_acc: 0.8205\n",
      "Epoch 7/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0256 - acc: 0.9909 - val_loss: 0.9562 - val_acc: 0.7803\n",
      "Epoch 8/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0394 - acc: 0.9876 - val_loss: 0.4767 - val_acc: 0.8635\n",
      "Epoch 9/100\n",
      "4277/4277 [==============================] - 19s - loss: 0.0356 - acc: 0.9876 - val_loss: 0.3915 - val_acc: 0.8808\n",
      "Epoch 10/100\n",
      "4277/4277 [==============================] - 19s - loss: 0.0244 - acc: 0.9930 - val_loss: 0.3757 - val_acc: 0.8976\n",
      "Epoch 11/100\n",
      "4277/4277 [==============================] - 20s - loss: 0.0153 - acc: 0.9958 - val_loss: 0.3141 - val_acc: 0.9140\n",
      "Epoch 12/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0144 - acc: 0.9958 - val_loss: 0.5675 - val_acc: 0.8892\n",
      "Epoch 13/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0108 - acc: 0.9970 - val_loss: 0.3954 - val_acc: 0.9107\n",
      "Epoch 14/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0097 - acc: 0.9974 - val_loss: 0.3476 - val_acc: 0.9018\n",
      "Epoch 15/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0089 - acc: 0.9965 - val_loss: 0.3285 - val_acc: 0.9079\n",
      "Epoch 16/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.3376 - val_acc: 0.9173\n",
      "Epoch 17/100\n",
      "4277/4277 [==============================] - 16s - loss: 8.9890e-04 - acc: 1.0000 - val_loss: 0.3397 - val_acc: 0.9173\n",
      "Epoch 18/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.3333 - val_acc: 0.9210\n",
      "Epoch 19/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0034 - acc: 0.9986 - val_loss: 0.4934 - val_acc: 0.8962\n",
      "Epoch 20/100\n",
      "4224/4277 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 00019: reducing learning rate to 3.9999998989515007e-05.\n",
      "4277/4277 [==============================] - 20s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.4117 - val_acc: 0.9121\n",
      "Epoch 21/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0013 - acc: 0.9995 - val_loss: 0.4011 - val_acc: 0.9187\n",
      "Epoch 22/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0014 - acc: 0.9998 - val_loss: 0.3771 - val_acc: 0.9205\n",
      "Epoch 23/100\n",
      "4277/4277 [==============================] - 16s - loss: 7.2883e-04 - acc: 1.0000 - val_loss: 0.3681 - val_acc: 0.9247\n",
      "Epoch 24/100\n",
      "4277/4277 [==============================] - 16s - loss: 3.0510e-04 - acc: 1.0000 - val_loss: 0.3772 - val_acc: 0.9257\n",
      "Epoch 25/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0016 - acc: 0.9993 - val_loss: 0.3599 - val_acc: 0.9233\n",
      "Epoch 26/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0046 - acc: 0.9991 - val_loss: 0.3833 - val_acc: 0.9182\n",
      "Epoch 27/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0018 - acc: 0.9998 - val_loss: 0.5162 - val_acc: 0.8967\n",
      "Epoch 28/100\n",
      "4224/4277 [============================>.] - ETA: 0s - loss: 9.6650e-04 - acc: 1.0000\n",
      "Epoch 00027: reducing learning rate to 1.5999999595806004e-05.\n",
      "4277/4277 [==============================] - 16s - loss: 9.6416e-04 - acc: 1.0000 - val_loss: 0.3529 - val_acc: 0.9252\n",
      "Epoch 29/100\n",
      "4277/4277 [==============================] - 16s - loss: 3.4091e-04 - acc: 1.0000 - val_loss: 0.3527 - val_acc: 0.9243\n",
      "Epoch 30/100\n",
      "4277/4277 [==============================] - 16s - loss: 5.5504e-04 - acc: 0.9998 - val_loss: 0.3450 - val_acc: 0.9238\n",
      "Epoch 31/100\n",
      "4277/4277 [==============================] - 16s - loss: 3.4656e-04 - acc: 1.0000 - val_loss: 0.3457 - val_acc: 0.9233\n",
      "Epoch 32/100\n",
      "4277/4277 [==============================] - 16s - loss: 7.2153e-04 - acc: 0.9998 - val_loss: 0.3597 - val_acc: 0.9247\n",
      "6416/6416 [==============================] - 7s     \n",
      "Train score: 0.118959339211\n",
      "Train accuracy: 0.966178304239\n",
      "\n",
      "===================FOLD= 1\n",
      "Train on 4277 samples, validate on 2139 samples\n",
      "Epoch 1/100\n",
      "4277/4277 [==============================] - 142s - loss: 0.6193 - acc: 0.7292 - val_loss: 1.1313 - val_acc: 0.4988\n",
      "Epoch 2/100\n",
      "4277/4277 [==============================] - 20s - loss: 0.1822 - acc: 0.9252 - val_loss: 0.8632 - val_acc: 0.4988\n",
      "Epoch 3/100\n",
      "4277/4277 [==============================] - 19s - loss: 0.0690 - acc: 0.9750 - val_loss: 0.7124 - val_acc: 0.5909\n",
      "Epoch 4/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0266 - acc: 0.9921 - val_loss: 0.9709 - val_acc: 0.5993\n",
      "Epoch 5/100\n",
      "4277/4277 [==============================] - 20s - loss: 0.0123 - acc: 0.9965 - val_loss: 0.5389 - val_acc: 0.7536\n",
      "Epoch 6/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0168 - acc: 0.9932 - val_loss: 0.6430 - val_acc: 0.7723\n",
      "Epoch 7/100\n",
      "4277/4277 [==============================] - 19s - loss: 0.0354 - acc: 0.9874 - val_loss: 0.4953 - val_acc: 0.8242\n",
      "Epoch 8/100\n",
      "4277/4277 [==============================] - 19s - loss: 0.0208 - acc: 0.9939 - val_loss: 0.3384 - val_acc: 0.8855\n",
      "Epoch 9/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0216 - acc: 0.9935 - val_loss: 0.4283 - val_acc: 0.8897\n",
      "Epoch 10/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0168 - acc: 0.9953 - val_loss: 0.4319 - val_acc: 0.9028\n",
      "Epoch 11/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0220 - acc: 0.9906 - val_loss: 0.6646 - val_acc: 0.8775\n",
      "Epoch 12/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0260 - acc: 0.9909 - val_loss: 0.5309 - val_acc: 0.8738\n",
      "Epoch 13/100\n",
      "4277/4277 [==============================] - 19s - loss: 0.0151 - acc: 0.9963 - val_loss: 0.3283 - val_acc: 0.9060\n",
      "Epoch 14/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0068 - acc: 0.9977 - val_loss: 0.3957 - val_acc: 0.9093\n",
      "Epoch 15/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0018 - acc: 0.9998 - val_loss: 0.3960 - val_acc: 0.9163\n",
      "Epoch 16/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0032 - acc: 0.9986 - val_loss: 0.3979 - val_acc: 0.9182\n",
      "Epoch 17/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0071 - acc: 0.9977 - val_loss: 0.5983 - val_acc: 0.8897\n",
      "Epoch 18/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0175 - acc: 0.9937 - val_loss: 0.4097 - val_acc: 0.8873\n",
      "Epoch 19/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0095 - acc: 0.9965 - val_loss: 0.3746 - val_acc: 0.9112\n",
      "Epoch 20/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0040 - acc: 0.9988 - val_loss: 0.5570 - val_acc: 0.8929\n",
      "Epoch 21/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0091 - acc: 0.9965 - val_loss: 0.4826 - val_acc: 0.8939\n",
      "Epoch 22/100\n",
      "4224/4277 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9943\n",
      "Epoch 00021: reducing learning rate to 3.9999998989515007e-05.\n",
      "4277/4277 [==============================] - 20s - loss: 0.0140 - acc: 0.9944 - val_loss: 0.3486 - val_acc: 0.8925\n",
      "Epoch 23/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0089 - acc: 0.9963 - val_loss: 0.4250 - val_acc: 0.9051\n",
      "Epoch 24/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.3685 - val_acc: 0.9205\n",
      "Epoch 25/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0013 - acc: 0.9998 - val_loss: 0.3318 - val_acc: 0.9275\n",
      "Epoch 26/100\n",
      "4277/4277 [==============================] - 19s - loss: 7.5367e-04 - acc: 1.0000 - val_loss: 0.3242 - val_acc: 0.9285\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4277/4277 [==============================] - 16s - loss: 4.8491e-04 - acc: 1.0000 - val_loss: 0.3289 - val_acc: 0.9252\n",
      "Epoch 28/100\n",
      "4277/4277 [==============================] - 16s - loss: 2.3965e-04 - acc: 1.0000 - val_loss: 0.3349 - val_acc: 0.9294\n",
      "Epoch 29/100\n",
      "4277/4277 [==============================] - 16s - loss: 4.3895e-04 - acc: 0.9998 - val_loss: 0.3838 - val_acc: 0.9266\n",
      "Epoch 30/100\n",
      "4277/4277 [==============================] - 16s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.4282 - val_acc: 0.9163\n",
      "Epoch 31/100\n",
      "4277/4277 [==============================] - 16s - loss: 9.2000e-04 - acc: 0.9998 - val_loss: 0.3622 - val_acc: 0.9243\n",
      "Epoch 32/100\n",
      "4277/4277 [==============================] - 16s - loss: 4.1632e-04 - acc: 1.0000 - val_loss: 0.3720 - val_acc: 0.9257\n",
      "Epoch 33/100\n",
      "4277/4277 [==============================] - 16s - loss: 4.6054e-04 - acc: 1.0000 - val_loss: 0.3664 - val_acc: 0.9247\n",
      "Epoch 34/100\n",
      "4277/4277 [==============================] - 16s - loss: 1.9601e-04 - acc: 1.0000 - val_loss: 0.3578 - val_acc: 0.9266\n",
      "Epoch 35/100\n",
      "4224/4277 [============================>.] - ETA: 0s - loss: 1.7629e-04 - acc: 1.0000\n",
      "Epoch 00034: reducing learning rate to 1.5999999595806004e-05.\n",
      "4277/4277 [==============================] - 16s - loss: 1.7435e-04 - acc: 1.0000 - val_loss: 0.3577 - val_acc: 0.9289\n",
      "Epoch 36/100\n",
      "4277/4277 [==============================] - 16s - loss: 2.3521e-04 - acc: 1.0000 - val_loss: 0.3580 - val_acc: 0.9294\n",
      "Epoch 37/100\n",
      "4277/4277 [==============================] - 16s - loss: 2.5522e-04 - acc: 1.0000 - val_loss: 0.3572 - val_acc: 0.9280\n",
      "Epoch 38/100\n",
      "4277/4277 [==============================] - 16s - loss: 1.3153e-04 - acc: 1.0000 - val_loss: 0.3616 - val_acc: 0.9285\n",
      "Epoch 39/100\n",
      "4277/4277 [==============================] - 16s - loss: 1.4990e-04 - acc: 1.0000 - val_loss: 0.3625 - val_acc: 0.9289\n",
      "Epoch 40/100\n",
      "4277/4277 [==============================] - 16s - loss: 4.2121e-04 - acc: 1.0000 - val_loss: 0.3800 - val_acc: 0.9280\n",
      "Epoch 41/100\n",
      "4277/4277 [==============================] - 16s - loss: 2.6163e-04 - acc: 1.0000 - val_loss: 0.3686 - val_acc: 0.9303\n",
      "Epoch 42/100\n",
      "4277/4277 [==============================] - 16s - loss: 1.0168e-04 - acc: 1.0000 - val_loss: 0.3596 - val_acc: 0.9275\n",
      "Epoch 43/100\n",
      "4224/4277 [============================>.] - ETA: 0s - loss: 1.7020e-04 - acc: 1.0000\n",
      "Epoch 00042: reducing learning rate to 6.399999983841554e-06.\n",
      "4277/4277 [==============================] - 16s - loss: 1.7041e-04 - acc: 1.0000 - val_loss: 0.3602 - val_acc: 0.9266\n",
      "Epoch 44/100\n",
      "4277/4277 [==============================] - 16s - loss: 1.5070e-04 - acc: 1.0000 - val_loss: 0.3629 - val_acc: 0.9266\n",
      "Epoch 45/100\n",
      "4277/4277 [==============================] - 16s - loss: 1.3162e-04 - acc: 1.0000 - val_loss: 0.3633 - val_acc: 0.9257\n",
      "Epoch 46/100\n",
      "4277/4277 [==============================] - 16s - loss: 1.6439e-04 - acc: 1.0000 - val_loss: 0.3640 - val_acc: 0.9261\n",
      "Epoch 47/100\n",
      "4277/4277 [==============================] - 16s - loss: 4.2625e-05 - acc: 1.0000 - val_loss: 0.3634 - val_acc: 0.9261\n",
      "6416/6416 [==============================] - 7s     \n",
      "Train score: 0.108145886909\n",
      "Train accuracy: 0.976153366584\n",
      "\n",
      "===================FOLD= 2\n",
      "Train on 4278 samples, validate on 2138 samples\n",
      "Epoch 1/100\n",
      "4278/4278 [==============================] - 159s - loss: 0.5495 - acc: 0.7489 - val_loss: 0.5849 - val_acc: 0.7021\n",
      "Epoch 2/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.1766 - acc: 0.9278 - val_loss: 0.6533 - val_acc: 0.6539\n",
      "Epoch 3/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0645 - acc: 0.9752 - val_loss: 1.6669 - val_acc: 0.5309\n",
      "Epoch 4/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0405 - acc: 0.9864 - val_loss: 0.9582 - val_acc: 0.5982\n",
      "Epoch 5/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0295 - acc: 0.9899 - val_loss: 0.9721 - val_acc: 0.6478\n",
      "Epoch 6/100\n",
      "4278/4278 [==============================] - 20s - loss: 0.0269 - acc: 0.9916 - val_loss: 0.4872 - val_acc: 0.8110\n",
      "Epoch 7/100\n",
      "4278/4278 [==============================] - 19s - loss: 0.0238 - acc: 0.9914 - val_loss: 0.4088 - val_acc: 0.8499\n",
      "Epoch 8/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0225 - acc: 0.9942 - val_loss: 0.4296 - val_acc: 0.8732\n",
      "Epoch 9/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0146 - acc: 0.9953 - val_loss: 0.5043 - val_acc: 0.8718\n",
      "Epoch 10/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0179 - acc: 0.9939 - val_loss: 0.6233 - val_acc: 0.8728\n",
      "Epoch 11/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0189 - acc: 0.9914 - val_loss: 0.4594 - val_acc: 0.9027\n",
      "Epoch 12/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0121 - acc: 0.9958 - val_loss: 0.5313 - val_acc: 0.9004\n",
      "Epoch 13/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0076 - acc: 0.9972 - val_loss: 0.4567 - val_acc: 0.8948\n",
      "Epoch 14/100\n",
      "4278/4278 [==============================] - 20s - loss: 0.0122 - acc: 0.9963 - val_loss: 0.3711 - val_acc: 0.9200\n",
      "Epoch 15/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0138 - acc: 0.9956 - val_loss: 0.3948 - val_acc: 0.9065\n",
      "Epoch 16/100\n",
      "4278/4278 [==============================] - 20s - loss: 0.0052 - acc: 0.9988 - val_loss: 0.3517 - val_acc: 0.9210\n",
      "Epoch 17/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0022 - acc: 0.9998 - val_loss: 0.5273 - val_acc: 0.8980\n",
      "Epoch 18/100\n",
      "4278/4278 [==============================] - 19s - loss: 0.0017 - acc: 1.0000 - val_loss: 0.3419 - val_acc: 0.9270\n",
      "Epoch 19/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0028 - acc: 0.9991 - val_loss: 0.5139 - val_acc: 0.9097\n",
      "Epoch 20/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0132 - acc: 0.9960 - val_loss: 0.4262 - val_acc: 0.9107\n",
      "Epoch 21/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0191 - acc: 0.9944 - val_loss: 0.5190 - val_acc: 0.8807\n",
      "Epoch 22/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0195 - acc: 0.9930 - val_loss: 0.4383 - val_acc: 0.8999\n",
      "Epoch 23/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0206 - acc: 0.9946 - val_loss: 0.4770 - val_acc: 0.8934\n",
      "Epoch 24/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0137 - acc: 0.9956 - val_loss: 0.3581 - val_acc: 0.9153\n",
      "Epoch 25/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0046 - acc: 0.9981 - val_loss: 0.4641 - val_acc: 0.9074\n",
      "Epoch 26/100\n",
      "4278/4278 [==============================] - 16s - loss: 0.0043 - acc: 0.9986 - val_loss: 0.3863 - val_acc: 0.9181\n",
      "Epoch 27/100\n",
      "4224/4278 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9998\n",
      "Epoch 00026: reducing learning rate to 3.9999998989515007e-05.\n",
      "4278/4278 [==============================] - 21s - loss: 0.0017 - acc: 0.9998 - val_loss: 0.4432 - val_acc: 0.9041\n",
      "Epoch 28/100\n",
      "4278/4278 [==============================] - 16s - loss: 6.1971e-04 - acc: 1.0000 - val_loss: 0.3783 - val_acc: 0.9196\n",
      "Epoch 29/100\n",
      "4278/4278 [==============================] - 16s - loss: 2.4154e-04 - acc: 1.0000 - val_loss: 0.3711 - val_acc: 0.9228\n",
      "Epoch 30/100\n",
      "4278/4278 [==============================] - 16s - loss: 2.4573e-04 - acc: 1.0000 - val_loss: 0.3731 - val_acc: 0.9256\n",
      "Epoch 31/100\n",
      "4278/4278 [==============================] - 16s - loss: 4.1191e-04 - acc: 1.0000 - val_loss: 0.3790 - val_acc: 0.9238\n",
      "Epoch 32/100\n",
      "4278/4278 [==============================] - 16s - loss: 3.0175e-04 - acc: 1.0000 - val_loss: 0.3878 - val_acc: 0.9270\n",
      "Epoch 33/100\n",
      "4278/4278 [==============================] - 16s - loss: 5.4544e-04 - acc: 1.0000 - val_loss: 0.3986 - val_acc: 0.9247\n",
      "Epoch 34/100\n",
      "4278/4278 [==============================] - 16s - loss: 2.0519e-04 - acc: 1.0000 - val_loss: 0.3862 - val_acc: 0.9256\n",
      "Epoch 35/100\n",
      "4224/4278 [============================>.] - ETA: 0s - loss: 9.1444e-05 - acc: 1.0000\n",
      "Epoch 00034: reducing learning rate to 1.5999999595806004e-05.\n",
      "4278/4278 [==============================] - 16s - loss: 9.1008e-05 - acc: 1.0000 - val_loss: 0.3845 - val_acc: 0.9247\n",
      "Epoch 36/100\n",
      "4278/4278 [==============================] - 16s - loss: 4.0741e-04 - acc: 1.0000 - val_loss: 0.3868 - val_acc: 0.9256\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4278/4278 [==============================] - 16s - loss: 1.7091e-04 - acc: 1.0000 - val_loss: 0.3855 - val_acc: 0.9252\n",
      "Epoch 38/100\n",
      "4278/4278 [==============================] - 16s - loss: 1.2238e-04 - acc: 1.0000 - val_loss: 0.3835 - val_acc: 0.9261\n",
      "Epoch 39/100\n",
      "4278/4278 [==============================] - 16s - loss: 6.3106e-05 - acc: 1.0000 - val_loss: 0.3836 - val_acc: 0.9261\n",
      "6416/6416 [==============================] - 7s     \n",
      "Train score: 0.114027724927\n",
      "Train accuracy: 0.975685785536\n"
     ]
    }
   ],
   "source": [
    "K=3\n",
    "epochs = 100\n",
    "Kfolds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED).split(Xtrain, Ytrain))\n",
    "y_test_pred_log = 0\n",
    "for j, (train_idx, test_idx) in enumerate(Kfolds):\n",
    "    print('\\n===================FOLD=',j)\n",
    "    Xtrain_cv = Xtrain[train_idx]\n",
    "    Ytrain_cv = Ytrain[train_idx]\n",
    "    Xangle_cv = Xangle[train_idx]\n",
    "    Xtrain_val = Xtrain[test_idx]\n",
    "    Ytrain_val = Ytrain[test_idx]\n",
    "    Xangle_val = Xangle[test_idx]\n",
    "    \n",
    "    model_file = 'model_%s_%s.hdf5' % (baseModelName.lower(), j)\n",
    "    \n",
    "    mcp_save = ModelCheckpoint(model_file, save_best_only=True, monitor='val_loss', mode='min')\n",
    "    model = getModel(baseModelName)\n",
    "    \n",
    "    if useAngle:\n",
    "        Xtrain_cv = [Xtrain_cv, Xangle_cv]\n",
    "        Xtrain_val = [Xtrain_val, Xangle_val]\n",
    "        Xtrain_input = [Xtrain, Xangle]\n",
    "        Xtest_input = [Xtest, Xangle_test]\n",
    "    else:\n",
    "        Xtrain_input = Xtrain\n",
    "        Xtest_input = Xtest\n",
    "        \n",
    "    model.fit(Xtrain_cv, Ytrain_cv, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=True, callbacks=[\n",
    "        earlyStopping, \n",
    "        mcp_save, \n",
    "        reduce_lr_loss, \n",
    "#       tensorboard  \n",
    "    ], validation_data=(Xtrain_val, Ytrain_val))\n",
    "    \n",
    "    model.load_weights(filepath = model_file)    \n",
    "    \n",
    "    score = model.evaluate(Xtrain_input, Ytrain, verbose=1)\n",
    "    print('Train score:', score[0])\n",
    "    print('Train accuracy:', score[1])\n",
    "    y_test_pred_log += model.predict(Xtest_input).reshape(Xtest.shape[0])\n",
    "    \n",
    "y_test_pred_log /= K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorboard.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  is_iceberg\n",
      "0  5941774d    0.000002\n",
      "1  4023181e    0.641938\n",
      "2  b20200e4    0.000076\n",
      "3  e7f018bb    0.999889\n",
      "4  4371c8c3    0.535228\n",
      "5  a8d9b1fd    0.105813\n",
      "6  29e7727e    0.285361\n",
      "7  92a51ffb    0.999978\n",
      "8  c769ac97    0.000331\n",
      "9  aee0547d    0.000002\n",
      "id            8424\n",
      "is_iceberg    8424\n",
      "dtype: int64 8424\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': y_test_pred_log})\n",
    "print(submission.head(10))\n",
    "print(submission.count(), Xtest.shape[0])\n",
    "\n",
    "submission.to_csv('submission-cnn-custom-%s.csv' % baseModelName, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
