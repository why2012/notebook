{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import importlib\n",
    "SEED = 1234\n",
    "np.random.seed(SEED) \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D, AveragePooling2D, Concatenate, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import mylibs.ResNet as ResNet\n",
    "import mylibs.SENet as SENet\n",
    "importlib.reload(ResNet)\n",
    "importlib.reload(SENet)\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import uniform_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras.backend.tensorflow_backend import set_session  \n",
    "# config = tf.ConfigProto()  \n",
    "# config.gpu_options.allow_growth = True\n",
    "# set_session(tf.Session(config=config)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\kaggle\\iceberg\n"
     ]
    }
   ],
   "source": [
    "%cd E:\\kaggle\\iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scaled_imgs(df):\n",
    "    imgs = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        #make 75x75 image\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 + band_2 # plus since log(x*y) = log(x) + log(y)\n",
    "        \n",
    "        # Rescale\n",
    "        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
    "        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "\n",
    "        imgs.append(np.dstack((a, b, c)))\n",
    "\n",
    "    return np.array(imgs)\n",
    "\n",
    "def get_more_images(imgs):\n",
    "    more_images = []\n",
    "    vert_flip_imgs = []\n",
    "    hori_flip_imgs = []\n",
    "    vh_flip_imgs = []\n",
    "      \n",
    "    for i in range(0,imgs.shape[0]):\n",
    "        vert_flip_imgs.append(cv2.flip(imgs[i], 1))\n",
    "        hori_flip_imgs.append(cv2.flip(imgs[i], 0))\n",
    "        vh_flip_imgs.append(cv2.flip(imgs[i], -1))\n",
    "      \n",
    "    v = np.array(vert_flip_imgs)\n",
    "    h = np.array(hori_flip_imgs)\n",
    "    vh = np.array(vh_flip_imgs)\n",
    "       \n",
    "    more_images = np.concatenate((imgs,v,h, vh))\n",
    "    \n",
    "    return more_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_img(band_1, band_2, is_iceberg, angle = None):\n",
    "    if angle is None:\n",
    "        title_str = 'Iceberg' if is_iceberg == 1 else 'Ship'\n",
    "    else:\n",
    "        title_str = 'Iceberg-' + str(angle) if is_iceberg == 1 else 'Ship-' + str(angle)\n",
    "    fig = plt.figure(0, figsize=(10,10))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    ax.set_title(title_str + ' - Band 1')\n",
    "    ax.imshow(band_1,cmap='jet')\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    ax.set_title(title_str + ' - Band 2')\n",
    "    ax.imshow(band_2,cmap='jet')\n",
    "    plt.show()\n",
    "\n",
    "# implement functions to convert SAR data from decibel units to linear units and back again\n",
    "def decibel_to_linear(band):\n",
    "     # convert to linear units\n",
    "    return np.power(10,np.array(band)/10)\n",
    "\n",
    "def linear_to_decibel(band):\n",
    "    return 10*np.log10(band)\n",
    "\n",
    "# implement the Lee Filter for a band in an image already reshaped into the proper dimensions\n",
    "def lee_filter(band, window, var_noise = 0.25):\n",
    "    # band: SAR data to be despeckled (already reshaped into image dimensions)\n",
    "    # window: descpeckling filter window (tuple)\n",
    "    # default noise variance = 0.25\n",
    "    # assumes noise mean = 0\n",
    "    \n",
    "    mean_window = uniform_filter(band, window)\n",
    "    mean_sqr_window = uniform_filter(band**2, window)\n",
    "    var_window = mean_sqr_window - mean_window**2\n",
    "\n",
    "    weights = var_window / (var_window + var_noise)\n",
    "    band_filtered = mean_window + weights*(band - mean_window)\n",
    "    return band_filtered\n",
    "\n",
    "def apply_lee_filter(band_1_linear, band_2_linear, window_var_index = 0, noise_var_index = 0):\n",
    "    windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "    noise_var = np.array([1, 2, 4])\n",
    "    noise_var_1 = np.round(np.var(band_1_linear) * noise_var, 10)\n",
    "    noise_var_2 = np.round(np.var(band_2_linear) * noise_var, 10)\n",
    "    band_1_linear_filtered = lee_filter(band_1_linear, windows[window_var_index], noise_var_1[noise_var_index])\n",
    "    band_2_linear_filtered = lee_filter(band_2_linear, windows[window_var_index], noise_var_2[noise_var_index])\n",
    "    return band_1_linear_filtered, band_2_linear_filtered\n",
    "\n",
    "def apply_lee_filter_single(band_linear, window_var_index = 0, noise_var_index = 0):\n",
    "    windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "    noise_var = np.array([1, 2, 4])\n",
    "    noise_var = np.round(np.var(band_linear) * noise_var, 10)\n",
    "    band_linear_filtered = lee_filter(band_linear, windows[window_var_index], noise_var[noise_var_index])\n",
    "    return band_linear_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_custom_augmentation = False\n",
    "if use_custom_augmentation:\n",
    "    df_train = pd.read_json('E:/kaggle/iceberg/train.json/data/processed/train.json')\n",
    "    df_test = pd.read_json('E:/kaggle/iceberg/test.json/data/processed/test.json')\n",
    "    Xtrain = get_scaled_imgs(df_train)\n",
    "    Xtest = get_scaled_imgs(df_test)\n",
    "    Ytrain = np.array(df_train['is_iceberg'])\n",
    "    \n",
    "    df_train[\"inc_angle\"] = df_train[\"inc_angle\"].replace('na',0)\n",
    "    df_test[\"inc_angle\"] = df_test[\"inc_angle\"].replace('na',0)\n",
    "    idx_tr = np.where(df_train[\"inc_angle\"]>0)\n",
    "    Xtrain = Xtrain[idx_tr[0]]\n",
    "    Ytrain = Ytrain[idx_tr[0]]\n",
    "    \n",
    "    Xtrain = get_more_images(Xtrain) \n",
    "    Ytrain = np.concatenate((Ytrain,Ytrain,Ytrain, Ytrain))\n",
    "else:\n",
    "    train = pd.read_json(\"E:/kaggle/iceberg/train.json/data/processed/train.json\")\n",
    "    target_train=train['is_iceberg']\n",
    "    test = pd.read_json(\"E:/kaggle/iceberg/test.json/data/processed/test.json\")\n",
    "#     train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\n",
    "#     train['inc_angle']=train['inc_angle'].fillna(method='pad')\n",
    "    train[\"inc_angle\"] = train[\"inc_angle\"].replace('na',0)\n",
    "    idx_tr = np.where(train[\"inc_angle\"]>0)\n",
    "    train = train.iloc[idx_tr[0]]\n",
    "    target_train = target_train.iloc[idx_tr[0]]\n",
    "    X_angle=train['inc_angle']\n",
    "#     test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "    test['inc_angle']=test['inc_angle'].fillna(method='pad')\n",
    "    X_test_angle=test['inc_angle']\n",
    "    \n",
    "    #Generate the training data\n",
    "    X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "    X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "    #apply filter\n",
    "    X_band_1_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_1])\n",
    "    X_band_2_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_2])\n",
    "    X_band_1_filtered = linear_to_decibel(X_band_1_filtered)\n",
    "    X_band_2_filtered = linear_to_decibel(X_band_2_filtered)\n",
    "    X_band_1 = X_band_1_filtered\n",
    "    X_band_2 = X_band_2_filtered\n",
    "\n",
    "    X_band_3=np.fabs(np.subtract(X_band_1,X_band_2))\n",
    "    X_band_4=np.maximum(X_band_1,X_band_2)\n",
    "    X_band_5=np.minimum(X_band_1,X_band_2)\n",
    "    X_train = np.concatenate([X_band_3[:, :, :, np.newaxis],X_band_4[:, :, :, np.newaxis],X_band_5[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "    X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "    X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "    #apply filter\n",
    "    X_band_test_1_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_test_1])\n",
    "    X_band_test_2_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_test_2])\n",
    "    X_band_test_1_filtered = linear_to_decibel(X_band_test_1_filtered)\n",
    "    X_band_test_2_filtered = linear_to_decibel(X_band_test_2_filtered)\n",
    "    X_band_test_1 = X_band_test_1_filtered\n",
    "    X_band_test_2 = X_band_test_2_filtered\n",
    "\n",
    "    X_band_test_3=np.fabs(np.subtract(X_band_test_1,X_band_test_2))\n",
    "    X_band_test_4=np.maximum(X_band_test_1,X_band_test_2)\n",
    "    X_band_test_5=np.minimum(X_band_test_1,X_band_test_2)\n",
    "    X_test = np.concatenate([X_band_test_3[:, :, :, np.newaxis], X_band_test_4[:, :, :, np.newaxis],X_band_test_5[:, :, :, np.newaxis]],axis=-1)\n",
    "    \n",
    "    X_train = get_more_images(X_train)\n",
    "    target_train = np.concatenate((target_train, target_train, target_train, target_train))\n",
    "    X_angle = np.concatenate((X_angle, X_angle, X_angle, X_angle))\n",
    "    \n",
    "    Xtrain = X_train\n",
    "    Ytrain = target_train\n",
    "    Xtest = X_test\n",
    "    Xangle = X_angle\n",
    "    Xangle_test = X_test_angle\n",
    "    df_train = train\n",
    "    df_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6416, 75, 75, 3) (6416,) (6416,) (8424, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape, Ytrain.shape, Xangle.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEtCAYAAAAsgeXEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e491555240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_img(X_band_1[0], X_band_2[0], target_train[0], X_angle[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VGG16, ResNet50, SENet50\n",
    "baseModelName = \"ResNet34\"\n",
    "useAngle = False\n",
    "def getModel(baseModelName):\n",
    "    global useAngle\n",
    "    angle_input = Input(shape=[1], name=\"angle\")\n",
    "    angle_layer = Dense(1)(angle_input)\n",
    "    if baseModelName == \"VGG16\":\n",
    "        baseModel = VGG16(weights='imagenet', include_top=False, input_shape=Xtrain.shape[1:], pooling = \"avg\")\n",
    "        cnnOutput = baseModel.output\n",
    "    elif baseModelName == \"ResNet50\":\n",
    "        baseModel = ResNet.ResNet50(weights='imagenet', include_top=False, input_shape=Xtrain.shape[1:], pooling = \"avg\")\n",
    "        cnnOutput = baseModel.output\n",
    "#         cnnOutput = baseModel.get_layer(\"final\").output\n",
    "#         cnnOutput = AveragePooling2D((3, 3), name='avg_pool')(cnnOutput)\n",
    "#         cnnOutput = GlobalAveragePooling2D(name = \"global_avg_pool\")(cnnOutput)\n",
    "    elif baseModelName == \"SENet50\":\n",
    "        baseModel = SENet.SENet50(weights='imagenet', include_top=False, input_shape=Xtrain.shape[1:], pooling = \"avg\",\n",
    "#                                   kernel_regularizer = \"l2\",\n",
    "#                                   bias_regularizer = \"l2\",\n",
    "#                                   activity_regularizer = None,\n",
    "#                                   regularizer_value = 1e-4\n",
    "                                 )\n",
    "#         cnnOutput = baseModel.output\n",
    "        cnnOutput = baseModel.get_layer(\"final\").output\n",
    "        cnnOutput = AveragePooling2D((3, 3), name='avg_pool')(cnnOutput)\n",
    "        cnnOutput = GlobalAveragePooling2D(name = \"global_avg_pool\")(cnnOutput)\n",
    "    elif baseModelName == \"ResNet34\":\n",
    "        baseModel = ResNet.ResNet18(weights='imagenet', include_top=False, input_shape=Xtrain.shape[1:], base_channel_num=64, k=2)\n",
    "        cnnOutput = baseModel.get_layer(\"final\").output\n",
    "\n",
    "    if baseModelName == \"VGG16\":\n",
    "        fcOutput = Dropout(0.6)(cnnOutput)\n",
    "        predictions = Dense(1, activation=\"sigmoid\")(fcOutput)\n",
    "        model = Model(inputs=baseModel.input, outputs=predictions)\n",
    "    elif baseModelName == \"ResNet50\":\n",
    "        useAngle = True\n",
    "        predictions = Dense(1, activation=\"sigmoid\")(Concatenate()([Dropout(0.6)(cnnOutput), angle_layer]))\n",
    "        model = Model(inputs=[baseModel.input, angle_input], outputs=predictions)\n",
    "    elif baseModelName == \"SENet50\":\n",
    "        predictions = Dense(1, activation=\"sigmoid\")(cnnOutput)\n",
    "        model = Model(inputs=baseModel.input, outputs=predictions)\n",
    "    elif baseModelName == \"ResNet34\":\n",
    "        useAngle = True\n",
    "        cnnOutput = Dropout(0.6)(Flatten()(cnnOutput))\n",
    "        predictions = Dense(1, activation=\"sigmoid\")(Concatenate()([cnnOutput, angle_layer]))\n",
    "        model = Model(inputs=[baseModel.input, angle_input], outputs=predictions)\n",
    "    \n",
    "    optimizer = Adam(lr=1e-4)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_22 (InputLayer)            (None, 75, 75, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, 38, 38, 64)    9472        input_22[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_526 (Activation)      (None, 38, 38, 64)    0           conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D)  (None, 18, 18, 64)    0           activation_526[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)          (None, 18, 18, 128)   73856       max_pooling2d_22[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizatio (None, 18, 18, 128)   512         res2a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_527 (Activation)      (None, 18, 18, 128)   0           bn2a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)          (None, 18, 18, 128)   147584      activation_527[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizatio (None, 18, 18, 128)   512         res2a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_528 (Activation)      (None, 18, 18, 128)   0           bn2a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)           (None, 18, 18, 128)   73856       max_pooling2d_22[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "add_169 (Add)                    (None, 18, 18, 128)   0           activation_528[0][0]             \n",
      "                                                                   res2a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_529 (Activation)      (None, 18, 18, 128)   0           add_169[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)          (None, 18, 18, 128)   147584      activation_529[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizatio (None, 18, 18, 128)   512         res2b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_530 (Activation)      (None, 18, 18, 128)   0           bn2b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)          (None, 18, 18, 128)   147584      activation_530[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizatio (None, 18, 18, 128)   512         res2b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_531 (Activation)      (None, 18, 18, 128)   0           bn2b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_170 (Add)                    (None, 18, 18, 128)   0           activation_531[0][0]             \n",
      "                                                                   activation_529[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_532 (Activation)      (None, 18, 18, 128)   0           add_170[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)             (None, 18, 18, 128)   0           activation_532[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)          (None, 8, 8, 256)     295168      dropout_85[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizatio (None, 8, 8, 256)     1024        res3a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_533 (Activation)      (None, 8, 8, 256)     0           bn3a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)          (None, 8, 8, 256)     590080      activation_533[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizatio (None, 8, 8, 256)     1024        res3a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_534 (Activation)      (None, 8, 8, 256)     0           bn3a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)           (None, 8, 8, 256)     295168      dropout_85[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_171 (Add)                    (None, 8, 8, 256)     0           activation_534[0][0]             \n",
      "                                                                   res3a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_535 (Activation)      (None, 8, 8, 256)     0           add_171[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)          (None, 8, 8, 256)     590080      activation_535[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizatio (None, 8, 8, 256)     1024        res3b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_536 (Activation)      (None, 8, 8, 256)     0           bn3b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)          (None, 8, 8, 256)     590080      activation_536[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizatio (None, 8, 8, 256)     1024        res3b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_537 (Activation)      (None, 8, 8, 256)     0           bn3b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_172 (Add)                    (None, 8, 8, 256)     0           activation_537[0][0]             \n",
      "                                                                   activation_535[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_538 (Activation)      (None, 8, 8, 256)     0           add_172[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)             (None, 8, 8, 256)     0           activation_538[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)          (None, 3, 3, 256)     590080      dropout_86[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizatio (None, 3, 3, 256)     1024        res4a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_539 (Activation)      (None, 3, 3, 256)     0           bn4a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)          (None, 3, 3, 256)     590080      activation_539[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizatio (None, 3, 3, 256)     1024        res4a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_540 (Activation)      (None, 3, 3, 256)     0           bn4a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)           (None, 3, 3, 256)     590080      dropout_86[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_173 (Add)                    (None, 3, 3, 256)     0           activation_540[0][0]             \n",
      "                                                                   res4a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_541 (Activation)      (None, 3, 3, 256)     0           add_173[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)          (None, 3, 3, 256)     590080      activation_541[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizatio (None, 3, 3, 256)     1024        res4b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_542 (Activation)      (None, 3, 3, 256)     0           bn4b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)          (None, 3, 3, 256)     590080      activation_542[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizatio (None, 3, 3, 256)     1024        res4b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_543 (Activation)      (None, 3, 3, 256)     0           bn4b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_174 (Add)                    (None, 3, 3, 256)     0           activation_543[0][0]             \n",
      "                                                                   activation_541[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_544 (Activation)      (None, 3, 3, 256)     0           add_174[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)             (None, 3, 3, 256)     0           activation_544[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)          (None, 1, 1, 512)     1180160     dropout_87[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizatio (None, 1, 1, 512)     2048        res5a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_545 (Activation)      (None, 1, 1, 512)     0           bn5a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)          (None, 1, 1, 512)     2359808     activation_545[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizatio (None, 1, 1, 512)     2048        res5a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_546 (Activation)      (None, 1, 1, 512)     0           bn5a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)           (None, 1, 1, 512)     1180160     dropout_87[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_175 (Add)                    (None, 1, 1, 512)     0           activation_546[0][0]             \n",
      "                                                                   res5a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_547 (Activation)      (None, 1, 1, 512)     0           add_175[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)          (None, 1, 1, 512)     2359808     activation_547[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizatio (None, 1, 1, 512)     2048        res5b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_548 (Activation)      (None, 1, 1, 512)     0           bn5b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)          (None, 1, 1, 512)     2359808     activation_548[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizatio (None, 1, 1, 512)     2048        res5b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_549 (Activation)      (None, 1, 1, 512)     0           bn5b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_176 (Add)                    (None, 1, 1, 512)     0           activation_549[0][0]             \n",
      "                                                                   activation_547[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_550 (Activation)      (None, 1, 1, 512)     0           add_176[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "final (Dropout)                  (None, 1, 1, 512)     0           activation_550[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)             (None, 512)           0           final[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "angle (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_88 (Dropout)             (None, 512)           0           flatten_22[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_43 (Dense)                 (None, 1)             2           angle[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)     (None, 513)           0           dropout_88[0][0]                 \n",
      "                                                                   dense_43[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_44 (Dense)                 (None, 1)             514         concatenate_22[0][0]             \n",
      "====================================================================================================\n",
      "Total params: 15,369,604\n",
      "Trainable params: 15,360,388\n",
      "Non-trainable params: 9,216\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = getModel(baseModelName)\n",
    "model.summary()\n",
    "plot_model(model, to_file=baseModelName.lower() + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for layer in model.layers:\n",
    "#     print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=20, mode='min')\n",
    "# reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=8, verbose=1, epsilon=1e-4, mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=10, verbose=1, epsilon=1e-4, mode='min')\n",
    "tensorboard = TensorBoard(log_dir='./logs')\n",
    "\n",
    "# Define the image transformations here\n",
    "gen = ImageDataGenerator(horizontal_flip = False,\n",
    "                         vertical_flip = False,\n",
    "                         width_shift_range = 0.,\n",
    "                         height_shift_range = 0.,\n",
    "                         channel_shift_range=0,\n",
    "                         zoom_range = 0.5,\n",
    "                         rotation_range = 10)\n",
    "\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_two_inputs(X1, X2, y):\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=55)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size,seed=55)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            X2i = genX2.next()\n",
    "            #Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "            #np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "            yield [X1i[0], X2i[1]], X1i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================FOLD= 0\n",
      "Epoch 1/500\n",
      "330/330 [==============================] - 101s - loss: 0.7707 - acc: 0.6682 - val_loss: 0.6054 - val_acc: 0.7270\n",
      "Epoch 2/500\n",
      "330/330 [==============================] - 59s - loss: 0.4050 - acc: 0.8102 - val_loss: 0.2892 - val_acc: 0.8784\n",
      "Epoch 3/500\n",
      "330/330 [==============================] - 57s - loss: 0.3321 - acc: 0.8523 - val_loss: 0.3262 - val_acc: 0.8583\n",
      "Epoch 4/500\n",
      "330/330 [==============================] - 59s - loss: 0.2894 - acc: 0.8736 - val_loss: 0.2692 - val_acc: 0.8808\n",
      "Epoch 5/500\n",
      "330/330 [==============================] - 58s - loss: 0.2679 - acc: 0.8842 - val_loss: 0.2922 - val_acc: 0.8761\n",
      "Epoch 6/500\n",
      "330/330 [==============================] - 58s - loss: 0.2514 - acc: 0.8922 - val_loss: 0.3493 - val_acc: 0.8439\n",
      "Epoch 7/500\n",
      "330/330 [==============================] - 56s - loss: 0.2290 - acc: 0.9001 - val_loss: 0.5654 - val_acc: 0.8065\n",
      "Epoch 8/500\n",
      "330/330 [==============================] - 58s - loss: 0.2111 - acc: 0.9106 - val_loss: 0.2348 - val_acc: 0.9074\n",
      "Epoch 9/500\n",
      "330/330 [==============================] - 57s - loss: 0.2051 - acc: 0.9136 - val_loss: 0.3033 - val_acc: 0.8686\n",
      "Epoch 10/500\n",
      "330/330 [==============================] - 59s - loss: 0.1888 - acc: 0.9219 - val_loss: 0.3818 - val_acc: 0.8420\n",
      "Epoch 11/500\n",
      "330/330 [==============================] - 58s - loss: 0.1733 - acc: 0.9271 - val_loss: 0.2679 - val_acc: 0.8859\n",
      "Epoch 12/500\n",
      "330/330 [==============================] - 57s - loss: 0.1662 - acc: 0.9299 - val_loss: 0.3791 - val_acc: 0.8429\n",
      "Epoch 13/500\n",
      "330/330 [==============================] - 56s - loss: 0.1609 - acc: 0.9337 - val_loss: 0.2932 - val_acc: 0.8981\n",
      "Epoch 14/500\n",
      "330/330 [==============================] - 57s - loss: 0.1473 - acc: 0.9407 - val_loss: 1.3913 - val_acc: 0.7312\n",
      "Epoch 15/500\n",
      "330/330 [==============================] - 56s - loss: 0.1412 - acc: 0.9430 - val_loss: 0.2436 - val_acc: 0.9046\n",
      "Epoch 16/500\n",
      "330/330 [==============================] - 57s - loss: 0.1316 - acc: 0.9486 - val_loss: 0.4383 - val_acc: 0.8181\n",
      "Epoch 17/500\n",
      "330/330 [==============================] - 56s - loss: 0.1252 - acc: 0.9499 - val_loss: 0.3895 - val_acc: 0.8901\n",
      "Epoch 18/500\n",
      "330/330 [==============================] - 56s - loss: 0.1103 - acc: 0.9560 - val_loss: 0.4212 - val_acc: 0.8420\n",
      "Epoch 19/500\n",
      "329/330 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9578\n",
      "Epoch 00018: reducing learning rate to 3.9999998989515007e-05.\n",
      "330/330 [==============================] - 60s - loss: 0.1095 - acc: 0.9579 - val_loss: 0.5464 - val_acc: 0.8738\n",
      "Epoch 20/500\n",
      "330/330 [==============================] - 55s - loss: 0.0752 - acc: 0.9716 - val_loss: 0.3465 - val_acc: 0.9070\n",
      "Epoch 21/500\n",
      "330/330 [==============================] - 57s - loss: 0.0681 - acc: 0.9740 - val_loss: 0.3322 - val_acc: 0.8939\n",
      "Epoch 22/500\n",
      "330/330 [==============================] - 55s - loss: 0.0616 - acc: 0.9771 - val_loss: 0.3531 - val_acc: 0.9060\n",
      "Epoch 23/500\n",
      "330/330 [==============================] - 58s - loss: 0.0551 - acc: 0.9783 - val_loss: 0.3127 - val_acc: 0.9018\n",
      "Epoch 24/500\n",
      "330/330 [==============================] - 58s - loss: 0.0495 - acc: 0.9806 - val_loss: 0.2797 - val_acc: 0.9201\n",
      "Epoch 25/500\n",
      "330/330 [==============================] - 57s - loss: 0.0519 - acc: 0.9811 - val_loss: 0.2918 - val_acc: 0.9233\n",
      "Epoch 26/500\n",
      "330/330 [==============================] - 57s - loss: 0.0493 - acc: 0.9823 - val_loss: 0.3699 - val_acc: 0.9023\n",
      "Epoch 27/500\n",
      "330/330 [==============================] - 57s - loss: 0.0441 - acc: 0.9834 - val_loss: 0.3323 - val_acc: 0.9299\n",
      "Epoch 28/500\n",
      "330/330 [==============================] - 57s - loss: 0.0457 - acc: 0.9828 - val_loss: 0.4901 - val_acc: 0.9158\n",
      "Epoch 29/500\n",
      "329/330 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9843\n",
      "Epoch 00028: reducing learning rate to 1.5999999595806004e-05.\n",
      "330/330 [==============================] - 57s - loss: 0.0433 - acc: 0.9843 - val_loss: 0.3691 - val_acc: 0.9163\n",
      "Epoch 30/500\n",
      "330/330 [==============================] - 58s - loss: 0.0286 - acc: 0.9891 - val_loss: 0.3094 - val_acc: 0.9327\n",
      "Epoch 31/500\n",
      "330/330 [==============================] - 57s - loss: 0.0269 - acc: 0.9904 - val_loss: 0.4046 - val_acc: 0.9079\n",
      "Epoch 32/500\n",
      "330/330 [==============================] - 57s - loss: 0.0259 - acc: 0.9910 - val_loss: 0.3502 - val_acc: 0.9271\n",
      "Epoch 33/500\n",
      "330/330 [==============================] - 59s - loss: 0.0228 - acc: 0.9916 - val_loss: 0.3571 - val_acc: 0.9303\n",
      "Epoch 34/500\n",
      "330/330 [==============================] - 58s - loss: 0.0233 - acc: 0.9914 - val_loss: 0.3322 - val_acc: 0.9243\n",
      "Epoch 35/500\n",
      "330/330 [==============================] - 58s - loss: 0.0235 - acc: 0.9914 - val_loss: 0.3326 - val_acc: 0.9299\n",
      "Epoch 36/500\n",
      "330/330 [==============================] - 57s - loss: 0.0207 - acc: 0.9924 - val_loss: 0.3439 - val_acc: 0.9294\n",
      "Epoch 37/500\n",
      "330/330 [==============================] - 58s - loss: 0.0219 - acc: 0.9917 - val_loss: 0.3787 - val_acc: 0.9345\n",
      "Epoch 38/500\n",
      "330/330 [==============================] - 57s - loss: 0.0217 - acc: 0.9926 - val_loss: 0.4101 - val_acc: 0.9102\n",
      "Epoch 39/500\n",
      "329/330 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9929\n",
      "Epoch 00038: reducing learning rate to 6.399999983841554e-06.\n",
      "330/330 [==============================] - 58s - loss: 0.0186 - acc: 0.9929 - val_loss: 0.3247 - val_acc: 0.9402\n",
      "Epoch 40/500\n",
      "330/330 [==============================] - 57s - loss: 0.0160 - acc: 0.9940 - val_loss: 0.3241 - val_acc: 0.9355\n",
      "Epoch 41/500\n",
      "330/330 [==============================] - 57s - loss: 0.0160 - acc: 0.9941 - val_loss: 0.3409 - val_acc: 0.9448\n",
      "Epoch 42/500\n",
      "330/330 [==============================] - 57s - loss: 0.0151 - acc: 0.9947 - val_loss: 0.3443 - val_acc: 0.9383\n",
      "Epoch 43/500\n",
      "330/330 [==============================] - 56s - loss: 0.0139 - acc: 0.9952 - val_loss: 0.3385 - val_acc: 0.9392\n",
      "Epoch 44/500\n",
      "330/330 [==============================] - 57s - loss: 0.0140 - acc: 0.9950 - val_loss: 0.3577 - val_acc: 0.9322\n",
      "Epoch 45/500\n",
      "330/330 [==============================] - 57s - loss: 0.0143 - acc: 0.9950 - val_loss: 0.3810 - val_acc: 0.9383\n",
      "Epoch 46/500\n",
      "330/330 [==============================] - 59s - loss: 0.0129 - acc: 0.9958 - val_loss: 0.3565 - val_acc: 0.9388\n",
      "Epoch 47/500\n",
      "330/330 [==============================] - 59s - loss: 0.0126 - acc: 0.9950 - val_loss: 0.4191 - val_acc: 0.9406\n",
      "Epoch 48/500\n",
      "330/330 [==============================] - 58s - loss: 0.0124 - acc: 0.9957 - val_loss: 0.4087 - val_acc: 0.9196\n",
      "Epoch 49/500\n",
      "329/330 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9954\n",
      "Epoch 00048: reducing learning rate to 2.5600000299164097e-06.\n",
      "330/330 [==============================] - 58s - loss: 0.0136 - acc: 0.9954 - val_loss: 0.3843 - val_acc: 0.9411\n",
      "Epoch 50/500\n",
      "330/330 [==============================] - 59s - loss: 0.0131 - acc: 0.9956 - val_loss: 0.3439 - val_acc: 0.9402\n",
      "Epoch 51/500\n",
      "330/330 [==============================] - 58s - loss: 0.0113 - acc: 0.9960 - val_loss: 0.3608 - val_acc: 0.9406\n",
      "Epoch 52/500\n",
      "330/330 [==============================] - 56s - loss: 0.0111 - acc: 0.9960 - val_loss: 0.3595 - val_acc: 0.9364\n",
      "Epoch 53/500\n",
      "330/330 [==============================] - 58s - loss: 0.0124 - acc: 0.9957 - val_loss: 0.3538 - val_acc: 0.9369\n",
      "Epoch 54/500\n",
      "330/330 [==============================] - 58s - loss: 0.0104 - acc: 0.9964 - val_loss: 0.3659 - val_acc: 0.9369\n",
      "Epoch 55/500\n",
      "330/330 [==============================] - 59s - loss: 0.0123 - acc: 0.9956 - val_loss: 0.3603 - val_acc: 0.9317\n",
      "Epoch 56/500\n",
      "330/330 [==============================] - 59s - loss: 0.0126 - acc: 0.9958 - val_loss: 0.3485 - val_acc: 0.9374\n",
      "Epoch 57/500\n",
      "330/330 [==============================] - 58s - loss: 0.0106 - acc: 0.9959 - val_loss: 0.3574 - val_acc: 0.9402\n",
      "Epoch 58/500\n",
      "330/330 [==============================] - 58s - loss: 0.0102 - acc: 0.9966 - val_loss: 0.3821 - val_acc: 0.9388\n",
      "Epoch 59/500\n",
      "329/330 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9962\n",
      "Epoch 00058: reducing learning rate to 1.0239999937766699e-06.\n",
      "330/330 [==============================] - 58s - loss: 0.0115 - acc: 0.9963 - val_loss: 0.3704 - val_acc: 0.9345\n",
      "Epoch 60/500\n",
      "330/330 [==============================] - 58s - loss: 0.0101 - acc: 0.9963 - val_loss: 0.3690 - val_acc: 0.9434\n",
      "Epoch 61/500\n",
      "330/330 [==============================] - 57s - loss: 0.0114 - acc: 0.9960 - val_loss: 0.3617 - val_acc: 0.9425\n",
      "Epoch 62/500\n",
      "330/330 [==============================] - 59s - loss: 0.0095 - acc: 0.9968 - val_loss: 0.3595 - val_acc: 0.9430\n",
      "Epoch 63/500\n",
      "330/330 [==============================] - 60s - loss: 0.0111 - acc: 0.9962 - val_loss: 0.3551 - val_acc: 0.9416\n",
      "Epoch 64/500\n",
      "330/330 [==============================] - 58s - loss: 0.0104 - acc: 0.9963 - val_loss: 0.3601 - val_acc: 0.9430\n",
      "Epoch 65/500\n",
      "330/330 [==============================] - 59s - loss: 0.0088 - acc: 0.9966 - val_loss: 0.3605 - val_acc: 0.9416\n",
      "Epoch 66/500\n",
      "330/330 [==============================] - 58s - loss: 0.0083 - acc: 0.9974 - val_loss: 0.3662 - val_acc: 0.9406\n",
      "Epoch 67/500\n",
      "330/330 [==============================] - 60s - loss: 0.0090 - acc: 0.9970 - val_loss: 0.3688 - val_acc: 0.9406\n",
      "Epoch 68/500\n",
      "330/330 [==============================] - 59s - loss: 0.0112 - acc: 0.9961 - val_loss: 0.3664 - val_acc: 0.9388\n",
      "Epoch 69/500\n",
      "329/330 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9960\n",
      "Epoch 00068: reducing learning rate to 4.095999884157209e-07.\n",
      "330/330 [==============================] - 59s - loss: 0.0108 - acc: 0.9960 - val_loss: 0.3598 - val_acc: 0.9434\n",
      "Epoch 70/500\n",
      "330/330 [==============================] - 59s - loss: 0.0106 - acc: 0.9964 - val_loss: 0.3634 - val_acc: 0.9425\n",
      "Epoch 71/500\n",
      "330/330 [==============================] - 58s - loss: 0.0091 - acc: 0.9971 - val_loss: 0.3654 - val_acc: 0.9411\n",
      "Epoch 72/500\n",
      "330/330 [==============================] - 57s - loss: 0.0082 - acc: 0.9969 - val_loss: 0.3650 - val_acc: 0.9425\n",
      "Epoch 73/500\n",
      "330/330 [==============================] - 57s - loss: 0.0084 - acc: 0.9969 - val_loss: 0.3645 - val_acc: 0.9420\n",
      "Epoch 74/500\n",
      "330/330 [==============================] - 57s - loss: 0.0099 - acc: 0.9966 - val_loss: 0.3636 - val_acc: 0.9411\n",
      "Epoch 75/500\n",
      "330/330 [==============================] - 58s - loss: 0.0083 - acc: 0.9970 - val_loss: 0.3656 - val_acc: 0.9420\n",
      "Epoch 76/500\n",
      "330/330 [==============================] - 59s - loss: 0.0093 - acc: 0.9968 - val_loss: 0.3668 - val_acc: 0.9411\n",
      "Epoch 77/500\n",
      "330/330 [==============================] - 59s - loss: 0.0097 - acc: 0.9969 - val_loss: 0.3656 - val_acc: 0.9416\n",
      "Epoch 78/500\n",
      "330/330 [==============================] - 59s - loss: 0.0081 - acc: 0.9971 - val_loss: 0.3694 - val_acc: 0.9402\n",
      "Epoch 79/500\n",
      "329/330 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9960\n",
      "Epoch 00078: reducing learning rate to 1.6383999081881485e-07.\n",
      "330/330 [==============================] - 59s - loss: 0.0120 - acc: 0.9960 - val_loss: 0.3636 - val_acc: 0.9402\n",
      "Epoch 80/500\n",
      "330/330 [==============================] - 59s - loss: 0.0081 - acc: 0.9974 - val_loss: 0.3680 - val_acc: 0.9411\n",
      "Epoch 81/500\n",
      "330/330 [==============================] - 59s - loss: 0.0104 - acc: 0.9966 - val_loss: 0.3649 - val_acc: 0.9425\n",
      "Epoch 82/500\n",
      "330/330 [==============================] - 59s - loss: 0.0086 - acc: 0.9971 - val_loss: 0.3635 - val_acc: 0.9411\n",
      "Epoch 83/500\n",
      "330/330 [==============================] - 59s - loss: 0.0094 - acc: 0.9970 - val_loss: 0.3669 - val_acc: 0.9425\n",
      "Epoch 84/500\n",
      "330/330 [==============================] - 60s - loss: 0.0091 - acc: 0.9968 - val_loss: 0.3665 - val_acc: 0.9430\n",
      "Epoch 85/500\n",
      "330/330 [==============================] - 57s - loss: 0.0101 - acc: 0.9969 - val_loss: 0.3663 - val_acc: 0.9425\n",
      "Epoch 86/500\n",
      "330/330 [==============================] - 58s - loss: 0.0084 - acc: 0.9969 - val_loss: 0.3665 - val_acc: 0.9420\n",
      "Epoch 87/500\n",
      "330/330 [==============================] - 58s - loss: 0.0090 - acc: 0.9965 - val_loss: 0.3653 - val_acc: 0.9430\n",
      "Epoch 88/500\n",
      "330/330 [==============================] - 58s - loss: 0.0085 - acc: 0.9970 - val_loss: 0.3663 - val_acc: 0.9420\n",
      "Epoch 89/500\n",
      "329/330 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9963\n",
      "Epoch 00088: reducing learning rate to 6.553599405378919e-08.\n",
      "330/330 [==============================] - 59s - loss: 0.0106 - acc: 0.9963 - val_loss: 0.3646 - val_acc: 0.9416\n",
      "Epoch 90/500\n",
      "330/330 [==============================] - 58s - loss: 0.0082 - acc: 0.9970 - val_loss: 0.3641 - val_acc: 0.9420\n",
      "Epoch 91/500\n",
      "330/330 [==============================] - 59s - loss: 0.0093 - acc: 0.9970 - val_loss: 0.3645 - val_acc: 0.9416\n",
      "Epoch 92/500\n",
      "330/330 [==============================] - 60s - loss: 0.0117 - acc: 0.9964 - val_loss: 0.3644 - val_acc: 0.9425\n",
      "Epoch 93/500\n",
      "330/330 [==============================] - 58s - loss: 0.0086 - acc: 0.9964 - val_loss: 0.3634 - val_acc: 0.9420\n",
      "Epoch 94/500\n",
      "330/330 [==============================] - 57s - loss: 0.0078 - acc: 0.9973 - val_loss: 0.3653 - val_acc: 0.9420\n",
      "Epoch 95/500\n",
      "330/330 [==============================] - 56s - loss: 0.0092 - acc: 0.9970 - val_loss: 0.3652 - val_acc: 0.9416\n",
      "Epoch 96/500\n",
      "330/330 [==============================] - 56s - loss: 0.0081 - acc: 0.9970 - val_loss: 0.3659 - val_acc: 0.9420\n",
      "Epoch 97/500\n",
      "330/330 [==============================] - 56s - loss: 0.0086 - acc: 0.9971 - val_loss: 0.3665 - val_acc: 0.9420\n",
      "Epoch 98/500\n",
      "330/330 [==============================] - 56s - loss: 0.0089 - acc: 0.9971 - val_loss: 0.3659 - val_acc: 0.9420\n",
      "Epoch 99/500\n",
      "329/330 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9965\n",
      "Epoch 00098: reducing learning rate to 2.6214397053081487e-08.\n",
      "330/330 [==============================] - 56s - loss: 0.0105 - acc: 0.9965 - val_loss: 0.3639 - val_acc: 0.9416\n",
      "Epoch 100/500\n",
      "330/330 [==============================] - 57s - loss: 0.0096 - acc: 0.9962 - val_loss: 0.3644 - val_acc: 0.9425\n",
      "Epoch 101/500\n",
      "330/330 [==============================] - 56s - loss: 0.0101 - acc: 0.9966 - val_loss: 0.3660 - val_acc: 0.9425\n",
      "Epoch 102/500\n",
      " 85/330 [======>.......................] - ETA: 38s - loss: 0.0076 - acc: 0.9978"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-e936ba054538>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mreduce_lr_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m#       tensorboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         ], validation_data=(Xtrain_val, Ytrain_val))\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2009\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2011\u001b[1;33m                     \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2013\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    643\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K=3\n",
    "epochs = 500\n",
    "Kfolds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED).split(Xtrain, Ytrain))\n",
    "y_test_pred_log = 0\n",
    "useGen = True\n",
    "for j, (train_idx, test_idx) in enumerate(Kfolds):\n",
    "    print('\\n===================FOLD=',j)\n",
    "    Xtrain_cv = Xtrain[train_idx]\n",
    "    Ytrain_cv = Ytrain[train_idx]\n",
    "    Xangle_cv = Xangle[train_idx]\n",
    "    Xtrain_val = Xtrain[test_idx]\n",
    "    Ytrain_val = Ytrain[test_idx]\n",
    "    Xangle_val = Xangle[test_idx]\n",
    "    \n",
    "    model_file = 'model_%s_%s.hdf5' % (baseModelName.lower(), j)\n",
    "    \n",
    "    mcp_save = ModelCheckpoint(model_file, save_best_only=True, monitor='val_loss', mode='min')\n",
    "    model = getModel(baseModelName)\n",
    "    \n",
    "    if useGen and useAngle:\n",
    "        gen_flow = gen_flow_for_two_inputs(Xtrain_cv, Xangle_cv, Ytrain_cv)\n",
    "        Xtrain_val = [Xtrain_val, Xangle_val]\n",
    "        Xtrain_input = [Xtrain, Xangle]\n",
    "        Xtest_input = [Xtest, Xangle_test]\n",
    "    elif useAngle:\n",
    "        Xtrain_cv = [Xtrain_cv, Xangle_cv]\n",
    "        Xtrain_val = [Xtrain_val, Xangle_val]\n",
    "        Xtrain_input = [Xtrain, Xangle]\n",
    "        Xtest_input = [Xtest, Xangle_test]\n",
    "    else:\n",
    "        Xtrain_input = Xtrain\n",
    "        Xtest_input = Xtest\n",
    "    \n",
    "    if not useGen:\n",
    "        model.fit(Xtrain_cv, Ytrain_cv, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=True, callbacks=[\n",
    "            earlyStopping, \n",
    "            mcp_save, \n",
    "            reduce_lr_loss, \n",
    "    #       tensorboard  \n",
    "        ], validation_data=(Xtrain_val, Ytrain_val))\n",
    "    else:\n",
    "        model.fit_generator(gen_flow, steps_per_epoch=int(len(Xtrain_cv) / batch_size) * 5, epochs=epochs, verbose=1, shuffle=True, callbacks=[\n",
    "#             earlyStopping, \n",
    "            mcp_save, \n",
    "            reduce_lr_loss, \n",
    "    #       tensorboard  \n",
    "        ], validation_data=(Xtrain_val, Ytrain_val))\n",
    "        \n",
    "    model.load_weights(filepath = model_file)    \n",
    "    \n",
    "    score = model.evaluate(Xtrain_input, Ytrain, verbose=1)\n",
    "    print('Train score:', score[0])\n",
    "    print('Train accuracy:', score[1])\n",
    "    y_test_pred_log += model.predict(Xtest_input).reshape(Xtest.shape[0])\n",
    "    \n",
    "y_test_pred_log /= K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorboard.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id    is_iceberg\n",
      "0  5941774d  5.561437e-02\n",
      "1  4023181e  1.674212e-01\n",
      "2  b20200e4  2.623082e-18\n",
      "3  e7f018bb  9.991274e-01\n",
      "4  4371c8c3  7.676017e-02\n",
      "5  a8d9b1fd  1.398854e-08\n",
      "6  29e7727e  3.647814e-01\n",
      "7  92a51ffb  9.991319e-01\n",
      "8  c769ac97  3.487041e-12\n",
      "9  aee0547d  8.132521e-15\n",
      "id            8424\n",
      "is_iceberg    8424\n",
      "dtype: int64 8424\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': y_test_pred_log})\n",
    "print(submission.head(10))\n",
    "print(submission.count(), Xtest.shape[0])\n",
    "\n",
    "submission.to_csv('submission-cnn-custom-%s.csv' % baseModelName, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
