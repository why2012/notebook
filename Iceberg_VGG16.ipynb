{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import importlib\n",
    "SEED = 1234\n",
    "np.random.seed(SEED) \n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D, AveragePooling2D, Concatenate, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from mylibs.VggNet import VGG16\n",
    "import mylibs.ResNet as ResNet\n",
    "import mylibs.SENet as SENet\n",
    "importlib.reload(ResNet)\n",
    "importlib.reload(SENet)\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import uniform_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\kaggle\\iceberg\n"
     ]
    }
   ],
   "source": [
    "%cd E:\\kaggle\\iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_img(band_1, band_2, is_iceberg, angle = None):\n",
    "    if angle is None:\n",
    "        title_str = 'Iceberg' if is_iceberg == 1 else 'Ship'\n",
    "    else:\n",
    "        title_str = 'Iceberg-' + str(angle) if is_iceberg == 1 else 'Ship-' + str(angle)\n",
    "    fig = plt.figure(0, figsize=(10,10))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    ax.set_title(title_str + ' - Band 1')\n",
    "    ax.imshow(band_1,cmap='jet')\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    ax.set_title(title_str + ' - Band 2')\n",
    "    ax.imshow(band_2,cmap='jet')\n",
    "    plt.show()\n",
    "\n",
    "# implement functions to convert SAR data from decibel units to linear units and back again\n",
    "def decibel_to_linear(band):\n",
    "     # convert to linear units\n",
    "    return np.power(10,np.array(band)/10)\n",
    "\n",
    "def linear_to_decibel(band):\n",
    "    return 10*np.log10(band)\n",
    "\n",
    "# implement the Lee Filter for a band in an image already reshaped into the proper dimensions\n",
    "def lee_filter(band, window, var_noise = 0.25):\n",
    "    # band: SAR data to be despeckled (already reshaped into image dimensions)\n",
    "    # window: descpeckling filter window (tuple)\n",
    "    # default noise variance = 0.25\n",
    "    # assumes noise mean = 0\n",
    "    \n",
    "    mean_window = uniform_filter(band, window)\n",
    "    mean_sqr_window = uniform_filter(band**2, window)\n",
    "    var_window = mean_sqr_window - mean_window**2\n",
    "\n",
    "    weights = var_window / (var_window + var_noise)\n",
    "    band_filtered = mean_window + weights*(band - mean_window)\n",
    "    return band_filtered\n",
    "\n",
    "def apply_lee_filter(band_1_linear, band_2_linear, window_var_index = 0, noise_var_index = 0):\n",
    "    windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "    noise_var = np.array([1, 2, 4])\n",
    "    noise_var_1 = np.round(np.var(band_1_linear) * noise_var, 10)\n",
    "    noise_var_2 = np.round(np.var(band_2_linear) * noise_var, 10)\n",
    "    band_1_linear_filtered = lee_filter(band_1_linear, windows[window_var_index], noise_var_1[noise_var_index])\n",
    "    band_2_linear_filtered = lee_filter(band_2_linear, windows[window_var_index], noise_var_2[noise_var_index])\n",
    "    return band_1_linear_filtered, band_2_linear_filtered\n",
    "\n",
    "def apply_lee_filter_single(band_linear, window_var_index = 0, noise_var_index = 0):\n",
    "    windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "    noise_var = np.array([1, 2, 4])\n",
    "    noise_var = np.round(np.var(band_linear) * noise_var, 10)\n",
    "    band_linear_filtered = lee_filter(band_linear, windows[window_var_index], noise_var[noise_var_index])\n",
    "    return band_linear_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_get_scaled_band(band_list):\n",
    "    imgs = []\n",
    "    for band in band_list:        \n",
    "        imgs.append((band - band.mean()) / (band.max() - band.min()))\n",
    "#         imgs.append(band - band.mean())\n",
    "#         imgs.append((band - band.mean()) / band.std())\n",
    "#         imgs.append(cv2.normalize(band, None, -1, 1, norm_type=cv2.NORM_MINMAX))\n",
    "    return np.array(imgs)\n",
    "\n",
    "def get_more_images(imgs):\n",
    "    more_images = []\n",
    "    vert_flip_imgs = []\n",
    "    hori_flip_imgs = []\n",
    "    vh_flip_imgs = []\n",
    "      \n",
    "    for i in range(0,imgs.shape[0]):\n",
    "        vert_flip_imgs.append(cv2.flip(imgs[i], 1))\n",
    "        hori_flip_imgs.append(cv2.flip(imgs[i], 0))\n",
    "        vh_flip_imgs.append(cv2.flip(imgs[i], -1))\n",
    "      \n",
    "    v = np.array(vert_flip_imgs)\n",
    "    h = np.array(hori_flip_imgs)\n",
    "    vh = np.array(vh_flip_imgs)\n",
    "       \n",
    "    more_images = np.concatenate((imgs,v,h, vh))\n",
    "    \n",
    "    return more_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_img(band_1, band_2, is_iceberg, angle = None):\n",
    "    if angle is None:\n",
    "        title_str = 'Iceberg' if is_iceberg == 1 else 'Ship'\n",
    "    else:\n",
    "        title_str = 'Iceberg-' + str(angle) if is_iceberg == 1 else 'Ship-' + str(angle)\n",
    "    fig = plt.figure(0, figsize=(10,10))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    ax.set_title(title_str + ' - Band 1')\n",
    "    ax.imshow(band_1,cmap='jet')\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    ax.set_title(title_str + ' - Band 2')\n",
    "    ax.imshow(band_2,cmap='jet')\n",
    "    plt.show()\n",
    "\n",
    "# implement functions to convert SAR data from decibel units to linear units and back again\n",
    "def decibel_to_linear(band):\n",
    "     # convert to linear units\n",
    "    return np.power(10,np.array(band)/10)\n",
    "\n",
    "def linear_to_decibel(band):\n",
    "    return 10*np.log10(band)\n",
    "\n",
    "# implement the Lee Filter for a band in an image already reshaped into the proper dimensions\n",
    "def lee_filter(band, window, var_noise = 0.25):\n",
    "    # band: SAR data to be despeckled (already reshaped into image dimensions)\n",
    "    # window: descpeckling filter window (tuple)\n",
    "    # default noise variance = 0.25\n",
    "    # assumes noise mean = 0\n",
    "    \n",
    "    mean_window = uniform_filter(band, window)\n",
    "    mean_sqr_window = uniform_filter(band**2, window)\n",
    "    var_window = mean_sqr_window - mean_window**2\n",
    "\n",
    "    weights = var_window / (var_window + var_noise)\n",
    "    band_filtered = mean_window + weights*(band - mean_window)\n",
    "    return band_filtered\n",
    "\n",
    "def apply_lee_filter(band_1_linear, band_2_linear, window_var_index = 0, noise_var_index = 0):\n",
    "    windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "    noise_var = np.array([1, 2, 4])\n",
    "    noise_var_1 = np.round(np.var(band_1_linear) * noise_var, 10)\n",
    "    noise_var_2 = np.round(np.var(band_2_linear) * noise_var, 10)\n",
    "    band_1_linear_filtered = lee_filter(band_1_linear, windows[window_var_index], noise_var_1[noise_var_index])\n",
    "    band_2_linear_filtered = lee_filter(band_2_linear, windows[window_var_index], noise_var_2[noise_var_index])\n",
    "    return band_1_linear_filtered, band_2_linear_filtered\n",
    "\n",
    "def apply_lee_filter_single(band_linear, window_var_index = 0, noise_var_index = 0):\n",
    "    windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "    noise_var = np.array([1, 2, 4])\n",
    "    noise_var = np.round(np.var(band_linear) * noise_var, 10)\n",
    "    band_linear_filtered = lee_filter(band_linear, windows[window_var_index], noise_var[noise_var_index])\n",
    "    return band_linear_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(\"E:/kaggle/iceberg/train.json/data/processed/train.json\")\n",
    "test = pd.read_json(\"E:/kaggle/iceberg/test.json/data/processed/test.json\")\n",
    "train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')\n",
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_train=np.array(train['is_iceberg'])\n",
    "train['inc_angle']=train['inc_angle'].fillna(method='pad')\n",
    "test['inc_angle']=test['inc_angle'].fillna(method='pad')\n",
    "# train[\"inc_angle\"] = train[\"inc_angle\"].replace('na',0)\n",
    "# idx_tr = np.where(train[\"inc_angle\"]>0)\n",
    "# train = train.iloc[idx_tr[0]]\n",
    "# target_train = target_train.iloc[idx_tr[0]]\n",
    "X_angle=train['inc_angle']\n",
    "X_test_angle=test['inc_angle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the training data\n",
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "#apply filter\n",
    "X_band_1_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_1])\n",
    "X_band_2_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_2])\n",
    "X_band_1_filtered = linear_to_decibel(X_band_1_filtered)\n",
    "X_band_2_filtered = linear_to_decibel(X_band_2_filtered)\n",
    "X_band_1 = X_band_1_filtered\n",
    "X_band_2 = X_band_2_filtered\n",
    "\n",
    "X_band_3=np.fabs(np.subtract(X_band_1,X_band_2))\n",
    "X_band_4=np.maximum(X_band_1,X_band_2)\n",
    "X_band_5=np.minimum(X_band_1,X_band_2)\n",
    "\n",
    "X_band_3 = np_get_scaled_band(X_band_3)\n",
    "X_band_4 = np_get_scaled_band(X_band_4)\n",
    "X_band_5 = np_get_scaled_band(X_band_5)\n",
    "\n",
    "X_train = np.concatenate([X_band_3[:, :, :, np.newaxis],X_band_4[:, :, :, np.newaxis],X_band_5[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "#apply filter\n",
    "X_band_test_1_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_test_1])\n",
    "X_band_test_2_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_test_2])\n",
    "X_band_test_1_filtered = linear_to_decibel(X_band_test_1_filtered)\n",
    "X_band_test_2_filtered = linear_to_decibel(X_band_test_2_filtered)\n",
    "X_band_test_1 = X_band_test_1_filtered\n",
    "X_band_test_2 = X_band_test_2_filtered\n",
    "\n",
    "X_band_test_3=np.fabs(np.subtract(X_band_test_1,X_band_test_2))\n",
    "X_band_test_4=np.maximum(X_band_test_1,X_band_test_2)\n",
    "X_band_test_5=np.minimum(X_band_test_1,X_band_test_2)\n",
    "\n",
    "X_band_test_3 = np_get_scaled_band(X_band_test_3)\n",
    "X_band_test_4 = np_get_scaled_band(X_band_test_4)\n",
    "X_band_test_5 = np_get_scaled_band(X_band_test_5)\n",
    "\n",
    "X_test = np.concatenate([X_band_test_3[:, :, :, np.newaxis], X_band_test_4[:, :, :, np.newaxis],X_band_test_5[:, :, :, np.newaxis]],axis=-1)\n",
    "\n",
    "Xtrain = X_train\n",
    "Ytrain = target_train\n",
    "Xtest = X_test\n",
    "Xangle = X_angle\n",
    "Xangle_test = X_test_angle\n",
    "df_train = train\n",
    "df_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resize_shape = tuple(np.array(Xtrain.shape[1:3]) * 2)\n",
    "# Xtrain = np.array([cv2.resize(img, resize_shape) for img in Xtrain])\n",
    "# Xtest = np.array([cv2.resize(img, resize_shape) for img in Xtest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 75, 75, 3) (1604,) (1604,) (8424, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape, Ytrain.shape, Xangle.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    angle_input = Input(shape=[1], name=\"angle\")\n",
    "    angle_layer = Dense(1, )(angle_input)\n",
    "    vgg16_model = VGG16(weights=None, include_top=False, input_shape=Xtrain.shape[1:], pooling=\"max\")\n",
    "    mobile_model = MobileNet(weights=None, include_top=False, input_tensor = vgg16_model.input, input_shape=X_train.shape[1:], pooling=\"max\")\n",
    "#     incept_model = InceptionV3(include_top=False, input_tensor = vgg16_model.input, input_shape=Xtrain.shape[1:], pooling=\"max\")\n",
    "    xception_model = Xception(weights=None, include_top=False, input_tensor = vgg16_model.input, input_shape=X_train.shape[1:], pooling=\"max\")\n",
    "    \n",
    "    regularizer1 = l1(0.05)\n",
    "    regularizer2 = l1(0.08)\n",
    "    \n",
    "    x1 = vgg16_model.output\n",
    "#     x1 = Flatten()(x1)\n",
    "#     x1 = Dropout(0.3)(x1)\n",
    "    x1 = Concatenate()([x1, angle_layer])\n",
    "    x1 = Dense(512, activation='relu', kernel_regularizer=regularizer1)(x1)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    \n",
    "    x2 = mobile_model.output\n",
    "#     x2 = Flatten()(x2)\n",
    "#     x2 = Dropout(0.3)(x2)\n",
    "    x2 = Concatenate()([x2, angle_layer])\n",
    "    x2 = Dense(512, activation='relu', kernel_regularizer=regularizer1)(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    \n",
    "    x3 = xception_model.output\n",
    "#     x3 = Flatten()(x3)\n",
    "#     x3 = Dropout(0.3)(x3)\n",
    "    x3 = Concatenate()([x3, angle_layer])\n",
    "    x3 = Dense(512, activation='relu', kernel_regularizer=regularizer2)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "    \n",
    "    x = Concatenate()([x1, x2, x3, angle_layer])\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(1, activation='sigmoid', name='predictions')(x)\n",
    "    model = Model(inputs=[vgg16_model.input, angle_input], outputs=predictions)\n",
    "    optimizer = Adam(lr=1e-4)\n",
    "    log_loss = keras.losses.binary_crossentropy\n",
    "    log_loss.__name__ = \"log_loss\"\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy', log_loss])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 75, 75, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)            (None, 37, 37, 32)    864         input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormalizat (None, 37, 37, 32)    128         block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)    (None, 37, 37, 32)    0           block1_conv1_bn[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)            (None, 35, 35, 64)    18432       block1_conv1_act[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormalizat (None, 35, 35, 64)    256         block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)    (None, 35, 35, 64)    0           block1_conv2_bn[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2D (None, 35, 35, 128)   8768        block1_conv2_act[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormali (None, 35, 35, 128)   512         block2_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation) (None, 35, 35, 128)   0           block2_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2D (None, 35, 35, 128)   17536       block2_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormali (None, 35, 35, 128)   512         block2_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, 18, 18, 128)   8192        block1_conv2_act[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, 18, 18, 128)   0           block2_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 18, 18, 128)   512         conv2d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_37 (Add)                     (None, 18, 18, 128)   0           block2_pool[0][0]                \n",
      "                                                                   batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation) (None, 18, 18, 128)   0           add_37[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2D (None, 18, 18, 256)   33920       block3_sepconv1_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormali (None, 18, 18, 256)   1024        block3_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation) (None, 18, 18, 256)   0           block3_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2D (None, 18, 18, 256)   67840       block3_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormali (None, 18, 18, 256)   1024        block3_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)               (None, 9, 9, 256)     32768       add_37[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, 9, 9, 256)     0           block3_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 9, 9, 256)     1024        conv2d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_38 (Add)                     (None, 9, 9, 256)     0           block3_pool[0][0]                \n",
      "                                                                   batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation) (None, 9, 9, 256)     0           add_38[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2D (None, 9, 9, 728)     188672      block4_sepconv1_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormali (None, 9, 9, 728)     2912        block4_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation) (None, 9, 9, 728)     0           block4_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2D (None, 9, 9, 728)     536536      block4_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormali (None, 9, 9, 728)     2912        block4_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)               (None, 5, 5, 728)     186368      add_38[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)       (None, 5, 5, 728)     0           block4_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNor (None, 5, 5, 728)     2912        conv2d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_39 (Add)                     (None, 5, 5, 728)     0           block4_pool[0][0]                \n",
      "                                                                   batch_normalization_15[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation) (None, 5, 5, 728)     0           add_39[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2D (None, 5, 5, 728)     536536      block5_sepconv1_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormali (None, 5, 5, 728)     2912        block5_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation) (None, 5, 5, 728)     0           block5_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2D (None, 5, 5, 728)     536536      block5_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormali (None, 5, 5, 728)     2912        block5_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation) (None, 5, 5, 728)     0           block5_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2D (None, 5, 5, 728)     536536      block5_sepconv3_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormali (None, 5, 5, 728)     2912        block5_sepconv3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "add_40 (Add)                     (None, 5, 5, 728)     0           block5_sepconv3_bn[0][0]         \n",
      "                                                                   add_39[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation) (None, 5, 5, 728)     0           add_40[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2D (None, 5, 5, 728)     536536      block6_sepconv1_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormali (None, 5, 5, 728)     2912        block6_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, 38, 38, 32)    864         input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation) (None, 5, 5, 728)     0           block6_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)    (None, 38, 38, 32)    128         conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2D (None, 5, 5, 728)     536536      block6_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv1_relu (Activation)          (None, 38, 38, 32)    0           conv1_bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormali (None, 5, 5, 728)     2912        block6_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)      (None, 38, 38, 32)    288         conv1_relu[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation) (None, 5, 5, 728)     0           block6_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormalization (None, 38, 38, 32)    128         conv_dw_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2D (None, 5, 5, 728)     536536      block6_sepconv3_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_1_relu (Activation)      (None, 38, 38, 32)    0           conv_dw_1_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormali (None, 5, 5, 728)     2912        block6_sepconv3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_1 (Conv2D)               (None, 38, 38, 64)    2048        conv_dw_1_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_41 (Add)                     (None, 5, 5, 728)     0           block6_sepconv3_bn[0][0]         \n",
      "                                                                   add_40[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormalization (None, 38, 38, 64)    256         conv_pw_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation) (None, 5, 5, 728)     0           add_41[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_1_relu (Activation)      (None, 38, 38, 64)    0           conv_pw_1_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2D (None, 5, 5, 728)     536536      block7_sepconv1_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)      (None, 19, 19, 64)    576         conv_pw_1_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormali (None, 5, 5, 728)     2912        block7_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormalization (None, 19, 19, 64)    256         conv_dw_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation) (None, 5, 5, 728)     0           block7_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_2_relu (Activation)      (None, 19, 19, 64)    0           conv_dw_2_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2D (None, 5, 5, 728)     536536      block7_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_2 (Conv2D)               (None, 19, 19, 128)   8192        conv_dw_2_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormali (None, 5, 5, 728)     2912        block7_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormalization (None, 19, 19, 128)   512         conv_pw_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation) (None, 5, 5, 728)     0           block7_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_2_relu (Activation)      (None, 19, 19, 128)   0           conv_pw_2_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2D (None, 5, 5, 728)     536536      block7_sepconv3_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)      (None, 19, 19, 128)   1152        conv_pw_2_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormali (None, 5, 5, 728)     2912        block7_sepconv3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormalization (None, 19, 19, 128)   512         conv_dw_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_42 (Add)                     (None, 5, 5, 728)     0           block7_sepconv3_bn[0][0]         \n",
      "                                                                   add_41[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_3_relu (Activation)      (None, 19, 19, 128)   0           conv_dw_3_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation) (None, 5, 5, 728)     0           add_42[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_3 (Conv2D)               (None, 19, 19, 128)   16384       conv_dw_3_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2D (None, 5, 5, 728)     536536      block8_sepconv1_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormalization (None, 19, 19, 128)   512         conv_pw_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormali (None, 5, 5, 728)     2912        block8_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_3_relu (Activation)      (None, 19, 19, 128)   0           conv_pw_3_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation) (None, 5, 5, 728)     0           block8_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)      (None, 10, 10, 128)   1152        conv_pw_3_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2D (None, 5, 5, 728)     536536      block8_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormalization (None, 10, 10, 128)   512         conv_dw_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormali (None, 5, 5, 728)     2912        block8_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_4_relu (Activation)      (None, 10, 10, 128)   0           conv_dw_4_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation) (None, 5, 5, 728)     0           block8_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_4 (Conv2D)               (None, 10, 10, 256)   32768       conv_dw_4_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2D (None, 5, 5, 728)     536536      block8_sepconv3_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormalization (None, 10, 10, 256)   1024        conv_pw_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormali (None, 5, 5, 728)     2912        block8_sepconv3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_4_relu (Activation)      (None, 10, 10, 256)   0           conv_pw_4_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_43 (Add)                     (None, 5, 5, 728)     0           block8_sepconv3_bn[0][0]         \n",
      "                                                                   add_42[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)      (None, 10, 10, 256)   2304        conv_pw_4_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation) (None, 5, 5, 728)     0           add_43[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormalization (None, 10, 10, 256)   1024        conv_dw_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2D (None, 5, 5, 728)     536536      block9_sepconv1_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_5_relu (Activation)      (None, 10, 10, 256)   0           conv_dw_5_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormali (None, 5, 5, 728)     2912        block9_sepconv1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_5 (Conv2D)               (None, 10, 10, 256)   65536       conv_dw_5_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation) (None, 5, 5, 728)     0           block9_sepconv1_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormalization (None, 10, 10, 256)   1024        conv_pw_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2D (None, 5, 5, 728)     536536      block9_sepconv2_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_5_relu (Activation)      (None, 10, 10, 256)   0           conv_pw_5_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormali (None, 5, 5, 728)     2912        block9_sepconv2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)      (None, 5, 5, 256)     2304        conv_pw_5_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation) (None, 5, 5, 728)     0           block9_sepconv2_bn[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormalization (None, 5, 5, 256)     1024        conv_dw_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2D (None, 5, 5, 728)     536536      block9_sepconv3_act[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_6_relu (Activation)      (None, 5, 5, 256)     0           conv_dw_6_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormali (None, 5, 5, 728)     2912        block9_sepconv3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_6 (Conv2D)               (None, 5, 5, 512)     131072      conv_dw_6_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_44 (Add)                     (None, 5, 5, 728)     0           block9_sepconv3_bn[0][0]         \n",
      "                                                                   add_43[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormalization (None, 5, 5, 512)     2048        conv_pw_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activation (None, 5, 5, 728)     0           add_44[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_6_relu (Activation)      (None, 5, 5, 512)     0           conv_pw_6_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv2 (None, 5, 5, 728)     536536      block10_sepconv1_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)      (None, 5, 5, 512)     4608        conv_pw_6_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNormal (None, 5, 5, 728)     2912        block10_sepconv1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormalization (None, 5, 5, 512)     2048        conv_dw_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activation (None, 5, 5, 728)     0           block10_sepconv1_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_7_relu (Activation)      (None, 5, 5, 512)     0           conv_dw_7_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv2 (None, 5, 5, 728)     536536      block10_sepconv2_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_7 (Conv2D)               (None, 5, 5, 512)     262144      conv_dw_7_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNormal (None, 5, 5, 728)     2912        block10_sepconv2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormalization (None, 5, 5, 512)     2048        conv_pw_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activation (None, 5, 5, 728)     0           block10_sepconv2_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_7_relu (Activation)      (None, 5, 5, 512)     0           conv_pw_7_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv2 (None, 5, 5, 728)     536536      block10_sepconv3_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)      (None, 5, 5, 512)     4608        conv_pw_7_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNormal (None, 5, 5, 728)     2912        block10_sepconv3[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormalization (None, 5, 5, 512)     2048        conv_dw_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_45 (Add)                     (None, 5, 5, 728)     0           block10_sepconv3_bn[0][0]        \n",
      "                                                                   add_44[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_8_relu (Activation)      (None, 5, 5, 512)     0           conv_dw_8_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activation (None, 5, 5, 728)     0           add_45[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_8 (Conv2D)               (None, 5, 5, 512)     262144      conv_dw_8_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv2 (None, 5, 5, 728)     536536      block11_sepconv1_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormalization (None, 5, 5, 512)     2048        conv_pw_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNormal (None, 5, 5, 728)     2912        block11_sepconv1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_8_relu (Activation)      (None, 5, 5, 512)     0           conv_pw_8_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activation (None, 5, 5, 728)     0           block11_sepconv1_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)      (None, 5, 5, 512)     4608        conv_pw_8_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv2 (None, 5, 5, 728)     536536      block11_sepconv2_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormalization (None, 5, 5, 512)     2048        conv_dw_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNormal (None, 5, 5, 728)     2912        block11_sepconv2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_9_relu (Activation)      (None, 5, 5, 512)     0           conv_dw_9_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activation (None, 5, 5, 728)     0           block11_sepconv2_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_9 (Conv2D)               (None, 5, 5, 512)     262144      conv_dw_9_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv2 (None, 5, 5, 728)     536536      block11_sepconv3_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormalization (None, 5, 5, 512)     2048        conv_pw_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNormal (None, 5, 5, 728)     2912        block11_sepconv3[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_9_relu (Activation)      (None, 5, 5, 512)     0           conv_pw_9_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_46 (Add)                     (None, 5, 5, 728)     0           block11_sepconv3_bn[0][0]        \n",
      "                                                                   add_45[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D)     (None, 5, 5, 512)     4608        conv_pw_9_relu[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activation (None, 5, 5, 728)     0           add_46[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormalizatio (None, 5, 5, 512)     2048        conv_dw_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv2 (None, 5, 5, 728)     536536      block12_sepconv1_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_10_relu (Activation)     (None, 5, 5, 512)     0           conv_dw_10_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNormal (None, 5, 5, 728)     2912        block12_sepconv1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_10 (Conv2D)              (None, 5, 5, 512)     262144      conv_dw_10_relu[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activation (None, 5, 5, 728)     0           block12_sepconv1_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormalizatio (None, 5, 5, 512)     2048        conv_pw_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv2 (None, 5, 5, 728)     536536      block12_sepconv2_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_10_relu (Activation)     (None, 5, 5, 512)     0           conv_pw_10_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNormal (None, 5, 5, 728)     2912        block12_sepconv2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1_ (Conv2D)           (None, 75, 75, 64)    1792        input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D)     (None, 5, 5, 512)     4608        conv_pw_10_relu[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activation (None, 5, 5, 728)     0           block12_sepconv2_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2_ (Conv2D)           (None, 75, 75, 64)    36928       block1_conv1_[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormalizatio (None, 5, 5, 512)     2048        conv_dw_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv2 (None, 5, 5, 728)     536536      block12_sepconv3_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block1_pool_ (MaxPooling2D)      (None, 37, 37, 64)    0           block1_conv2_[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_11_relu (Activation)     (None, 5, 5, 512)     0           conv_dw_11_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNormal (None, 5, 5, 728)     2912        block12_sepconv3[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1_ (Conv2D)           (None, 37, 37, 128)   73856       block1_pool_[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_11 (Conv2D)              (None, 5, 5, 512)     262144      conv_dw_11_relu[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "add_47 (Add)                     (None, 5, 5, 728)     0           block12_sepconv3_bn[0][0]        \n",
      "                                                                   add_46[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2_ (Conv2D)           (None, 37, 37, 128)   147584      block2_conv1_[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormalizatio (None, 5, 5, 512)     2048        conv_pw_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activation (None, 5, 5, 728)     0           add_47[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool_ (MaxPooling2D)      (None, 18, 18, 128)   0           block2_conv2_[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_11_relu (Activation)     (None, 5, 5, 512)     0           conv_pw_11_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv2 (None, 5, 5, 728)     536536      block13_sepconv1_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1_ (Conv2D)           (None, 18, 18, 256)   295168      block2_pool_[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D)     (None, 3, 3, 512)     4608        conv_pw_11_relu[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNormal (None, 5, 5, 728)     2912        block13_sepconv1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2_ (Conv2D)           (None, 18, 18, 256)   590080      block3_conv1_[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormalizatio (None, 3, 3, 512)     2048        conv_dw_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activation (None, 5, 5, 728)     0           block13_sepconv1_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv3_ (Conv2D)           (None, 18, 18, 256)   590080      block3_conv2_[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_12_relu (Activation)     (None, 3, 3, 512)     0           conv_dw_12_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv2 (None, 5, 5, 1024)    752024      block13_sepconv2_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool_ (MaxPooling2D)      (None, 9, 9, 256)     0           block3_conv3_[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_12 (Conv2D)              (None, 3, 3, 1024)    524288      conv_dw_12_relu[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNormal (None, 5, 5, 1024)    4096        block13_sepconv2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)               (None, 3, 3, 1024)    745472      add_47[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1_ (Conv2D)           (None, 9, 9, 512)     1180160     block3_pool_[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormalizatio (None, 3, 3, 1024)    4096        conv_pw_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)      (None, 3, 3, 1024)    0           block13_sepconv2_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 3, 3, 1024)    4096        conv2d_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2_ (Conv2D)           (None, 9, 9, 512)     2359808     block4_conv1_[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_12_relu (Activation)     (None, 3, 3, 1024)    0           conv_pw_12_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_48 (Add)                     (None, 3, 3, 1024)    0           block13_pool[0][0]               \n",
      "                                                                   batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv3_ (Conv2D)           (None, 9, 9, 512)     2359808     block4_conv2_[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D)     (None, 3, 3, 1024)    9216        conv_pw_12_relu[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv2 (None, 3, 3, 1536)    1582080     add_48[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool_ (MaxPooling2D)      (None, 4, 4, 512)     0           block4_conv3_[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormalizatio (None, 3, 3, 1024)    4096        conv_dw_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNormal (None, 3, 3, 1536)    6144        block14_sepconv1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv1_ (Conv2D)           (None, 4, 4, 512)     2359808     block4_pool_[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv_dw_13_relu (Activation)     (None, 3, 3, 1024)    0           conv_dw_13_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activation (None, 3, 3, 1536)    0           block14_sepconv1_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv2_ (Conv2D)           (None, 4, 4, 512)     2359808     block5_conv1_[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_13 (Conv2D)              (None, 3, 3, 1024)    1048576     conv_dw_13_relu[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv2 (None, 3, 3, 2048)    3159552     block14_sepconv1_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv3_ (Conv2D)           (None, 4, 4, 512)     2359808     block5_conv2_[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormalizatio (None, 3, 3, 1024)    4096        conv_pw_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNormal (None, 3, 3, 2048)    8192        block14_sepconv2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "block5_pool_ (MaxPooling2D)      (None, 2, 2, 512)     0           block5_conv3_[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "angle (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_pw_13_relu (Activation)     (None, 3, 3, 1024)    0           conv_pw_13_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activation (None, 3, 3, 2048)    0           block14_sepconv2_bn[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling2d_6 (GlobalMa (None, 512)           0           block5_pool_[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 1)             2           angle[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling2d_7 (GlobalMa (None, 1024)          0           conv_pw_13_relu[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling2d_8 (GlobalMa (None, 2048)          0           block14_sepconv2_act[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)     (None, 513)           0           global_max_pooling2d_6[0][0]     \n",
      "                                                                   dense_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)     (None, 1025)          0           global_max_pooling2d_7[0][0]     \n",
      "                                                                   dense_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)     (None, 2049)          0           global_max_pooling2d_8[0][0]     \n",
      "                                                                   dense_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 512)           263168      concatenate_13[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 512)           525312      concatenate_14[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 512)           1049600     concatenate_15[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 512)           0           dense_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)             (None, 512)           0           dense_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)             (None, 512)           0           dense_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)     (None, 1537)          0           dropout_13[0][0]                 \n",
      "                                                                   dropout_14[0][0]                 \n",
      "                                                                   dropout_15[0][0]                 \n",
      "                                                                   dense_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)             (None, 1537)          0           concatenate_16[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "predictions (Dense)              (None, 1)             1538        dropout_16[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 40,644,652\n",
      "Trainable params: 40,568,236\n",
      "Non-trainable params: 76,416\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = getModel()\n",
    "model.summary()\n",
    "plot_model(model, to_file=\"vgg16.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the image transformations here\n",
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = True,\n",
    "                         width_shift_range = 0.2,\n",
    "                         height_shift_range = 0.2,\n",
    "                         channel_shift_range=0,\n",
    "                         zoom_range = 0.5,\n",
    "                         rotation_range = 15)\n",
    "\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_two_inputs(X1, X2, y):\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size, seed=SEED)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size, seed=SEED)\n",
    "    while True:\n",
    "        X1i = genX1.next()\n",
    "        X2i = genX2.next()\n",
    "        yield [X1i[0], X2i[1]], X1i[1]\n",
    "\n",
    "def get_callbacks(filepath):\n",
    "    es = EarlyStopping('val_loss', patience=30, mode=\"min\")\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave, reduce_lr_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# K=3\n",
    "# epochs = 150\n",
    "# batch_size = 64\n",
    "# Kfolds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED).split(Xtrain, Ytrain))\n",
    "# y_test_pred_log = 0\n",
    "# for j, (train_idx, test_idx) in enumerate(Kfolds):\n",
    "#     print('\\n===================FOLD=',j)\n",
    "#     Xtrain_cv = Xtrain[train_idx]\n",
    "#     Ytrain_cv = Ytrain[train_idx]\n",
    "#     Xangle_cv = Xangle[train_idx]\n",
    "#     Xtrain_val = Xtrain[test_idx]\n",
    "#     Ytrain_val = Ytrain[test_idx]\n",
    "#     Xangle_val = Xangle[test_idx]\n",
    "    \n",
    "#     Xtrain_input = [Xtrain, Xangle]\n",
    "#     Xval_input = [Xtrain_val, Xangle_val]\n",
    "#     Xtest_input = [Xtest, Xangle_test]\n",
    "    \n",
    "#     model_file = 'vgg16_%s.hdf5' % j\n",
    "\n",
    "#     model = getModel()\n",
    "#     steps = np.ceil(len(Xtrain_cv) / batch_size) * 3\n",
    "#     model.fit_generator(gen_flow_for_two_inputs(Xtrain_cv, Xangle_cv, Ytrain_cv), \n",
    "#                         steps_per_epoch=steps, epochs=epochs, verbose=1, shuffle=True, \n",
    "#                         callbacks=get_callbacks(model_file), validation_data=(Xval_input, Ytrain_val))\n",
    "    \n",
    "#     model.load_weights(filepath = model_file)    \n",
    "    \n",
    "#     train_score = model.evaluate(Xtrain_input, Ytrain, verbose=1)\n",
    "#     val_score = model.evaluate(Xval_input, Ytrain_val, verbose=1)\n",
    "#     print('Train score:', train_score[0], ' ;Train accuracy:', train_score[1])\n",
    "#     print('Valid score:', val_score[0], ' ;Valid accuracy:', val_score[1])\n",
    "#     y_test_pred_log += model.predict(Xtest_input, verbose=1).reshape(Xtest.shape[0])\n",
    "    \n",
    "# y_test_pred_log /= K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "54/54 [==============================] - 198s - loss: 3045.5428 - acc: 0.5205 - log_loss: 1.2430 - val_loss: 2726.8255 - val_acc: 0.4917 - val_log_loss: 0.7372\n",
      "Epoch 2/400\n",
      "54/54 [==============================] - 31s - loss: 2443.0998 - acc: 0.5801 - log_loss: 1.0130 - val_loss: 2159.9549 - val_acc: 0.4917 - val_log_loss: 0.7256\n",
      "Epoch 3/400\n",
      "54/54 [==============================] - 31s - loss: 1910.2131 - acc: 0.7037 - log_loss: 0.7179 - val_loss: 1662.6844 - val_acc: 0.4917 - val_log_loss: 0.7154\n",
      "Epoch 4/400\n",
      "54/54 [==============================] - 31s - loss: 1446.6253 - acc: 0.7350 - log_loss: 0.6250 - val_loss: 1234.1272 - val_acc: 0.4917 - val_log_loss: 0.6973\n",
      "Epoch 5/400\n",
      "54/54 [==============================] - 31s - loss: 1051.4915 - acc: 0.7571 - log_loss: 0.5677 - val_loss: 873.7345 - val_acc: 0.4917 - val_log_loss: 0.7147\n",
      "Epoch 6/400\n",
      "54/54 [==============================] - 32s - loss: 724.4256 - acc: 0.7708 - log_loss: 0.5154 - val_loss: 581.3531 - val_acc: 0.4917 - val_log_loss: 0.6994\n",
      "Epoch 7/400\n",
      "54/54 [==============================] - 30s - loss: 465.3466 - acc: 0.7587 - log_loss: 0.5216 - val_loss: 356.8154 - val_acc: 0.4917 - val_log_loss: 0.7038\n",
      "Epoch 8/400\n",
      "54/54 [==============================] - 31s - loss: 274.0890 - acc: 0.7650 - log_loss: 0.5007 - val_loss: 200.1745 - val_acc: 0.4917 - val_log_loss: 0.7090\n",
      "Epoch 9/400\n",
      "54/54 [==============================] - 31s - loss: 150.8940 - acc: 0.6912 - log_loss: 0.6019 - val_loss: 111.2018 - val_acc: 0.4917 - val_log_loss: 0.7013\n",
      "Epoch 10/400\n",
      "54/54 [==============================] - 30s - loss: 85.3678 - acc: 0.5718 - log_loss: 0.7315 - val_loss: 61.7629 - val_acc: 0.5062 - val_log_loss: 0.6501\n",
      "Epoch 11/400\n",
      "54/54 [==============================] - 31s - loss: 44.9085 - acc: 0.6444 - log_loss: 0.6588 - val_loss: 30.3411 - val_acc: 0.6432 - val_log_loss: 0.6250\n",
      "Epoch 12/400\n",
      "54/54 [==============================] - 30s - loss: 21.5671 - acc: 0.7143 - log_loss: 0.5489 - val_loss: 14.4980 - val_acc: 0.8050 - val_log_loss: 0.4359\n",
      "Epoch 13/400\n",
      "54/54 [==============================] - 30s - loss: 10.0880 - acc: 0.7494 - log_loss: 0.5019 - val_loss: 6.1647 - val_acc: 0.8299 - val_log_loss: 0.3967\n",
      "Epoch 14/400\n",
      "54/54 [==============================] - 31s - loss: 4.1534 - acc: 0.7622 - log_loss: 0.4852 - val_loss: 2.6521 - val_acc: 0.8340 - val_log_loss: 0.3649\n",
      "Epoch 15/400\n",
      "54/54 [==============================] - 30s - loss: 2.5703 - acc: 0.7986 - log_loss: 0.4499 - val_loss: 2.3798 - val_acc: 0.8340 - val_log_loss: 0.3964\n",
      "Epoch 16/400\n",
      "54/54 [==============================] - 31s - loss: 2.3508 - acc: 0.8114 - log_loss: 0.4166 - val_loss: 2.2009 - val_acc: 0.8631 - val_log_loss: 0.3184\n",
      "Epoch 17/400\n",
      "54/54 [==============================] - 29s - loss: 2.2856 - acc: 0.8145 - log_loss: 0.4005 - val_loss: 2.1259 - val_acc: 0.8651 - val_log_loss: 0.2992\n",
      "Epoch 18/400\n",
      "54/54 [==============================] - 24s - loss: 2.2097 - acc: 0.8147 - log_loss: 0.3952 - val_loss: 2.1671 - val_acc: 0.8651 - val_log_loss: 0.3005\n",
      "Epoch 19/400\n",
      "54/54 [==============================] - 31s - loss: 2.1152 - acc: 0.8426 - log_loss: 0.3608 - val_loss: 2.0314 - val_acc: 0.8610 - val_log_loss: 0.3202\n",
      "Epoch 20/400\n",
      "54/54 [==============================] - 30s - loss: 2.0538 - acc: 0.8529 - log_loss: 0.3463 - val_loss: 1.9578 - val_acc: 0.8776 - val_log_loss: 0.2640\n",
      "Epoch 21/400\n",
      "54/54 [==============================] - 30s - loss: 1.9849 - acc: 0.8561 - log_loss: 0.3281 - val_loss: 1.9193 - val_acc: 0.8859 - val_log_loss: 0.2699\n",
      "Epoch 22/400\n",
      "54/54 [==============================] - 24s - loss: 1.9589 - acc: 0.8620 - log_loss: 0.3315 - val_loss: 1.9372 - val_acc: 0.8755 - val_log_loss: 0.2902\n",
      "Epoch 23/400\n",
      "54/54 [==============================] - 29s - loss: 1.9436 - acc: 0.8579 - log_loss: 0.3189 - val_loss: 1.8483 - val_acc: 0.8817 - val_log_loss: 0.2615\n",
      "Epoch 24/400\n",
      "54/54 [==============================] - 24s - loss: 1.8846 - acc: 0.8670 - log_loss: 0.3019 - val_loss: 1.8513 - val_acc: 0.8817 - val_log_loss: 0.2769\n",
      "Epoch 25/400\n",
      "54/54 [==============================] - 29s - loss: 1.8557 - acc: 0.8707 - log_loss: 0.2946 - val_loss: 1.8368 - val_acc: 0.8734 - val_log_loss: 0.2792\n",
      "Epoch 26/400\n",
      "54/54 [==============================] - 24s - loss: 1.8397 - acc: 0.8700 - log_loss: 0.2910 - val_loss: 1.8643 - val_acc: 0.8485 - val_log_loss: 0.3360\n",
      "Epoch 27/400\n",
      "54/54 [==============================] - 24s - loss: 1.8210 - acc: 0.8678 - log_loss: 0.2926 - val_loss: 1.8656 - val_acc: 0.8527 - val_log_loss: 0.3422\n",
      "Epoch 28/400\n",
      "54/54 [==============================] - 30s - loss: 1.8054 - acc: 0.8727 - log_loss: 0.2890 - val_loss: 1.7600 - val_acc: 0.8900 - val_log_loss: 0.2599\n",
      "Epoch 29/400\n",
      "54/54 [==============================] - 24s - loss: 1.7975 - acc: 0.8836 - log_loss: 0.2889 - val_loss: 1.7985 - val_acc: 0.8672 - val_log_loss: 0.2816\n",
      "Epoch 30/400\n",
      "54/54 [==============================] - 24s - loss: 1.7770 - acc: 0.8812 - log_loss: 0.2827 - val_loss: 1.7677 - val_acc: 0.8838 - val_log_loss: 0.2716\n",
      "Epoch 31/400\n",
      "54/54 [==============================] - 24s - loss: 1.7708 - acc: 0.8741 - log_loss: 0.2795 - val_loss: 1.7661 - val_acc: 0.8859 - val_log_loss: 0.2699\n",
      "Epoch 32/400\n",
      "54/54 [==============================] - 29s - loss: 1.7614 - acc: 0.8775 - log_loss: 0.2777 - val_loss: 1.7471 - val_acc: 0.8714 - val_log_loss: 0.2681\n",
      "Epoch 33/400\n",
      "54/54 [==============================] - 29s - loss: 1.7349 - acc: 0.8870 - log_loss: 0.2649 - val_loss: 1.7111 - val_acc: 0.8921 - val_log_loss: 0.2589\n",
      "Epoch 34/400\n",
      "54/54 [==============================] - 24s - loss: 1.7422 - acc: 0.8795 - log_loss: 0.2768 - val_loss: 1.7729 - val_acc: 0.8880 - val_log_loss: 0.2825\n",
      "Epoch 35/400\n",
      "54/54 [==============================] - 24s - loss: 1.7302 - acc: 0.8891 - log_loss: 0.2625 - val_loss: 1.7206 - val_acc: 0.8942 - val_log_loss: 0.2604\n",
      "Epoch 36/400\n",
      "54/54 [==============================] - 24s - loss: 1.7132 - acc: 0.8929 - log_loss: 0.2587 - val_loss: 1.7761 - val_acc: 0.8672 - val_log_loss: 0.3091\n",
      "Epoch 37/400\n",
      "54/54 [==============================] - 25s - loss: 1.7073 - acc: 0.8890 - log_loss: 0.2546 - val_loss: 1.7510 - val_acc: 0.8797 - val_log_loss: 0.3120\n",
      "Epoch 38/400\n",
      "54/54 [==============================] - 25s - loss: 1.7259 - acc: 0.8883 - log_loss: 0.2726 - val_loss: 1.7202 - val_acc: 0.8838 - val_log_loss: 0.2756\n",
      "Epoch 39/400\n",
      "54/54 [==============================] - 31s - loss: 1.7044 - acc: 0.8867 - log_loss: 0.2597 - val_loss: 1.7104 - val_acc: 0.8921 - val_log_loss: 0.2745\n",
      "Epoch 40/400\n",
      "54/54 [==============================] - 24s - loss: 1.6853 - acc: 0.8899 - log_loss: 0.2454 - val_loss: 1.8005 - val_acc: 0.8817 - val_log_loss: 0.3619\n",
      "Epoch 41/400\n",
      "54/54 [==============================] - 24s - loss: 1.6980 - acc: 0.8873 - log_loss: 0.2578 - val_loss: 1.7594 - val_acc: 0.8776 - val_log_loss: 0.3238\n",
      "Epoch 42/400\n",
      "54/54 [==============================] - 24s - loss: 1.7031 - acc: 0.8904 - log_loss: 0.2587 - val_loss: 1.7300 - val_acc: 0.8797 - val_log_loss: 0.2889\n",
      "Epoch 43/400\n",
      "54/54 [==============================] - 30s - loss: 1.6729 - acc: 0.8989 - log_loss: 0.2391 - val_loss: 1.6927 - val_acc: 0.8921 - val_log_loss: 0.2677\n",
      "Epoch 44/400\n",
      "54/54 [==============================] - 24s - loss: 1.6594 - acc: 0.9016 - log_loss: 0.2358 - val_loss: 1.7519 - val_acc: 0.8506 - val_log_loss: 0.3370\n",
      "Epoch 45/400\n",
      "54/54 [==============================] - 24s - loss: 1.6791 - acc: 0.9002 - log_loss: 0.2429 - val_loss: 1.7763 - val_acc: 0.8880 - val_log_loss: 0.3374\n",
      "Epoch 46/400\n",
      "54/54 [==============================] - 24s - loss: 1.6865 - acc: 0.8932 - log_loss: 0.2470 - val_loss: 1.7503 - val_acc: 0.8900 - val_log_loss: 0.3181\n",
      "Epoch 47/400\n",
      "54/54 [==============================] - 24s - loss: 1.6656 - acc: 0.8951 - log_loss: 0.2396 - val_loss: 1.7027 - val_acc: 0.8900 - val_log_loss: 0.2764\n",
      "Epoch 48/400\n",
      "54/54 [==============================] - 24s - loss: 1.6437 - acc: 0.9042 - log_loss: 0.2230 - val_loss: 1.7045 - val_acc: 0.8838 - val_log_loss: 0.2890\n",
      "Epoch 49/400\n",
      "54/54 [==============================] - 24s - loss: 1.6517 - acc: 0.9043 - log_loss: 0.2293 - val_loss: 1.6959 - val_acc: 0.8942 - val_log_loss: 0.2701\n",
      "Epoch 50/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 24s - loss: 1.6723 - acc: 0.8971 - log_loss: 0.2422 - val_loss: 1.7160 - val_acc: 0.8900 - val_log_loss: 0.2879\n",
      "Epoch 51/400\n",
      "53/54 [============================>.] - ETA: 0s - loss: 1.6684 - acc: 0.8979 - log_loss: 0.2382\n",
      "Epoch 00050: reducing learning rate to 9.999999747378752e-06.\n",
      "54/54 [==============================] - 28s - loss: 1.6697 - acc: 0.8971 - log_loss: 0.2392 - val_loss: 1.7658 - val_acc: 0.9025 - val_log_loss: 0.3202\n",
      "Epoch 52/400\n",
      "54/54 [==============================] - 30s - loss: 0.5988 - acc: 0.9085 - log_loss: 0.2081 - val_loss: 0.4498 - val_acc: 0.8963 - val_log_loss: 0.2738\n",
      "Epoch 53/400\n",
      "54/54 [==============================] - 24s - loss: 0.3656 - acc: 0.9162 - log_loss: 0.1998 - val_loss: 0.4557 - val_acc: 0.8983 - val_log_loss: 0.2935\n",
      "Epoch 54/400\n",
      "54/54 [==============================] - 24s - loss: 0.3574 - acc: 0.9182 - log_loss: 0.1965 - val_loss: 0.4560 - val_acc: 0.8983 - val_log_loss: 0.2937\n",
      "Epoch 55/400\n",
      "54/54 [==============================] - 24s - loss: 0.3609 - acc: 0.9090 - log_loss: 0.2006 - val_loss: 0.4577 - val_acc: 0.8963 - val_log_loss: 0.2988\n",
      "Epoch 56/400\n",
      "54/54 [==============================] - 24s - loss: 0.3609 - acc: 0.9108 - log_loss: 0.2008 - val_loss: 0.4522 - val_acc: 0.8921 - val_log_loss: 0.2922\n",
      "Epoch 57/400\n",
      "54/54 [==============================] - 30s - loss: 0.3588 - acc: 0.9176 - log_loss: 0.1993 - val_loss: 0.4479 - val_acc: 0.8983 - val_log_loss: 0.2863\n",
      "Epoch 58/400\n",
      "54/54 [==============================] - 24s - loss: 0.3577 - acc: 0.9182 - log_loss: 0.1988 - val_loss: 0.4489 - val_acc: 0.9004 - val_log_loss: 0.2901\n",
      "Epoch 59/400\n",
      "54/54 [==============================] - 24s - loss: 0.3634 - acc: 0.9121 - log_loss: 0.2040 - val_loss: 0.4504 - val_acc: 0.8942 - val_log_loss: 0.2916\n",
      "Epoch 60/400\n",
      "54/54 [==============================] - 24s - loss: 0.3592 - acc: 0.9166 - log_loss: 0.2009 - val_loss: 0.4502 - val_acc: 0.9004 - val_log_loss: 0.2935\n",
      "Epoch 61/400\n",
      "54/54 [==============================] - 30s - loss: 0.3582 - acc: 0.9142 - log_loss: 0.2005 - val_loss: 0.4466 - val_acc: 0.8921 - val_log_loss: 0.2891\n",
      "Epoch 62/400\n",
      "54/54 [==============================] - 24s - loss: 0.3614 - acc: 0.9143 - log_loss: 0.2039 - val_loss: 0.4557 - val_acc: 0.8921 - val_log_loss: 0.2959\n",
      "Epoch 63/400\n",
      "54/54 [==============================] - 24s - loss: 0.3475 - acc: 0.9222 - log_loss: 0.1907 - val_loss: 0.4710 - val_acc: 0.8921 - val_log_loss: 0.3150\n",
      "Epoch 64/400\n",
      "54/54 [==============================] - 24s - loss: 0.3569 - acc: 0.9187 - log_loss: 0.1999 - val_loss: 0.4501 - val_acc: 0.8963 - val_log_loss: 0.2919\n",
      "Epoch 65/400\n",
      "54/54 [==============================] - 24s - loss: 0.3539 - acc: 0.9159 - log_loss: 0.1968 - val_loss: 0.4568 - val_acc: 0.8880 - val_log_loss: 0.2999\n",
      "Epoch 66/400\n",
      "54/54 [==============================] - 24s - loss: 0.3469 - acc: 0.9178 - log_loss: 0.1903 - val_loss: 0.4672 - val_acc: 0.8880 - val_log_loss: 0.3098\n",
      "Epoch 67/400\n",
      "54/54 [==============================] - 24s - loss: 0.3538 - acc: 0.9158 - log_loss: 0.1970 - val_loss: 0.4611 - val_acc: 0.8880 - val_log_loss: 0.3036\n",
      "Epoch 68/400\n",
      "54/54 [==============================] - 24s - loss: 0.3443 - acc: 0.9215 - log_loss: 0.1877 - val_loss: 0.4524 - val_acc: 0.8983 - val_log_loss: 0.2975\n",
      "Epoch 69/400\n",
      "53/54 [============================>.] - ETA: 0s - loss: 0.3515 - acc: 0.9136 - log_loss: 0.1953\n",
      "Epoch 00068: reducing learning rate to 9.999999747378752e-07.\n",
      "54/54 [==============================] - 24s - loss: 0.3501 - acc: 0.9147 - log_loss: 0.1940 - val_loss: 0.4545 - val_acc: 0.8942 - val_log_loss: 0.2965\n",
      "Epoch 70/400\n",
      "54/54 [==============================] - 29s - loss: 0.2357 - acc: 0.9180 - log_loss: 0.1885 - val_loss: 0.3244 - val_acc: 0.8942 - val_log_loss: 0.3000\n",
      "Epoch 71/400\n",
      "54/54 [==============================] - 29s - loss: 0.2162 - acc: 0.9194 - log_loss: 0.1929 - val_loss: 0.3213 - val_acc: 0.8880 - val_log_loss: 0.2983\n",
      "Epoch 72/400\n",
      "54/54 [==============================] - 24s - loss: 0.2179 - acc: 0.9202 - log_loss: 0.1949 - val_loss: 0.3228 - val_acc: 0.8900 - val_log_loss: 0.2997\n",
      "Epoch 73/400\n",
      "54/54 [==============================] - 24s - loss: 0.2076 - acc: 0.9249 - log_loss: 0.1847 - val_loss: 0.3262 - val_acc: 0.8921 - val_log_loss: 0.3035\n",
      "Epoch 74/400\n",
      "54/54 [==============================] - 24s - loss: 0.2050 - acc: 0.9253 - log_loss: 0.1822 - val_loss: 0.3231 - val_acc: 0.8942 - val_log_loss: 0.3002\n",
      "Epoch 75/400\n",
      "54/54 [==============================] - 24s - loss: 0.2135 - acc: 0.9177 - log_loss: 0.1907 - val_loss: 0.3224 - val_acc: 0.8942 - val_log_loss: 0.2994\n",
      "Epoch 76/400\n",
      "54/54 [==============================] - 30s - loss: 0.2166 - acc: 0.9177 - log_loss: 0.1939 - val_loss: 0.3191 - val_acc: 0.8942 - val_log_loss: 0.2963\n",
      "Epoch 77/400\n",
      "54/54 [==============================] - 24s - loss: 0.2068 - acc: 0.9235 - log_loss: 0.1841 - val_loss: 0.3237 - val_acc: 0.8942 - val_log_loss: 0.3009\n",
      "Epoch 78/400\n",
      "54/54 [==============================] - 24s - loss: 0.2054 - acc: 0.9212 - log_loss: 0.1826 - val_loss: 0.3210 - val_acc: 0.8942 - val_log_loss: 0.2984\n",
      "Epoch 79/400\n",
      "54/54 [==============================] - 25s - loss: 0.2096 - acc: 0.9220 - log_loss: 0.1869 - val_loss: 0.3226 - val_acc: 0.8942 - val_log_loss: 0.2998\n",
      "Epoch 80/400\n",
      "54/54 [==============================] - 24s - loss: 0.2139 - acc: 0.9169 - log_loss: 0.1912 - val_loss: 0.3215 - val_acc: 0.8942 - val_log_loss: 0.2986\n",
      "Epoch 81/400\n",
      "54/54 [==============================] - 24s - loss: 0.2114 - acc: 0.9221 - log_loss: 0.1887 - val_loss: 0.3197 - val_acc: 0.8942 - val_log_loss: 0.2971\n",
      "Epoch 82/400\n",
      "54/54 [==============================] - 24s - loss: 0.2080 - acc: 0.9228 - log_loss: 0.1853 - val_loss: 0.3205 - val_acc: 0.8942 - val_log_loss: 0.2977\n",
      "Epoch 83/400\n",
      "54/54 [==============================] - 24s - loss: 0.2128 - acc: 0.9271 - log_loss: 0.1901 - val_loss: 0.3207 - val_acc: 0.8942 - val_log_loss: 0.2981\n",
      "Epoch 84/400\n",
      "53/54 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9212 - log_loss: 0.1831\n",
      "Epoch 00083: reducing learning rate to 9.999999974752428e-08.\n",
      "54/54 [==============================] - 24s - loss: 0.2056 - acc: 0.9215 - log_loss: 0.1829 - val_loss: 0.3212 - val_acc: 0.8942 - val_log_loss: 0.2984\n",
      "Epoch 85/400\n",
      "54/54 [==============================] - 30s - loss: 0.1992 - acc: 0.9178 - log_loss: 0.1876 - val_loss: 0.3080 - val_acc: 0.8942 - val_log_loss: 0.2986\n",
      "Epoch 86/400\n",
      "54/54 [==============================] - 24s - loss: 0.2026 - acc: 0.9212 - log_loss: 0.1934 - val_loss: 0.3081 - val_acc: 0.8942 - val_log_loss: 0.2988\n",
      "Epoch 87/400\n",
      "54/54 [==============================] - 24s - loss: 0.1993 - acc: 0.9194 - log_loss: 0.1901 - val_loss: 0.3082 - val_acc: 0.8942 - val_log_loss: 0.2989\n",
      "Epoch 88/400\n",
      "54/54 [==============================] - 24s - loss: 0.2071 - acc: 0.9172 - log_loss: 0.1978 - val_loss: 0.3083 - val_acc: 0.8942 - val_log_loss: 0.2991\n",
      "Epoch 89/400\n",
      "54/54 [==============================] - 24s - loss: 0.1921 - acc: 0.9230 - log_loss: 0.1828 - val_loss: 0.3088 - val_acc: 0.8942 - val_log_loss: 0.2996\n",
      "Epoch 90/400\n",
      "54/54 [==============================] - 24s - loss: 0.1963 - acc: 0.9189 - log_loss: 0.1871 - val_loss: 0.3084 - val_acc: 0.8942 - val_log_loss: 0.2992\n",
      "Epoch 91/400\n",
      "54/54 [==============================] - 24s - loss: 0.1971 - acc: 0.9197 - log_loss: 0.1879 - val_loss: 0.3088 - val_acc: 0.8942 - val_log_loss: 0.2996\n",
      "Epoch 92/400\n",
      "54/54 [==============================] - 24s - loss: 0.2016 - acc: 0.9180 - log_loss: 0.1924 - val_loss: 0.3083 - val_acc: 0.8942 - val_log_loss: 0.2990\n",
      "Epoch 93/400\n",
      "53/54 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9204 - log_loss: 0.1841\n",
      "Epoch 00092: reducing learning rate to 1.0000000116860975e-08.\n",
      "54/54 [==============================] - 24s - loss: 0.1934 - acc: 0.9208 - log_loss: 0.1842 - val_loss: 0.3086 - val_acc: 0.8942 - val_log_loss: 0.2994\n",
      "Epoch 94/400\n",
      "54/54 [==============================] - 29s - loss: 0.1868 - acc: 0.9259 - log_loss: 0.1787 - val_loss: 0.3073 - val_acc: 0.8942 - val_log_loss: 0.2994\n",
      "Epoch 95/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 29s - loss: 0.2001 - acc: 0.9210 - log_loss: 0.1922 - val_loss: 0.3073 - val_acc: 0.8942 - val_log_loss: 0.2994\n",
      "Epoch 96/400\n",
      "54/54 [==============================] - 29s - loss: 0.1955 - acc: 0.9210 - log_loss: 0.1877 - val_loss: 0.3073 - val_acc: 0.8942 - val_log_loss: 0.2994\n",
      "Epoch 97/400\n",
      "54/54 [==============================] - 24s - loss: 0.1938 - acc: 0.9214 - log_loss: 0.1860 - val_loss: 0.3073 - val_acc: 0.8942 - val_log_loss: 0.2994\n",
      "Epoch 98/400\n",
      "54/54 [==============================] - 24s - loss: 0.1981 - acc: 0.9227 - log_loss: 0.1902 - val_loss: 0.3073 - val_acc: 0.8942 - val_log_loss: 0.2994\n",
      "Epoch 99/400\n",
      "54/54 [==============================] - 24s - loss: 0.1959 - acc: 0.9238 - log_loss: 0.1881 - val_loss: 0.3073 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 100/400\n",
      "54/54 [==============================] - 24s - loss: 0.1978 - acc: 0.9213 - log_loss: 0.1900 - val_loss: 0.3073 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 101/400\n",
      "54/54 [==============================] - 24s - loss: 0.1996 - acc: 0.9187 - log_loss: 0.1918 - val_loss: 0.3073 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 102/400\n",
      "53/54 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9256 - log_loss: 0.1802\n",
      "Epoch 00101: reducing learning rate to 9.999999939225292e-10.\n",
      "54/54 [==============================] - 24s - loss: 0.1869 - acc: 0.9259 - log_loss: 0.1790 - val_loss: 0.3073 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 103/400\n",
      "54/54 [==============================] - 29s - loss: 0.1932 - acc: 0.9246 - log_loss: 0.1855 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 104/400\n",
      "54/54 [==============================] - 29s - loss: 0.1970 - acc: 0.9216 - log_loss: 0.1893 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 105/400\n",
      "54/54 [==============================] - 24s - loss: 0.1977 - acc: 0.9225 - log_loss: 0.1900 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 106/400\n",
      "54/54 [==============================] - 30s - loss: 0.1998 - acc: 0.9230 - log_loss: 0.1920 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 107/400\n",
      "54/54 [==============================] - 25s - loss: 0.1903 - acc: 0.9245 - log_loss: 0.1826 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 108/400\n",
      "54/54 [==============================] - 24s - loss: 0.1937 - acc: 0.9277 - log_loss: 0.1860 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 109/400\n",
      "54/54 [==============================] - 24s - loss: 0.1989 - acc: 0.9176 - log_loss: 0.1912 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 110/400\n",
      "54/54 [==============================] - 24s - loss: 0.1964 - acc: 0.9196 - log_loss: 0.1887 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 111/400\n",
      "53/54 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9170 - log_loss: 0.1908\n",
      "Epoch 00110: reducing learning rate to 9.999999717180686e-11.\n",
      "54/54 [==============================] - 30s - loss: 0.1977 - acc: 0.9169 - log_loss: 0.1900 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 112/400\n",
      "54/54 [==============================] - 30s - loss: 0.2051 - acc: 0.9154 - log_loss: 0.1974 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 113/400\n",
      "54/54 [==============================] - 29s - loss: 0.1893 - acc: 0.9198 - log_loss: 0.1816 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 114/400\n",
      "54/54 [==============================] - 29s - loss: 0.1976 - acc: 0.9198 - log_loss: 0.1899 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 115/400\n",
      "54/54 [==============================] - 29s - loss: 0.1939 - acc: 0.9223 - log_loss: 0.1861 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 116/400\n",
      "54/54 [==============================] - 24s - loss: 0.1863 - acc: 0.9256 - log_loss: 0.1786 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 117/400\n",
      "54/54 [==============================] - 24s - loss: 0.2033 - acc: 0.9173 - log_loss: 0.1956 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 118/400\n",
      "53/54 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9206 - log_loss: 0.1886\n",
      "Epoch 00117: reducing learning rate to 9.99999943962493e-12.\n",
      "54/54 [==============================] - 24s - loss: 0.1944 - acc: 0.9210 - log_loss: 0.1867 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 119/400\n",
      "54/54 [==============================] - 30s - loss: 0.1963 - acc: 0.9211 - log_loss: 0.1885 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 120/400\n",
      "54/54 [==============================] - 30s - loss: 0.1921 - acc: 0.9265 - log_loss: 0.1844 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 121/400\n",
      "54/54 [==============================] - 29s - loss: 0.2008 - acc: 0.9151 - log_loss: 0.1931 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 122/400\n",
      "54/54 [==============================] - 29s - loss: 0.1968 - acc: 0.9198 - log_loss: 0.1891 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 123/400\n",
      "54/54 [==============================] - 24s - loss: 0.1984 - acc: 0.9213 - log_loss: 0.1907 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 124/400\n",
      "54/54 [==============================] - 24s - loss: 0.1893 - acc: 0.9224 - log_loss: 0.1816 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 125/400\n",
      "53/54 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9297 - log_loss: 0.1744\n",
      "Epoch 00124: reducing learning rate to 9.999999092680235e-13.\n",
      "54/54 [==============================] - 24s - loss: 0.1851 - acc: 0.9283 - log_loss: 0.1774 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 126/400\n",
      "54/54 [==============================] - 30s - loss: 0.1965 - acc: 0.9244 - log_loss: 0.1887 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 127/400\n",
      "54/54 [==============================] - 30s - loss: 0.1944 - acc: 0.9215 - log_loss: 0.1867 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 128/400\n",
      "54/54 [==============================] - 29s - loss: 0.1923 - acc: 0.9178 - log_loss: 0.1846 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 129/400\n",
      "54/54 [==============================] - 24s - loss: 0.1917 - acc: 0.9170 - log_loss: 0.1839 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 130/400\n",
      "54/54 [==============================] - 24s - loss: 0.1941 - acc: 0.9242 - log_loss: 0.1864 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 131/400\n",
      "54/54 [==============================] - 24s - loss: 0.1949 - acc: 0.9195 - log_loss: 0.1872 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 132/400\n",
      "53/54 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9187 - log_loss: 0.1922\n",
      "Epoch 00131: reducing learning rate to 9.9999988758398e-14.\n",
      "54/54 [==============================] - 24s - loss: 0.1981 - acc: 0.9192 - log_loss: 0.1904 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 133/400\n",
      "54/54 [==============================] - 29s - loss: 0.1945 - acc: 0.9182 - log_loss: 0.1868 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 134/400\n",
      "54/54 [==============================] - 24s - loss: 0.2008 - acc: 0.9142 - log_loss: 0.1931 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 135/400\n",
      "54/54 [==============================] - 24s - loss: 0.1916 - acc: 0.9276 - log_loss: 0.1839 - val_loss: 0.3072 - val_acc: 0.8942 - val_log_loss: 0.2995\n",
      "Epoch 136/400\n",
      "27/54 [==============>...............] - ETA: 11s - loss: 0.1934 - acc: 0.9216 - log_loss: 0.1857"
     ]
    }
   ],
   "source": [
    "epochs = 400\n",
    "batch_size = 64\n",
    "Xtrain_cv, Xtrain_val, Ytrain_cv, Ytrain_val, Xangle_cv, Xangle_val = train_test_split(Xtrain, Ytrain, Xangle, shuffle=True, test_size=0.3, random_state=SEED)\n",
    "y_test_pred_log = 0\n",
    " \n",
    "Xtrain_input = [Xtrain, Xangle]\n",
    "Xval_input = [Xtrain_val, Xangle_val]\n",
    "Xtest_input = [Xtest, Xangle_test]\n",
    "\n",
    "model_file = 'vgg16_%s.hdf5' % 1\n",
    "\n",
    "model = getModel()\n",
    "steps = np.ceil(len(Xtrain_cv) / batch_size) * 3\n",
    "model.fit_generator(gen_flow_for_two_inputs(Xtrain_cv, Xangle_cv, Ytrain_cv), \n",
    "                    steps_per_epoch=steps, epochs=epochs, verbose=1, shuffle=True, \n",
    "                    callbacks=get_callbacks(model_file), validation_data=(Xval_input, Ytrain_val))\n",
    "\n",
    "model.load_weights(filepath = model_file)    \n",
    "\n",
    "train_score = model.evaluate(Xtrain_input, Ytrain, verbose=1)\n",
    "val_score = model.evaluate(Xval_input, Ytrain_val, verbose=1)\n",
    "print('Train score:', train_score[0], ' ;Train accuracy:', train_score[1])\n",
    "print('Valid score:', val_score[0], ' ;Valid accuracy:', val_score[1])\n",
    "y_test_pred_log += model.predict(Xtest_input, verbose=1).reshape(Xtest.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': y_test_pred_log})\n",
    "print(submission.count(), Xtest.shape[0])\n",
    "\n",
    "submission.to_csv('submission-vgg16-%s.csv' % baseModelName, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
