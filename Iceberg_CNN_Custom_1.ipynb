{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\kaggle\\iceberg\n"
     ]
    }
   ],
   "source": [
    "%cd E:\\kaggle\\iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "SEED = 1234\n",
    "np.random.seed(SEED) \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Concatenate, Input\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import uniform_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_imgs(df):\n",
    "    imgs = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        #make 75x75 image\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 + band_2 # plus since log(x*y) = log(x) + log(y)\n",
    "        \n",
    "        # Rescale\n",
    "        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
    "        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "\n",
    "        imgs.append(np.dstack((a, b, c)))\n",
    "\n",
    "    return np.array(imgs)\n",
    "\n",
    "def get_more_images(imgs):\n",
    "    more_images = []\n",
    "    vert_flip_imgs = []\n",
    "    hori_flip_imgs = []\n",
    "    vh_flip_imgs = []\n",
    "      \n",
    "    for i in range(0,imgs.shape[0]):\n",
    "        vert_flip_imgs.append(cv2.flip(imgs[i], 1))\n",
    "        hori_flip_imgs.append(cv2.flip(imgs[i], 0))\n",
    "        vh_flip_imgs.append(cv2.flip(imgs[i], -1))\n",
    "      \n",
    "    v = np.array(vert_flip_imgs)\n",
    "    h = np.array(hori_flip_imgs)\n",
    "    vh = np.array(vh_flip_imgs)\n",
    "       \n",
    "    more_images = np.concatenate((imgs,v,h, vh))\n",
    "    \n",
    "    return more_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(band_1, band_2, is_iceberg, angle = None):\n",
    "    if angle is None:\n",
    "        title_str = 'Iceberg' if is_iceberg == 1 else 'Ship'\n",
    "    else:\n",
    "        title_str = 'Iceberg-' + str(angle) if is_iceberg == 1 else 'Ship-' + str(angle)\n",
    "    fig = plt.figure(0, figsize=(10,10))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    ax.set_title(title_str + ' - Band 1')\n",
    "    ax.imshow(band_1,cmap='jet')\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    ax.set_title(title_str + ' - Band 2')\n",
    "    ax.imshow(band_2,cmap='jet')\n",
    "    plt.show()\n",
    "\n",
    "# implement functions to convert SAR data from decibel units to linear units and back again\n",
    "def decibel_to_linear(band):\n",
    "     # convert to linear units\n",
    "    return np.power(10,np.array(band)/10)\n",
    "\n",
    "def linear_to_decibel(band):\n",
    "    return 10*np.log10(band)\n",
    "\n",
    "# implement the Lee Filter for a band in an image already reshaped into the proper dimensions\n",
    "def lee_filter(band, window, var_noise = 0.25):\n",
    "    # band: SAR data to be despeckled (already reshaped into image dimensions)\n",
    "    # window: descpeckling filter window (tuple)\n",
    "    # default noise variance = 0.25\n",
    "    # assumes noise mean = 0\n",
    "    \n",
    "    mean_window = uniform_filter(band, window)\n",
    "    mean_sqr_window = uniform_filter(band**2, window)\n",
    "    var_window = mean_sqr_window - mean_window**2\n",
    "\n",
    "    weights = var_window / (var_window + var_noise)\n",
    "    band_filtered = mean_window + weights*(band - mean_window)\n",
    "    return band_filtered\n",
    "\n",
    "def apply_lee_filter(band_1_linear, band_2_linear, window_var_index = 0, noise_var_index = 0):\n",
    "    windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "    noise_var = np.array([1, 2, 4])\n",
    "    noise_var_1 = np.round(np.var(band_1_linear) * noise_var, 10)\n",
    "    noise_var_2 = np.round(np.var(band_2_linear) * noise_var, 10)\n",
    "    band_1_linear_filtered = lee_filter(band_1_linear, windows[window_var_index], noise_var_1[noise_var_index])\n",
    "    band_2_linear_filtered = lee_filter(band_2_linear, windows[window_var_index], noise_var_2[noise_var_index])\n",
    "    return band_1_linear_filtered, band_2_linear_filtered\n",
    "\n",
    "def apply_lee_filter_single(band_linear, window_var_index = 0, noise_var_index = 0):\n",
    "    windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "    noise_var = np.array([1, 2, 4])\n",
    "    noise_var = np.round(np.var(band_linear) * noise_var, 10)\n",
    "    band_linear_filtered = lee_filter(band_linear, windows[window_var_index], noise_var[noise_var_index])\n",
    "    return band_linear_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use_custom_augmentation = True\n",
    "# if use_custom_augmentation:\n",
    "#     df_train = pd.read_json('E:/kaggle/iceberg/train.json/data/processed/train.json')\n",
    "#     df_test = pd.read_json('E:/kaggle/iceberg/test.json/data/processed/test.json')\n",
    "#     Xtrain = get_scaled_imgs(df_train)\n",
    "#     Xtest = get_scaled_imgs(df_test)\n",
    "#     Ytrain = np.array(df_train['is_iceberg'])\n",
    "    \n",
    "#     df_train[\"inc_angle\"] = df_train[\"inc_angle\"].replace('na',0)\n",
    "#     df_test[\"inc_angle\"] = df_test[\"inc_angle\"].replace('na',0)\n",
    "#     idx_tr = np.where(df_train[\"inc_angle\"]>0)\n",
    "#     Xtrain = Xtrain[idx_tr[0]]\n",
    "#     Ytrain = Ytrain[idx_tr[0]]\n",
    "    \n",
    "#     Xtrain = get_more_images(Xtrain) \n",
    "#     Ytrain = np.concatenate((Ytrain,Ytrain,Ytrain, Ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if not use_custom_augmentation:\n",
    "#     df_train = pd.read_json('E:/kaggle/iceberg/train.json/data/processed/train.json')\n",
    "#     Xtrain_band_1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df_train[\"band_1\"]])\n",
    "#     Xtrain_band_2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df_train[\"band_2\"]])\n",
    "#     Xtrain_band_3 = Xtrain_band_1 + Xtrain_band_2\n",
    "\n",
    "#     Xtrain = np.concatenate([Xtrain_band_1[:, :, :, np.newaxis], Xtrain_band_2[:, :, :, np.newaxis], Xtrain_band_3[:, :, :, np.newaxis]], axis=-1)\n",
    "#     Ytrain = df_train[\"is_iceberg\"].values\n",
    "\n",
    "#     df_train[\"inc_angle\"] = df_train[\"inc_angle\"].replace('na',0)\n",
    "#     idx_tr = np.where(df_train[\"inc_angle\"]>0)\n",
    "\n",
    "#     Xtrain = Xtrain[idx_tr[0]]\n",
    "#     Ytrain = Ytrain[idx_tr[0]]\n",
    "\n",
    "#     df_test = pd.read_json('E:/kaggle/iceberg/test.json/data/processed/test.json')\n",
    "#     Xtest_band_1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df_test[\"band_1\"]])\n",
    "#     Xtest_band_2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df_test[\"band_2\"]])\n",
    "#     Xtest_band_3 = Xtest_band_1 + Xtest_band_2\n",
    "\n",
    "#     Xtest = np.concatenate([Xtest_band_1[:, :, :, np.newaxis], Xtest_band_2[:, :, :, np.newaxis], Xtest_band_3[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "#     df_test[\"inc_angle\"] = df_test[\"inc_angle\"].replace('na',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_custom_augmentation = False\n",
    "if use_custom_augmentation:\n",
    "    df_train = pd.read_json('E:/kaggle/iceberg/train.json/data/processed/train.json')\n",
    "    df_test = pd.read_json('E:/kaggle/iceberg/test.json/data/processed/test.json')\n",
    "    Xtrain = get_scaled_imgs(df_train)\n",
    "    Xtest = get_scaled_imgs(df_test)\n",
    "    Ytrain = np.array(df_train['is_iceberg'])\n",
    "    \n",
    "    df_train[\"inc_angle\"] = df_train[\"inc_angle\"].replace('na',0)\n",
    "    df_test[\"inc_angle\"] = df_test[\"inc_angle\"].replace('na',0)\n",
    "    idx_tr = np.where(df_train[\"inc_angle\"]>0)\n",
    "    Xtrain = Xtrain[idx_tr[0]]\n",
    "    Ytrain = Ytrain[idx_tr[0]]\n",
    "    \n",
    "    Xtrain = get_more_images(Xtrain) \n",
    "    Ytrain = np.concatenate((Ytrain,Ytrain,Ytrain, Ytrain))\n",
    "else:\n",
    "    train = pd.read_json(\"E:/kaggle/iceberg/train.json/data/processed/train.json\")\n",
    "    target_train=train['is_iceberg']\n",
    "    test = pd.read_json(\"E:/kaggle/iceberg/test.json/data/processed/test.json\")\n",
    "#     train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\n",
    "#     train['inc_angle']=train['inc_angle'].fillna(method='pad')\n",
    "    train[\"inc_angle\"] = train[\"inc_angle\"].replace('na',0)\n",
    "    idx_tr = np.where(train[\"inc_angle\"]>0)\n",
    "    train = train.iloc[idx_tr[0]]\n",
    "    target_train = target_train.iloc[idx_tr[0]]\n",
    "    X_angle=train['inc_angle']\n",
    "#     test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "    test['inc_angle']=test['inc_angle'].fillna(method='pad')\n",
    "    X_test_angle=test['inc_angle']\n",
    "    \n",
    "    #Generate the training data\n",
    "    X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "    X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "    #apply filter\n",
    "    X_band_1_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_1])\n",
    "    X_band_2_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_2])\n",
    "    X_band_1_filtered = linear_to_decibel(X_band_1_filtered)\n",
    "    X_band_2_filtered = linear_to_decibel(X_band_2_filtered)\n",
    "    X_band_1 = X_band_1_filtered\n",
    "    X_band_2 = X_band_2_filtered\n",
    "\n",
    "    X_band_3=np.fabs(np.subtract(X_band_1,X_band_2))\n",
    "    X_band_4=np.maximum(X_band_1,X_band_2)\n",
    "    X_band_5=np.minimum(X_band_1,X_band_2)\n",
    "    X_train = np.concatenate([X_band_3[:, :, :, np.newaxis],X_band_4[:, :, :, np.newaxis],X_band_5[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "    X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "    X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "    #apply filter\n",
    "    X_band_test_1_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_test_1])\n",
    "    X_band_test_2_filtered = np.array([apply_lee_filter_single(decibel_to_linear(band)) for band in X_band_test_2])\n",
    "    X_band_test_1_filtered = linear_to_decibel(X_band_test_1_filtered)\n",
    "    X_band_test_2_filtered = linear_to_decibel(X_band_test_2_filtered)\n",
    "    X_band_test_1 = X_band_test_1_filtered\n",
    "    X_band_test_2 = X_band_test_2_filtered\n",
    "\n",
    "    X_band_test_3=np.fabs(np.subtract(X_band_test_1,X_band_test_2))\n",
    "    X_band_test_4=np.maximum(X_band_test_1,X_band_test_2)\n",
    "    X_band_test_5=np.minimum(X_band_test_1,X_band_test_2)\n",
    "    X_test = np.concatenate([X_band_test_3[:, :, :, np.newaxis], X_band_test_4[:, :, :, np.newaxis],X_band_test_5[:, :, :, np.newaxis]],axis=-1)\n",
    "    \n",
    "    X_train = get_more_images(X_train)\n",
    "    target_train = np.concatenate((target_train, target_train, target_train, target_train))\n",
    "    X_angle = np.concatenate((X_angle, X_angle, X_angle, X_angle))\n",
    "    \n",
    "    Xtrain = X_train\n",
    "    Ytrain = target_train\n",
    "    Xtest = X_test\n",
    "    Xangle = X_angle\n",
    "    Xangle_test = X_test_angle\n",
    "    df_train = train\n",
    "    df_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5884, 75, 75, 3) (5884,) (5884,) (8424, 75, 75, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEtCAYAAAAsgeXEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMUPglyljjDHGmCHwy5QxxhhjzBD8PzTZHA1Hg9jYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21bd3d6fa58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Xtrain.shape, Ytrain.shape, Xangle.shape, Xtest.shape)\n",
    "display_img(X_band_1[0], X_band_2[0], target_train[0], X_angle[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getModel():\n",
    "#     #Build keras model\n",
    "    \n",
    "#     model=Sequential()\n",
    "    \n",
    "#     # CNN 1\n",
    "#     model.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 3)))\n",
    "#     model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "#     # CNN 2\n",
    "#     model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "#     # CNN 3\n",
    "#     model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "#     model.add(Dropout(0.3))\n",
    "\n",
    "#     #CNN 4\n",
    "#     model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "#     model.add(Dropout(0.3))\n",
    "\n",
    "#     # You must flatten the data for the dense layers\n",
    "#     model.add(Flatten())\n",
    "\n",
    "#     #Dense 1\n",
    "#     model.add(Dense(512, activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "#     #Dense 2\n",
    "#     model.add(Dense(256, activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "#     # Output \n",
    "#     model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "#     optimizer = Adam(lr=0.001, decay=0.0)\n",
    "#     model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    #Build keras model\n",
    "    angle_input = Input(shape=[1], name=\"angle\")\n",
    "    angle_layer = Dense(1)(angle_input)\n",
    "    img_input = Input(shape=Xtrain.shape[1:], name=\"img\")\n",
    "    \n",
    "    # CNN 1\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu')(img_input)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # CNN 2\n",
    "    x = Conv2D(128, kernel_size=(3, 3), activation='relu' )(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # CNN 3\n",
    "    x = Conv2D(128, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    #CNN 4\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # You must flatten the data for the dense layers\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    #Dense 1\n",
    "    fc = Dense(512, activation='relu')(x)\n",
    "    fc = Dropout(0.2)(fc)\n",
    "\n",
    "    #Dense 2\n",
    "    fc = Dense(256, activation='relu')(fc)\n",
    "    fc = Dropout(0.2)(fc)\n",
    "    \n",
    "    #angle fuse\n",
    "    fc = Concatenate()([fc, angle_layer])\n",
    "                     \n",
    "    # Output \n",
    "    predictions = Dense(1, activation=\"sigmoid\")(fc)\n",
    "    \n",
    "    model = Model(inputs=[img_input, angle_input], outputs=predictions)\n",
    "                     \n",
    "    optimizer = Adam(lr=1e-3, decay=0.0)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "img (InputLayer)                 (None, 75, 75, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)               (None, 73, 73, 64)    1792        img[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling2D)  (None, 36, 36, 64)    0           conv2d_25[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)             (None, 36, 36, 64)    0           max_pooling2d_25[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)               (None, 34, 34, 128)   73856       dropout_37[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling2D)  (None, 17, 17, 128)   0           conv2d_26[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)             (None, 17, 17, 128)   0           max_pooling2d_26[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)               (None, 15, 15, 128)   147584      dropout_38[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling2D)  (None, 7, 7, 128)     0           conv2d_27[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)             (None, 7, 7, 128)     0           max_pooling2d_27[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)               (None, 5, 5, 64)      73792       dropout_39[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling2D)  (None, 2, 2, 64)      0           conv2d_28[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)             (None, 2, 2, 64)      0           max_pooling2d_28[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)              (None, 256)           0           dropout_40[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (None, 512)           131584      flatten_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)             (None, 512)           0           dense_22[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_23 (Dense)                 (None, 256)           131328      dropout_41[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "angle (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)             (None, 256)           0           dense_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 1)             2           angle[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 257)           0           dropout_42[0][0]                 \n",
      "                                                                   dense_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 1)             258         concatenate_3[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 560,196\n",
      "Trainable params: 560,196\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = getModel()\n",
    "model.summary()\n",
    "plot_model(model, \"basic_cnn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=20, mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "# Define the image transformations here\n",
    "use_zca_whitening = True\n",
    "gen = ImageDataGenerator(horizontal_flip = False,\n",
    "                         vertical_flip = False,\n",
    "                         samplewise_center = True,# check\n",
    "                         zca_whitening=use_zca_whitening,# check\n",
    "                         width_shift_range = 0.,\n",
    "                         height_shift_range = 0.,\n",
    "                         channel_shift_range=0,\n",
    "                         zoom_range = 0.5,\n",
    "                         rotation_range = 10)\n",
    "\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_two_inputs(X1, X2, y):\n",
    "    if use_zca_whitening:\n",
    "        gen.fit(X1)\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=55)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size,seed=55)\n",
    "    while True:\n",
    "        X1i = genX1.next()\n",
    "        X2i = genX2.next()\n",
    "        #Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "        #np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "        yield [X1i[0], X2i[1]], X1i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================FOLD= 0\n",
      "5884/5884 [==============================] - 16s    \n",
      "Train score: 0.149810824956\n",
      "Train accuracy: 0.945445275404\n",
      "\n",
      "===================FOLD= 1\n",
      "Epoch 1/100\n",
      "61/61 [==============================] - 42s - loss: 0.6549 - acc: 0.6537 - val_loss: 0.5295 - val_acc: 0.7226\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 40s - loss: 0.4777 - acc: 0.7724 - val_loss: 0.4673 - val_acc: 0.7680\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 39s - loss: 0.4394 - acc: 0.8045 - val_loss: 0.4182 - val_acc: 0.8358\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 38s - loss: 0.4322 - acc: 0.8094 - val_loss: 0.4390 - val_acc: 0.7904\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 40s - loss: 0.4179 - acc: 0.8087 - val_loss: 0.4148 - val_acc: 0.8021\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 39s - loss: 0.3980 - acc: 0.8190 - val_loss: 0.3653 - val_acc: 0.8368\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 39s - loss: 0.3924 - acc: 0.8317 - val_loss: 0.3487 - val_acc: 0.8526\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 39s - loss: 0.4169 - acc: 0.8094 - val_loss: 0.3371 - val_acc: 0.8531\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 39s - loss: 0.3746 - acc: 0.8325 - val_loss: 0.3271 - val_acc: 0.8542\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 39s - loss: 0.3621 - acc: 0.8384 - val_loss: 0.3479 - val_acc: 0.8338\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 40s - loss: 0.3650 - acc: 0.8320 - val_loss: 0.3223 - val_acc: 0.8567\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 46s - loss: 0.3677 - acc: 0.8367 - val_loss: 0.3151 - val_acc: 0.8542\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 44s - loss: 0.3504 - acc: 0.8468 - val_loss: 0.2951 - val_acc: 0.8786\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 43s - loss: 0.3444 - acc: 0.8497 - val_loss: 0.3053 - val_acc: 0.8705\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 43s - loss: 0.3500 - acc: 0.8470 - val_loss: 0.3287 - val_acc: 0.8399\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 43s - loss: 0.3516 - acc: 0.8512 - val_loss: 0.3183 - val_acc: 0.8761\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 44s - loss: 0.3441 - acc: 0.8437 - val_loss: 0.2856 - val_acc: 0.8751\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 43s - loss: 0.3316 - acc: 0.8584 - val_loss: 0.2961 - val_acc: 0.8705\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 44s - loss: 0.3318 - acc: 0.8601 - val_loss: 0.2808 - val_acc: 0.8934\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 45s - loss: 0.3154 - acc: 0.8601 - val_loss: 0.2701 - val_acc: 0.8863\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 43s - loss: 0.3269 - acc: 0.8576 - val_loss: 0.2692 - val_acc: 0.8853\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 43s - loss: 0.3379 - acc: 0.8529 - val_loss: 0.2987 - val_acc: 0.8771\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 44s - loss: 0.3170 - acc: 0.8698 - val_loss: 0.3160 - val_acc: 0.8450\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 44s - loss: 0.3067 - acc: 0.8648 - val_loss: 0.2675 - val_acc: 0.9026\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 42s - loss: 0.3087 - acc: 0.8672 - val_loss: 0.2690 - val_acc: 0.8868\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 49s - loss: 0.3060 - acc: 0.8607 - val_loss: 0.3253 - val_acc: 0.8383\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 44s - loss: 0.3148 - acc: 0.8617 - val_loss: 0.2561 - val_acc: 0.8934\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 44s - loss: 0.3145 - acc: 0.8640 - val_loss: 0.2710 - val_acc: 0.8756\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 45s - loss: 0.3052 - acc: 0.8640 - val_loss: 0.2856 - val_acc: 0.8628\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 47s - loss: 0.3172 - acc: 0.8592 - val_loss: 0.2571 - val_acc: 0.8848\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 52s - loss: 0.3036 - acc: 0.8715 - val_loss: 0.2606 - val_acc: 0.9062\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 45s - loss: 0.2830 - acc: 0.8785 - val_loss: 0.2565 - val_acc: 0.8827\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 45s - loss: 0.2940 - acc: 0.8720 - val_loss: 0.2638 - val_acc: 0.8771\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 44s - loss: 0.2876 - acc: 0.8769 - val_loss: 0.2539 - val_acc: 0.8950\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 44s - loss: 0.3012 - acc: 0.8758 - val_loss: 0.2475 - val_acc: 0.9046\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 46s - loss: 0.3029 - acc: 0.8699 - val_loss: 0.3036 - val_acc: 0.8531\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 44s - loss: 0.2736 - acc: 0.8832 - val_loss: 0.2331 - val_acc: 0.8980\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 42s - loss: 0.2884 - acc: 0.8708 - val_loss: 0.2251 - val_acc: 0.9036\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 41s - loss: 0.2916 - acc: 0.8801 - val_loss: 0.2419 - val_acc: 0.8970\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 43s - loss: 0.2731 - acc: 0.8819 - val_loss: 0.2251 - val_acc: 0.9097\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 42s - loss: 0.2871 - acc: 0.8827 - val_loss: 0.2387 - val_acc: 0.9006\n",
      "Epoch 42/100\n",
      "61/61 [==============================] - 41s - loss: 0.2764 - acc: 0.8867 - val_loss: 0.2271 - val_acc: 0.9046\n",
      "Epoch 43/100\n",
      "61/61 [==============================] - 43s - loss: 0.2895 - acc: 0.8777 - val_loss: 0.2381 - val_acc: 0.9072\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 41s - loss: 0.2769 - acc: 0.8857 - val_loss: 0.2786 - val_acc: 0.8674\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 42s - loss: 0.2786 - acc: 0.8850 - val_loss: 0.2614 - val_acc: 0.8858\n",
      "Epoch 46/100\n",
      "61/61 [==============================] - 43s - loss: 0.2652 - acc: 0.8846 - val_loss: 0.2589 - val_acc: 0.8873\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 42s - loss: 0.2915 - acc: 0.8784 - val_loss: 0.2717 - val_acc: 0.8684\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 42s - loss: 0.2983 - acc: 0.8706 - val_loss: 0.2373 - val_acc: 0.9036\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 43s - loss: 0.2676 - acc: 0.8891 - val_loss: 0.2193 - val_acc: 0.9128\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 41s - loss: 0.2713 - acc: 0.8837 - val_loss: 0.2406 - val_acc: 0.9031\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 42s - loss: 0.2704 - acc: 0.8894 - val_loss: 0.2781 - val_acc: 0.8781\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 43s - loss: 0.2520 - acc: 0.8978 - val_loss: 0.2137 - val_acc: 0.9169\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 41s - loss: 0.2645 - acc: 0.8871 - val_loss: 0.2404 - val_acc: 0.8939\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 43s - loss: 0.2737 - acc: 0.8822 - val_loss: 0.2819 - val_acc: 0.8771\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 42s - loss: 0.2767 - acc: 0.8874 - val_loss: 0.2642 - val_acc: 0.8776\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 41s - loss: 0.2613 - acc: 0.8896 - val_loss: 0.2206 - val_acc: 0.9077\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 44s - loss: 0.2464 - acc: 0.8988 - val_loss: 0.2300 - val_acc: 0.9082\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 44s - loss: 0.2570 - acc: 0.8883 - val_loss: 0.2330 - val_acc: 0.8980\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 43s - loss: 0.2569 - acc: 0.8896 - val_loss: 0.2478 - val_acc: 0.8868\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 44s - loss: 0.2562 - acc: 0.8888 - val_loss: 0.2225 - val_acc: 0.9057\n",
      "Epoch 61/100\n",
      "61/61 [==============================] - 44s - loss: 0.2706 - acc: 0.8882 - val_loss: 0.2301 - val_acc: 0.9021\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 44s - loss: 0.2587 - acc: 0.8898 - val_loss: 0.2300 - val_acc: 0.9097\n",
      "Epoch 63/100\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.8872\n",
      "Epoch 00062: reducing learning rate to 0.00010000000474974513.\n",
      "61/61 [==============================] - 43s - loss: 0.2600 - acc: 0.8873 - val_loss: 0.2411 - val_acc: 0.8985\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 42s - loss: 0.2299 - acc: 0.9049 - val_loss: 0.2210 - val_acc: 0.9041\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 42s - loss: 0.2271 - acc: 0.9075 - val_loss: 0.2146 - val_acc: 0.9123\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 43s - loss: 0.2233 - acc: 0.9108 - val_loss: 0.2170 - val_acc: 0.9118\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 44s - loss: 0.2223 - acc: 0.9114 - val_loss: 0.2082 - val_acc: 0.9159\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 42s - loss: 0.2227 - acc: 0.9085 - val_loss: 0.2091 - val_acc: 0.9153\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 43s - loss: 0.2096 - acc: 0.9113 - val_loss: 0.2032 - val_acc: 0.9148\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 42s - loss: 0.2259 - acc: 0.9038 - val_loss: 0.1990 - val_acc: 0.9225\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 44s - loss: 0.2212 - acc: 0.9093 - val_loss: 0.1980 - val_acc: 0.9215\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 46s - loss: 0.2018 - acc: 0.9213 - val_loss: 0.1974 - val_acc: 0.9230\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 45s - loss: 0.2047 - acc: 0.9178 - val_loss: 0.1936 - val_acc: 0.9245\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 45s - loss: 0.2108 - acc: 0.9171 - val_loss: 0.1970 - val_acc: 0.9225\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 45s - loss: 0.2284 - acc: 0.9059 - val_loss: 0.2065 - val_acc: 0.9159\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 48s - loss: 0.2067 - acc: 0.9136 - val_loss: 0.1992 - val_acc: 0.9194\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 43s - loss: 0.2084 - acc: 0.9160 - val_loss: 0.1985 - val_acc: 0.9199\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 45s - loss: 0.2221 - acc: 0.9055 - val_loss: 0.1972 - val_acc: 0.9230\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 45s - loss: 0.2113 - acc: 0.9201 - val_loss: 0.2047 - val_acc: 0.9179\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 48s - loss: 0.2150 - acc: 0.9142 - val_loss: 0.1973 - val_acc: 0.9215\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 47s - loss: 0.1984 - acc: 0.9215 - val_loss: 0.2004 - val_acc: 0.9179\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 46s - loss: 0.2036 - acc: 0.9180 - val_loss: 0.1941 - val_acc: 0.9235\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 48s - loss: 0.2001 - acc: 0.9137 - val_loss: 0.1950 - val_acc: 0.9220\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 48s - loss: 0.2101 - acc: 0.9149 - val_loss: 0.1895 - val_acc: 0.9250\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 45s - loss: 0.1974 - acc: 0.9180 - val_loss: 0.2005 - val_acc: 0.9194\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 47s - loss: 0.2023 - acc: 0.9127 - val_loss: 0.1910 - val_acc: 0.9271\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 46s - loss: 0.2079 - acc: 0.9195 - val_loss: 0.1934 - val_acc: 0.9230\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 49s - loss: 0.1986 - acc: 0.9265 - val_loss: 0.1959 - val_acc: 0.9204\n",
      "Epoch 89/100\n",
      "61/61 [==============================] - 46s - loss: 0.2022 - acc: 0.9208 - val_loss: 0.1986 - val_acc: 0.9220\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 46s - loss: 0.2124 - acc: 0.9135 - val_loss: 0.1981 - val_acc: 0.9199\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 47s - loss: 0.1995 - acc: 0.9178 - val_loss: 0.1986 - val_acc: 0.9189\n",
      "Epoch 92/100\n",
      "61/61 [==============================] - 47s - loss: 0.2058 - acc: 0.9129 - val_loss: 0.1937 - val_acc: 0.9235\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 47s - loss: 0.1944 - acc: 0.9188 - val_loss: 0.1873 - val_acc: 0.9291\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 48s - loss: 0.2046 - acc: 0.9156 - val_loss: 0.1879 - val_acc: 0.9271\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 47s - loss: 0.1929 - acc: 0.9210 - val_loss: 0.1901 - val_acc: 0.9266\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 47s - loss: 0.2106 - acc: 0.9190 - val_loss: 0.1890 - val_acc: 0.9266\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 46s - loss: 0.1914 - acc: 0.9233 - val_loss: 0.1921 - val_acc: 0.9204\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 45s - loss: 0.2095 - acc: 0.9149 - val_loss: 0.1908 - val_acc: 0.9245\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 43s - loss: 0.1907 - acc: 0.9235 - val_loss: 0.1905 - val_acc: 0.9225\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 44s - loss: 0.2026 - acc: 0.9196 - val_loss: 0.1914 - val_acc: 0.9240\n",
      "5884/5884 [==============================] - 17s    \n",
      "Train score: 0.153015656071\n",
      "Train accuracy: 0.944085656097\n",
      "\n",
      "===================FOLD= 2\n",
      "Epoch 1/100\n",
      "61/61 [==============================] - 44s - loss: 0.7496 - acc: 0.5743 - val_loss: 0.7365 - val_acc: 0.4880\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 42s - loss: 0.5179 - acc: 0.7282 - val_loss: 0.4655 - val_acc: 0.7945\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 44s - loss: 0.4748 - acc: 0.7719 - val_loss: 0.5104 - val_acc: 0.6940\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 44s - loss: 0.4276 - acc: 0.8013 - val_loss: 0.4232 - val_acc: 0.8072\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 45s - loss: 0.4119 - acc: 0.8026 - val_loss: 0.4069 - val_acc: 0.8261\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 40s - loss: 0.3997 - acc: 0.8248 - val_loss: 0.4115 - val_acc: 0.8225\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 45s - loss: 0.3866 - acc: 0.8155 - val_loss: 0.3562 - val_acc: 0.8429\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 42s - loss: 0.3885 - acc: 0.8194 - val_loss: 0.3887 - val_acc: 0.8434\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 44s - loss: 0.3807 - acc: 0.8256 - val_loss: 0.3662 - val_acc: 0.8633\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 46s - loss: 0.3624 - acc: 0.8380 - val_loss: 0.3486 - val_acc: 0.8710\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 43s - loss: 0.3579 - acc: 0.8389 - val_loss: 0.3032 - val_acc: 0.8710\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 43s - loss: 0.3442 - acc: 0.8466 - val_loss: 0.2964 - val_acc: 0.8776\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 44s - loss: 0.3456 - acc: 0.8471 - val_loss: 0.2964 - val_acc: 0.8740\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 47s - loss: 0.3439 - acc: 0.8459 - val_loss: 0.2964 - val_acc: 0.8725\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 48s - loss: 0.3482 - acc: 0.8419 - val_loss: 0.3426 - val_acc: 0.8542\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 47s - loss: 0.3458 - acc: 0.8402 - val_loss: 0.3448 - val_acc: 0.8399\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 42s - loss: 0.3169 - acc: 0.8617 - val_loss: 0.2784 - val_acc: 0.8797\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 44s - loss: 0.3345 - acc: 0.8599 - val_loss: 0.3209 - val_acc: 0.8603\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 47s - loss: 0.3262 - acc: 0.8537 - val_loss: 0.3429 - val_acc: 0.8689\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 46s - loss: 0.3140 - acc: 0.8589 - val_loss: 0.3115 - val_acc: 0.8547\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 42s - loss: 0.3153 - acc: 0.8645 - val_loss: 0.2932 - val_acc: 0.8771\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 48s - loss: 0.3081 - acc: 0.8668 - val_loss: 0.2679 - val_acc: 0.8802\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 45s - loss: 0.2890 - acc: 0.8705 - val_loss: 0.2704 - val_acc: 0.8944\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 45s - loss: 0.3219 - acc: 0.8571 - val_loss: 0.2921 - val_acc: 0.8735\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 51s - loss: 0.2986 - acc: 0.8611 - val_loss: 0.2500 - val_acc: 0.8995\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 48s - loss: 0.2964 - acc: 0.8681 - val_loss: 0.2568 - val_acc: 0.9026\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 45s - loss: 0.2947 - acc: 0.8705 - val_loss: 0.2734 - val_acc: 0.8848\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 45s - loss: 0.2919 - acc: 0.8696 - val_loss: 0.3153 - val_acc: 0.8644\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 45s - loss: 0.2795 - acc: 0.8834 - val_loss: 0.2671 - val_acc: 0.8842\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 46s - loss: 0.3017 - acc: 0.8677 - val_loss: 0.2397 - val_acc: 0.9077\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 46s - loss: 0.2837 - acc: 0.8715 - val_loss: 0.2246 - val_acc: 0.9087\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 45s - loss: 0.2852 - acc: 0.8749 - val_loss: 0.2484 - val_acc: 0.8929\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 46s - loss: 0.2849 - acc: 0.8798 - val_loss: 0.2601 - val_acc: 0.8914\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 47s - loss: 0.2845 - acc: 0.8797 - val_loss: 0.2447 - val_acc: 0.9006\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 46s - loss: 0.2760 - acc: 0.8781 - val_loss: 0.3549 - val_acc: 0.8389\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 45s - loss: 0.3114 - acc: 0.8698 - val_loss: 0.3416 - val_acc: 0.8491\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 46s - loss: 0.2708 - acc: 0.8803 - val_loss: 0.2552 - val_acc: 0.8924\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 45s - loss: 0.2856 - acc: 0.8802 - val_loss: 0.2594 - val_acc: 0.9052\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 46s - loss: 0.2710 - acc: 0.8837 - val_loss: 0.2489 - val_acc: 0.8995\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 42s - loss: 0.2758 - acc: 0.8854 - val_loss: 0.2619 - val_acc: 0.8863\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 43s - loss: 0.2731 - acc: 0.8898 - val_loss: 0.2422 - val_acc: 0.9026\n",
      "Epoch 42/100\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.8845\n",
      "Epoch 00041: reducing learning rate to 0.00010000000474974513.\n",
      "61/61 [==============================] - 44s - loss: 0.2641 - acc: 0.8851 - val_loss: 0.2253 - val_acc: 0.9113\n",
      "Epoch 43/100\n",
      "61/61 [==============================] - 45s - loss: 0.2387 - acc: 0.9046 - val_loss: 0.2257 - val_acc: 0.9108\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 45s - loss: 0.2661 - acc: 0.8877 - val_loss: 0.2207 - val_acc: 0.9097\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 44s - loss: 0.2202 - acc: 0.9050 - val_loss: 0.2202 - val_acc: 0.9123\n",
      "Epoch 46/100\n",
      "61/61 [==============================] - 43s - loss: 0.2332 - acc: 0.9050 - val_loss: 0.2185 - val_acc: 0.9082\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 42s - loss: 0.2289 - acc: 0.9067 - val_loss: 0.2175 - val_acc: 0.9128\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 40s - loss: 0.2335 - acc: 0.9056 - val_loss: 0.2206 - val_acc: 0.9097\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 44s - loss: 0.2299 - acc: 0.9028 - val_loss: 0.2171 - val_acc: 0.9133\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 45s - loss: 0.2166 - acc: 0.9150 - val_loss: 0.2144 - val_acc: 0.9138\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 44s - loss: 0.2133 - acc: 0.9155 - val_loss: 0.2140 - val_acc: 0.9148\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 46s - loss: 0.2247 - acc: 0.9071 - val_loss: 0.2156 - val_acc: 0.9148\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 48s - loss: 0.2149 - acc: 0.9166 - val_loss: 0.2142 - val_acc: 0.9138\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 47s - loss: 0.2143 - acc: 0.9115 - val_loss: 0.2199 - val_acc: 0.9118\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 45s - loss: 0.2198 - acc: 0.9137 - val_loss: 0.2146 - val_acc: 0.9128\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 47s - loss: 0.2192 - acc: 0.9075 - val_loss: 0.2120 - val_acc: 0.9143\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 45s - loss: 0.2071 - acc: 0.9102 - val_loss: 0.2117 - val_acc: 0.9128\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 45s - loss: 0.2171 - acc: 0.9138 - val_loss: 0.2119 - val_acc: 0.9123\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 42s - loss: 0.2059 - acc: 0.9174 - val_loss: 0.2106 - val_acc: 0.9118\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 42s - loss: 0.2093 - acc: 0.9154 - val_loss: 0.2112 - val_acc: 0.9148\n",
      "Epoch 61/100\n",
      "61/61 [==============================] - 42s - loss: 0.2111 - acc: 0.9138 - val_loss: 0.2121 - val_acc: 0.9184\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 44s - loss: 0.2114 - acc: 0.9183 - val_loss: 0.2081 - val_acc: 0.9153\n",
      "Epoch 63/100\n",
      "61/61 [==============================] - 42s - loss: 0.2094 - acc: 0.9139 - val_loss: 0.2104 - val_acc: 0.9169\n",
      "Epoch 64/100\n",
      "61/61 [==============================] - 44s - loss: 0.2157 - acc: 0.9134 - val_loss: 0.2114 - val_acc: 0.9133\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 43s - loss: 0.2052 - acc: 0.9154 - val_loss: 0.2090 - val_acc: 0.9143\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 42s - loss: 0.2139 - acc: 0.9138 - val_loss: 0.2109 - val_acc: 0.9174\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 41s - loss: 0.2088 - acc: 0.9141 - val_loss: 0.2087 - val_acc: 0.9169\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 46s - loss: 0.2133 - acc: 0.9120 - val_loss: 0.2056 - val_acc: 0.9189\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 45s - loss: 0.1907 - acc: 0.9257 - val_loss: 0.2032 - val_acc: 0.9215\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 47s - loss: 0.2124 - acc: 0.9143 - val_loss: 0.2053 - val_acc: 0.9179\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 46s - loss: 0.1977 - acc: 0.9174 - val_loss: 0.2027 - val_acc: 0.9189\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 44s - loss: 0.2106 - acc: 0.9151 - val_loss: 0.2060 - val_acc: 0.9169\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 47s - loss: 0.2093 - acc: 0.9181 - val_loss: 0.2030 - val_acc: 0.9189\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 45s - loss: 0.2000 - acc: 0.9219 - val_loss: 0.2051 - val_acc: 0.9189\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 47s - loss: 0.1919 - acc: 0.9220 - val_loss: 0.2055 - val_acc: 0.9189\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 48s - loss: 0.1947 - acc: 0.9224 - val_loss: 0.2022 - val_acc: 0.9215\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 46s - loss: 0.2033 - acc: 0.9183 - val_loss: 0.2028 - val_acc: 0.9199\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 48s - loss: 0.2036 - acc: 0.9137 - val_loss: 0.1995 - val_acc: 0.9240\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 47s - loss: 0.1972 - acc: 0.9187 - val_loss: 0.2022 - val_acc: 0.9199\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 45s - loss: 0.1903 - acc: 0.9213 - val_loss: 0.2009 - val_acc: 0.9189\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 43s - loss: 0.1925 - acc: 0.9229 - val_loss: 0.1996 - val_acc: 0.9215\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 48s - loss: 0.1844 - acc: 0.9261 - val_loss: 0.2016 - val_acc: 0.9204\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 45s - loss: 0.2016 - acc: 0.9205 - val_loss: 0.2051 - val_acc: 0.9169\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 42s - loss: 0.1998 - acc: 0.9202 - val_loss: 0.2070 - val_acc: 0.9169\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 42s - loss: 0.1967 - acc: 0.9176 - val_loss: 0.2052 - val_acc: 0.9189\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 41s - loss: 0.1968 - acc: 0.9238 - val_loss: 0.2023 - val_acc: 0.9194\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 42s - loss: 0.1908 - acc: 0.9184 - val_loss: 0.2073 - val_acc: 0.9204\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 43s - loss: 0.1875 - acc: 0.9237 - val_loss: 0.2007 - val_acc: 0.9261\n",
      "Epoch 89/100\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9224\n",
      "Epoch 00088: reducing learning rate to 1.0000000474974514e-05.\n",
      "61/61 [==============================] - 41s - loss: 0.1931 - acc: 0.9226 - val_loss: 0.2028 - val_acc: 0.9235\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 42s - loss: 0.1793 - acc: 0.9271 - val_loss: 0.1999 - val_acc: 0.9240\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 42s - loss: 0.1866 - acc: 0.9243 - val_loss: 0.2003 - val_acc: 0.9235\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 42s - loss: 0.1859 - acc: 0.9265 - val_loss: 0.2002 - val_acc: 0.9230\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 42s - loss: 0.1979 - acc: 0.9212 - val_loss: 0.2002 - val_acc: 0.9225\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 44s - loss: 0.1917 - acc: 0.9235 - val_loss: 0.1989 - val_acc: 0.9261\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 43s - loss: 0.1904 - acc: 0.9300 - val_loss: 0.2006 - val_acc: 0.9235\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 42s - loss: 0.1888 - acc: 0.9238 - val_loss: 0.1993 - val_acc: 0.9210\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 42s - loss: 0.1885 - acc: 0.9230 - val_loss: 0.1995 - val_acc: 0.9245\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 42s - loss: 0.1919 - acc: 0.9228 - val_loss: 0.2007 - val_acc: 0.9225\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 41s - loss: 0.1800 - acc: 0.9260 - val_loss: 0.2000 - val_acc: 0.9245\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 40s - loss: 0.1928 - acc: 0.9252 - val_loss: 0.1989 - val_acc: 0.9230\n",
      "5884/5884 [==============================] - 17s    \n",
      "Train score: 0.146710414391\n",
      "Train accuracy: 0.948334466349\n"
     ]
    }
   ],
   "source": [
    "K=3\n",
    "Kfolds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED).split(Xtrain, Ytrain))\n",
    "y_test_pred_log = 0\n",
    "useGen = True\n",
    "useAngle = True\n",
    "epochs = 100\n",
    "for j, (train_idx, test_idx) in enumerate(Kfolds):\n",
    "    print('\\n===================FOLD=',j)\n",
    "    Xtrain_cv = Xtrain[train_idx]\n",
    "    Ytrain_cv = Ytrain[train_idx]\n",
    "    Xangle_cv = Xangle[train_idx]\n",
    "    Xtrain_val = Xtrain[test_idx]\n",
    "    Ytrain_val = Ytrain[test_idx]\n",
    "    Xangle_val = Xangle[test_idx]\n",
    "    \n",
    "    model_file = 'model_%s.hdf5' % j\n",
    "    \n",
    "    mcp_save = ModelCheckpoint(model_file, save_best_only=True, monitor='val_loss', mode='min')\n",
    "    model = getModel()\n",
    "    \n",
    "    if useGen and useAngle:\n",
    "        gen_flow = gen_flow_for_two_inputs(Xtrain_cv, Xangle_cv, Ytrain_cv)\n",
    "        Xtrain_val = [Xtrain_val, Xangle_val]\n",
    "        Xtrain_input = [Xtrain, Xangle]\n",
    "        Xtest_input = [Xtest, Xangle_test]\n",
    "    elif useAngle:\n",
    "        Xtrain_cv = [Xtrain_cv, Xangle_cv]\n",
    "        Xtrain_val = [Xtrain_val, Xangle_val]\n",
    "        Xtrain_input = [Xtrain, Xangle]\n",
    "        Xtest_input = [Xtest, Xangle_test]\n",
    "    else:\n",
    "        Xtrain_input = Xtrain\n",
    "        Xtest_input = Xtest\n",
    "        \n",
    "    if useGen:\n",
    "        model.fit_generator(gen_flow, steps_per_epoch=int(len(Xtrain_cv) / batch_size), epochs=epochs, shuffle=True, verbose=1, callbacks=[\n",
    "            earlyStopping, \n",
    "            mcp_save, \n",
    "            reduce_lr_loss, \n",
    "    #       tensorboard  \n",
    "        ], validation_data=(Xtrain_val, Ytrain_val))\n",
    "    else:\n",
    "        model.fit(Xtrain_cv, Ytrain_cv, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_data=(Xtrain_val, Ytrain_val))\n",
    "\n",
    "    model.load_weights(filepath = model_file)    \n",
    "    \n",
    "    score = model.evaluate(Xtrain_input, Ytrain, verbose=1)\n",
    "    print('Train score:', score[0])\n",
    "    print('Train accuracy:', score[1])\n",
    "    y_test_pred_log += model.predict(Xtest_input).reshape(Xtest.shape[0])\n",
    "    \n",
    "y_test_pred_log /= K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id    is_iceberg\n",
      "0  5941774d  1.965353e-01\n",
      "1  4023181e  3.879364e-01\n",
      "2  b20200e4  3.209522e-07\n",
      "3  e7f018bb  9.962336e-01\n",
      "4  4371c8c3  5.066199e-03\n",
      "5  a8d9b1fd  4.198272e-01\n",
      "6  29e7727e  5.725038e-02\n",
      "7  92a51ffb  9.907501e-01\n",
      "8  c769ac97  1.245574e-04\n",
      "9  aee0547d  4.414778e-24\n",
      "id            8424\n",
      "is_iceberg    8424\n",
      "dtype: int64 8424\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': y_test_pred_log.reshape(y_test_pred_log.shape[0])})\n",
    "print(submission.head(10))\n",
    "print(submission.count(), Xtest.shape[0])\n",
    "\n",
    "submission.to_csv('submission-cnn-custom.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
